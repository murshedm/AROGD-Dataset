Chapter 4
Multi-hop Question Generation
In this chapter, the problem definition is described first. Later, this research’s proposed
methodology is described in detail of its various components. The whole architecture of
KG4QG is shown in Figure 4.1.
Figure 4.1: Full Architecture of KG4QG.
33

4.2. KNOWLEDGE GRAPH CREATION
4.1
Problem Definition
In this section, we define the multi-hop question generation task based on conditional
probability. Suppose for each answer, an input context C is defined as C = (C1,C2,...,CN)
where Ci is one paragraph and N is the number of paragraphs in the context. We want to
generate question Q = (Q1,Q2,.....,QM), where M represents the number of tokens in the
question. The probability of the generated question can be defined by equation 4.1.
P(Q|C) = ΠM
i=1P(Qi|[Q1,Q2,....,Qi−1],[C1,C2,C3,....CN])
(4.1)
As for this research, the input context C is generated by considering two paragraphs as the
answer of the question can be gained by the reasoning path of the two paragraphs, then
equation 4.1 becomes,
P(Q|C) = ΠM
i=1P(Qi|[Q1,Q2,....,Qi−1],[C1,C2])
(4.2)
4.2
Knowledge Graph Creation
In this section, we discuss the process of creating a Knowledge Graph (KG). To create
the knowledge graph, the input text is converted to annotated text by performing coreference
resolution on the input text. Then the knowledge graph is generated by using Stanford
CoreNLP 6. The whole procedure is shown in Figure 4.2.
Figure 4.2: Knowledge Graph Creation.
6https://stanfordnlp.github.io/CoreNLP/.
34

4.2. KNOWLEDGE GRAPH CREATION
4.2.1
Coreference Resolution
In the input text, we have done a co-reference resolution to identify and link the same
entities. Our main aim is to create a knowledge graph from each input text. Coreference
resolution helps remove the redundancy in the nodes and links the same entities, ensuring
each entity corresponds to a single node in the graph. This results in a more accurate graph.
A coreference resolution model from spaCy 7 called en coreference web trf is used in this
research which identifies and refers to the same entity (e.g., replacing the pronoun with
its actual noun). Listing 4.1 shows the input data sample from the HotpotQA dataset and
Listing 4.2 shows the output of performing coreference resolution in the input data sample.
Listing 4.1: Input Data Sample for Coreference Resolution.
{
{
’p1’: {
’title ’: ’Shirley Temple ’,
’context ’: "Shirley Temple Black (April 23, 1928
February 10, 2014) was an American actress , singer ,
dancer , businesswoman , and diplomat who was
Hollywood ’s number one box -office draw as a child
actress from 1935 to 1938. As an adult , she was
named United States ambassador to Ghana and to
Czechoslovakia and also served as Chief of Protocol
of the United States."
},
’p2’: {
’title ’: ’Kiss and Tell (1945 film)’,
’context ’: "Kiss and Tell is a 1945 American comedy
7https://spacy.io/.
35

4.2. KNOWLEDGE GRAPH CREATION
film starring then 17-year -old Shirley Temple as
Corliss Archer. In the film , two teenage girls cause
their respective parents much concern when they
start to become interested in boys. The parents ’
bickering about which girl is the worse influence
causes more problems than it solves."
}
}
}
Listing 4.2: Output of Coreference Resolution.
{
{
’p1’: {
’title ’: ’Shirley Temple ’,
’context ’: "Shirley Temple was an American actress ,
singer , dancer , businesswoman , and diplomat who was
Hollywood ’s number one box -office draw as a child
actress from 1935 to 1938. As an adult , Shirley
Temple was named United States ambassador to Ghana
and to Czechoslovakia and also served as Chief of
Protocol of the United States."
},
’p2’: {
’title ’: ’Kiss and Tell (1945 film)’,
’context ’: "Kiss and Tell (1945 film) is a 1945
American comedy film starring then 17-year -old
36

4.2. KNOWLEDGE GRAPH CREATION
Shirley Temple as Corliss Archer. In Kiss and Tell
(1945 film) , two teenage girls cause two teenage
girls respective parents much concern when two
teenage girls start to become interested in boys.
their respective parents bickering about which girl
is the worse influence causes more problems than The
parents ’ bickering solves."
}
}
}
4.2.2
Creating Knowledge Graph Involving Stanford CoreNLP
The main aim was to involve Stanford CoreNLP to extract the entities from the input
texts, create relationships between them, and finally generate a graph structure for each
input text. To create the knowledge graph, we first have to start the CoreNLP server. The
Open Information Extraction (OpenIE) 8 annotator is used as it can extract triplets from the
input text. A triplet is a representation of a subject, a relation, and the object of the relation.
Finally, the graph is visualized using the NetworkX 9package in Python. Listing 4.3 is the
sample input for knowledge graph generation where Table 4.1 are the extarcted triplets from
this sample input involving Stanford CoreNLP server. Table 4.2 shows the node mapping
and edge mapping from the extracted triplets. Figure 4.3 is the final knowledge graph
representation of this input sample.
8https://stanfordnlp.github.io/CoreNLP/openie.html.
9https://networkx.org/.
37

4.2. KNOWLEDGE GRAPH CREATION
Listing 4.3: Sample Input for Knowledge Graph Generation.
Shirley Temple was an American actress , singer , dancer ,
businesswoman , and diplomat who was Hollywood ’s number
one box -office draw as a child actress from 1935 to
1938. As an adult , Shirley Temple was named United
States ambassador to Ghana and to Czechoslovakia and
also served as Chief of Protocol of the United States.
Table 4.1: Extracted Triplets from Sample Input Involving Stanford CoreNLP.
Subject
Relation
Object
Shirley Temple
was
American actress
Hollywood
has
number
one
box
office
draw as child actress from
1935 to 1938
Shirley Temple
was named United States
ambassador as
adult
Shirley Temple
was named United States
ambassador to
Ghana
Shirley Temple
was named United States
ambassador to
Czechoslovakia
Shirley Temple
served as
Chief of Protocol of United
States
4.2.3
Graph Representation Creation
After generating the knowledge graph, we created a graph representation of each knowl-
edge graph by constructing PyG 10 compatible tensors. The triplets are converted to the
following attributes.
• Node Features
A node features is a dictionary mapping where each node (subject or object) of the
graph is converted to an index number to convert human readable node labels to ma-
10ttps://pyg.org/.
38

4.2. KNOWLEDGE GRAPH CREATION
Table 4.2: Triplets to Nodes and Edges Generation.
Node
Num-
ber
Node Name
0
Shirley Temple
1
American actress
2
Hollywood
3
number one box office draw as
child actress from 1935 to 1938
4
adult
5
Ghana
6
Czechoslovakia
7
Chief of Protocol of United
States
(a) Node Mapping
Edge
Num-
ber
Node to Node Con-
nectivity
Relation
0
Shirley
Temple
→
American actress
was
1
Shirley
Temple
→
adult
was
named
United
States
ambas-
sador as
2
Shirley
Temple
→
Ghana
was
named
United
States
ambas-
sador to
3
Shirley
Temple
→
Czechoslovakia
was
named
United
States
ambas-
sador to
4
Shirley
Temple
→
Chief of Protocol of
United States
served
as
5
Hollywood
→
number one box of fice
draw as child actress
from 1935 to 193
has
(b) Edge Mapping
39

4.2. KNOWLEDGE GRAPH CREATION
Figure 4.3: Graph Representation with Nodes and Edges.
chine readable index number. The embedding for each node is generated using the
Sentence Transformer model all-Minim-L6-v2 11.This is a Pytorch Tensor of shape
[number of nodes in the graph, 384]. The value 384 comes from the output dimen-
sionality of the all-Minim-L6-v2 Sentence Transformer model.
• Edge Index
To represent the connection between source to destination of an edge, the edge index
represents the connectivity. This is a Pytorch Tensor 12 of shape [2, number of edges].
• Edge Attribute
The edge attribute represents the embeddings of each subject to object relations. The
embedding for each attribute are generated using the Sentence Transformer model
all-Minim-L6-v2. This is a Pytorch Tensor of shape [2, number of edges] where the
embedding dimension is 384.
After generating the attributes from the triplets, each row of questions is encoded by the
11https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2.
12https://pytorch.org/docs/stable/tensors.html.
40

4.3. GRAPH EMBEDDING CREATION
Table 4.3: Specification of Parameters for GAT Model.
Parameters
Values
Optimizer
Adam Optimizer
Learning Rate
0.001
Loss Function
Mean Squared Error (MSE)
Patience (early stopping)
15
Scheduler
ReduceLROnPlateau
Patience (ReduceLROnPlateau)
5
mode (ReduceLROnPlateau)
’min’
Weight Decay
1∗10−4
Epoch
200
Batch Size
32
Train, Validation, and Test Split
70:15:15
all-MiniLM-L6-v2 model to generate the space vector of the questions. Finally, the graph
object representation is generated by creating a Data object from the PyTorch Geometric
(PyG) 13 library. The graph data object is created by packaging the node features, edge
index, edge attributes, and the target variable, where the target variable is the question in
this research.
4.3
Graph Embedding Creation
The graph embedding is generated by the graph attention network (GAT) model. At
first, we load the graph data object, then the model is trained, and the evaluation is done on
a test set from the data object. The GAT architecture is shown in Figure 4.4. The training,
testing and validation split is done as 70:15:15 ratio. During training, the Mean Squared
Error (MSE) is used to calculate the loss. Early stopping and model checkpoint are also
used. The model is trained for 200 epochs. The configuration of the GAT model to create
graph object optimization is shown in Table 4.3.
The brief explanation of the GAT model architecture for graph embedding generation
is shown in Figure 4.4.
13https://pytorch-geometric.readthedocs.io/en/latest/generated/torch geometric.data.Data.html.
41

4.3. GRAPH EMBEDDING CREATION
• Input
The graph object, which holds the information of node features, edge index, and edge
attribute, is the input for the GAT model.
• Graph Convolution Layer
A total of five convolution layers are specified as graph convolution layers, where
in the first layer, the total number of input features for each node is 384, which is
the dimensionality of the feature vector of each node, and the total number of output
features for each node per head is 256.
• Attention Mechanism
A multi-head attention mechanism is used, as it allows learning different attention
patterns. The number of independent attention head set to 8.
• Dropout
A dropout layer is added after every Convolution layer and before the Layer Normal-
ization layer, where the dropout probability of the neuron is 20% during training.
• Layer Normalization
Before the dropout layer, normalization is added to normalize the output of the Con-
volution layer.
• ELU Activation
ELU is an activation function used after the Convolution layer, so non-linearity is
introduced in the model.
• Global Mean Pooling
At the end, a global mean pooling layer is added, which is a pooling operation. After
the final Convolution layer, when the final node embeddings are generated, this layer
averages them for each graph in the batch.
42

4.4. INPUT TEXT EMBEDDING CREATION
• Output
The final output is a single vector representation of each graph, whose size is batch
size * 384.
Figure 4.4: Visualization of GAT Model Architecture for Graph Embedding.
43

4.6. ENCODER
4.4
Input Text Embedding Creation
The input text is converted to a numerical representation using the Sentence Trans-
former. We utilize the all-Minim-L6-v2 model to generate the space vector of the input
text, so the encoded input text is generated.
4.5
Combining of Input Text Embedding and Graph Embedding
The graph embedding is concatenated at the end of the input text embedding by torch.stack14.
The torch.stack function is in the PyTorch15 library, which is used to concatenate the input
text embedding and the graph embedding along with a new dimension. Both input text
embedding and graph text embedding have the same shape. In this research, torch.stack is
used over torch.cat 16 as the torch.stack function is better for batching operations in deep
learning.
Figure 4.5: Concatenation of Input Text Embedding and Graph Embedding.
4.6
Encoder
The encoder of a large language model is used to improve the representation of input
by utilizing a transformer architecture, which includes feed-forward neural networks and a
14https://pytorch.org/docs/stable/generated/torch.stack.html.
15https://pytorch.org/.
16https://pytorch.org/docs/stable/generated/torch.cat.html.
44

4.7. DECODER
multi-head self-attention mechanism. As a backbone, we have utilized both BART and T5
as encoders separately to determine which model performs better. In both cases, we used
the base models. To ensure that the text fits the input length of the model and to generate
an attention mask for the input text, we have fed tokenization into the LLM encoder com-
ponent. We have generated the attention mask from the input text and then the combined
embedding layer is ready to be fed into the encoder of the LLMs (BART-base and T5-base).
Figure 4.6 describes the encoder components and its input in this research.
Figure 4.6: Encoder Part of KG4QG.
45

4.8. EXECUTION DETAILS
4.7
Decoder
The LLM decoder which follows the transformer architecture is used for autoregressive
generation. That is to say, based on previous predictions, it produces questions token by
token. It produces the prediction of likelihoods to generate the next token in the vocabulary.
The decoder uses mask self-attention so the model ensures autoregressive masking, so the
next prediction of word is based on the words that are previously examined in the sequence
plus the current word. Figure 4.1 shows the entire architecture of KG4QG, which contains
both the encoder and decoder. The encoder finally produces the context embeddings, which
are the input of the decoder.
4.8
Execution Details
In this research, using KG4QG (Knowledge Graph for Question Generation), we have
used both BART and T5. All of the implementation is done in Python programming lan-
guage using PyTorch (Paszke et al., 2019). PyTorch is popular to researchers because of its
flexibility, documentation, features, and efficiency. We have used the base models in both
BART and T5.
The Facebook/BART-base model is used in this research. The Facebook/BART-base 17
is a pre-trained model which is developed by Facebook AI. This model combines a bidi-
rectional encoder (like BERT) with an autoregressive decoder (like GPT). This model is
not pre-trained for any specific task and can be used in different text generation tasks like
summarization, machine translation, question answering and others. The total number of
parameters of this model is approximately 139 millions and the maximum sequence length
is 1024 tokens.
We have also used the T5-base 18 model in this research. The T5-base is a pre-trained
model which is a base size version of the T5 model developed by Google (Raffel et al.,
2023). This model is pre-trained on the span corruption task (Ye et al., 2024) using the C4
17https://huggingface.co/facebook/bart-base.
18https://huggingface.co/google-t5/t5-base.
46

4.8. EXECUTION DETAILS
dataset (Dodge et al., 2021). The total number of parameters of this model is approximately
220 million, and the maximum sequence length is 512 tokens.
4.8.1
Fine-tuning
To make the pre-trained models task-specific on multi-hop question generation, fine-
tuning is done. The fine-tuning is done based on input texts and knowledge graphs. That
means, for each input texts a knowledge graph is generated, and both the text and knowledge
graph are concatenated for fine-tuning. The HotpotQA dataset is used for this process. For
both BART and T5, the models are trained for 50 epochs, where early stopping is used as
a regularization technique to prevent the model from overfitting. Based on validation loss
the patience parameter is fixed as 3, which means if no change occurs in validation loss,
after three epochs, the training will be stopped. The best outcome of the T5-base model is
achieved after 50 epochs, where for the Facebook/BART-base model the best outcome is
achieved after 20 epochs. The Adam optimizer (Kingma and Ba, 2017) is used, where the
Cross-Entropy function (Mao et al., 2023) is used to calculate the loss. The learning rate is
fixed as 10−4. The whole configuration is shown in Table 4.4.
Table 4.4: Specification of parameters to fine-tune the models for KG4QG.
Model Name
Parametrs
Value
BART-base
Optimizer
AdamW
Learning rate
1e−4
Loss function
Cross-entropy
Weight decay
0.01
Epoch
20
Patience (early stopping)
3
Batch size
8
Train, validation and test split
0.70 : 0.15 :0.15
T5 -base
Optimizer
AdamW
Learning rate
1e−4
Loss function
Cross-entropy
Weight decay
0.01
Epoch
50
Patience (early stopping)
3
Batch size
8
Train, validation and test split
0.70 : 0.15 :0.15
47

4.8. EXECUTION DETAILS
4.8.2
Experiment Setup
The main computational resources of this research are the Narval and Cedar servers
from Compute Canada 19. The Narval server provides A100 GPUs and Cedar provides
V100 GPUs. We have used the Distributed Data Parallet (DDP) 20 feature from PyTorch so
that we can utilize 4 GPUs simultaneously. Details, code and implementations are available
on our Github 21 repository.
19https://docs.alliancecan.ca/.
20https://docs.alliancecan.ca/wiki/PyTorch.
21https://github.com/AlHasibMahamud/KG4QG MultihopQG.
48
