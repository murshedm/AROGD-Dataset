CS 4501/6501 Interpretable Machine Learning
Hanjie Chen, Yangfeng Ji
Department of Computer Science
University of Virginia
{hc9mx, yangfeng}@virginia.edu
1
Post-hoc explanations: perturbation-based methods

--------------------------------------------------------------------------------
[End of Page 1]

2
Trade-off
Black-box 
Neural Network
Interpretable GAM
Limitations
â€¢
Ignoring complex 
feature interactions
â€¢
Performance drop

--------------------------------------------------------------------------------
[End of Page 2]

3
Explaining Black-box Model
Black Box
Input
Output
ğ’™
ğ’š
How to improve model interpretability?
Modelâ€™s inner working and decision making are hidden in the black box 

--------------------------------------------------------------------------------
[End of Page 3]

4
Explaining Black-box Model
Black Box
Input
Output
ğ’™
ğ’š
How to improve model interpretability?
Explaining model predictions from the post-hoc manner
Interpreter
Post-hoc explanation
Extracting relationships 
between input features 
and the model prediction

--------------------------------------------------------------------------------
[End of Page 4]

5
Post-hoc Explanation
Input features
Model prediction
Importance
ğ‘¥1
ğ‘¥2
ğ‘¥ğ‘›
â‹¯
ğ‘¦
ğ‘1
ğ‘2
ğ‘ğ‘›
Identifying important features

--------------------------------------------------------------------------------
[End of Page 5]

6
Post-hoc Explanation
â€¢ Example 1: tabular data
-
Mushroom dataset
-
Task: predicting if a mushroom is edible or poisonous
Feature
Odor
gill size
stalk surface 
above ring
Spore print color
stalk surface 
below ring

--------------------------------------------------------------------------------
[End of Page 6]

7
Post-hoc Explanation
â€¢ Example 1: tabular data
-
Mushroom dataset
-
Task: predicting if a mushroom is edible or poisonous
Feature
Value
Odor=foul
ğ‘¥1 = 1 (true)
gill size=broad
ğ‘¥2 = 1
stalk surface above 
ring=silky
ğ‘¥3 = 1
Spore print 
color=chocolate
ğ‘¥4 = 1
stalk surface below 
ring=silky
ğ‘¥5 = 1
Model
Prediction
poisonous
Explanation
Input
Importance
ğ‘1 = 0.26
ğ‘2 = âˆ’0.13
ğ‘3 = 0.11
ğ‘4 = 0.08
ğ‘5 = 0.06

--------------------------------------------------------------------------------
[End of Page 7]

8
Post-hoc Explanation
â€¢ Example 1: tabular data
-
Mushroom dataset
-
Task: predicting if a mushroom is edible or poisonous
Feature
Value
Odor=foul
ğ‘¥1 = 1 (true)
gill size=broad
ğ‘¥2 = 1
stalk surface above 
ring=silky
ğ‘¥3 = 1
Spore print 
color=chocolate
ğ‘¥4 = 1
stalk surface below 
ring=silky
ğ‘¥5 = 1
Model
Prediction
poisonous
Explanation
Input
Importance
ğ‘1 = 0.26
ğ‘2 = âˆ’0.13
ğ‘3 = 0.11
ğ‘4 = 0.08
ğ‘5 = 0.06
(the most 
important feature)
(indicating edible)

--------------------------------------------------------------------------------
[End of Page 8]

9
Post-hoc Explanation
â€¢ Example 2: text data
-
Movie review
-
Task: predicting the sentiment of a text (positive or negative)
Model
Prediction
positive
Input
a
clever
piece
of
cinema
ğ’™1
ğ’™2
ğ’™3
ğ’™4
ğ’™5

--------------------------------------------------------------------------------
[End of Page 9]

10
Post-hoc Explanation
â€¢ Example 2: text data
-
Movie review
-
Task: predicting the sentiment of a text (positive or negative)
Model
Prediction
positive
Explanation
Input
a
clever
piece
of
cinema
ğ’™1
ğ’™2
ğ’™3
ğ’™4
ğ’™5
ğ‘1 = 0.11
ğ‘2 = 0.46
ğ‘3 = 0.01
ğ‘4 = âˆ’0.02
ğ‘5 = 0.06
(Word saliency map)
a
clever
piece
of
cinema
Pos
Neg
0.5
0
âˆ’0.5

--------------------------------------------------------------------------------
[End of Page 10]

11
Post-hoc Explanation
â€¢ Example 3: image data
Task: Object recognition
Source: https://medium.datadriveninvestor.com/visualizing-neural-networks-using-saliency-maps-in-pytorch-289d8e244ab4
Feature: a pixel ğ’™ğ‘–ğ‘—
(color, intensityâ€¦)

--------------------------------------------------------------------------------
[End of Page 11]

12
Post-hoc Explanation
â€¢ Example 3: image data
Task: Object recognition
Prediction: Dog
Source: https://medium.datadriveninvestor.com/visualizing-neural-networks-using-saliency-maps-in-pytorch-289d8e244ab4
ğ’™ğ‘–ğ‘—
ğ’‚ğ‘–ğ‘—
Saliency map: the lighter color, 
the larger value
Explanation
Input

--------------------------------------------------------------------------------
[End of Page 12]

13
Post-hoc Explanation
â€¢ Model-agnostic (black-box): not requiring access to model inner working  
How to learn feature importance?
Perturbation-based methods
â€¢ Local: explaining model prediction per example 

--------------------------------------------------------------------------------
[End of Page 13]

14
Perturbation-based methods
â€¢ LIME 
â€¢ SHAP
(Ribeiro et al., KDD, 2016)
(Lundberg and Lee, NIPS, 2017)

--------------------------------------------------------------------------------
[End of Page 14]

15
LIME
"Why Should I Trust You?"
Explaining the Predictions of Any Classifier
Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin
(KDD, 2016)

--------------------------------------------------------------------------------
[End of Page 15]

16
Interpretable Model
â€¢ Linear model
â„ğ‘¦ğ’™= ğ’˜ğ‘¦ğ‘‡ğ’™
- ğ‘¤ğ‘¦,ğ‘—: the contribution of ğ‘¥ğ‘—
-
Higher weights indicate more important features 
ğ’™âˆˆ0, 1 ğ‘›
Global interpretation
Feature
ğ’™ğ‘£1
ğ’™ğ‘£2
ğ’™ğ‘£ğ‘›
â‹¯
Importance
â‹¯
ğ‘¤ğ‘¦,ğ’™ğ‘£1
â‹¯
ğ‘¤ğ‘¦,ğ’™ğ‘£2
ğ‘¤ğ‘¦,ğ’™ğ‘£ğ‘›

--------------------------------------------------------------------------------
[End of Page 16]

17
Interpretable Model
â€¢ Linear model
â„ğ‘¦ğ’™= ğ’˜ğ‘¦ğ‘‡ğ’™
- ğ‘¤ğ‘¦,ğ‘—: the contribution of ğ‘¥ğ‘—
-
Higher weights indicate more important features 
ğ’™âˆˆ0, 1 ğ‘›
Global interpretation
Feature
ğ’™ğ‘£1
ğ’™ğ‘£2
ğ’™ğ‘£ğ‘›
â‹¯
Importance
ğ‘¤ğ‘¦,ğ’™ğ‘£1
â‹¯
ğ‘¤ğ‘¦,ğ’™ğ‘£2
ğ‘¤ğ‘¦,ğ’™ğ‘£ğ‘›
â‹¯
Logistic regression 
â€œItâ€        â€œisâ€       â€œaâ€      â€œfantasticâ€      â€œmovieâ€
ğ’˜0
ğ’˜1
[Pos]
[Neg]
0.89
0.72
1.13
-1.92
0.34
0.85
0.82
1.05
2.21
0.26
1.16
5.19
Prediction: positive

--------------------------------------------------------------------------------
[End of Page 17]

18
Neural Networks
Global interpretation is not capable of explaining each specific model prediction
-
Neural networks can capture complex relationships between features 
and the response
-
The meaning of a feature may vary across different examples
â€œgoodâ€
adjective
Morally excellent
noun
Possessions

--------------------------------------------------------------------------------
[End of Page 18]

19
Neural Networks
Global interpretation is not capable of explaining each specific model prediction
-
Neural networks can capture complex relationships between features 
and the response
-
The meaning of a feature may vary across different examples
â€œgoodâ€
adjective
Morally excellent
noun
Possessions
Local interpretation
Explaining model prediction 
per example by identifying 
local feature importance

--------------------------------------------------------------------------------
[End of Page 19]

20
LIME: Local Interpretable Model-Agnostic Explanations
The way that explains model 
predictions or the generated 
explanations are understandable 
to humans 

--------------------------------------------------------------------------------
[End of Page 20]

21
Idea: using local linear model to approximate neural network for each example 
â€¢
Decision boundary of a neural 
network ğ‘“
â€¢
Dashed line: local linear model ğ‘”
â€¢
Blue/pink background represents 
negative (-) /positive (+) class
â€¢
Bold red cross: the instance ğ’™being 
explained
ğ‘”â‰ˆğ‘“
LIME: Local Interpretable Model-Agnostic Explanations

--------------------------------------------------------------------------------
[End of Page 21]

22
â€¢ Interpretable data representations
Neural network ğ‘“
Linear model ğ‘”
ğ’™= ğ’™1, ğ’™2, â‹¯, ğ’™ğ‘›
ğ’™â€² = ğ‘¥â€²1, ğ‘¥â€²2, â‹¯, ğ‘¥â€²ğ‘
Feature representation 
ğ’™ğ‘–âˆˆâ„ğ’…is uninterpretable 
Feature representation 
ğ‘¥â€²ğ‘–âˆˆ0, 1 is interpretable 
-
ğ‘›: the number of features in the example
-
ğ‘: the number of all features
LIME: Local Interpretable Model-Agnostic Explanations

--------------------------------------------------------------------------------
[End of Page 22]

23
â€¢ Interpretable data representations
Neural network ğ‘“
Linear model ğ‘”
ğ’™= ğ’™1, ğ’™2, â‹¯, ğ’™ğ‘›
ğ’™â€² = ğ‘¥â€²1, ğ‘¥â€²2, â‹¯, ğ‘¥â€²ğ‘
Feature representation 
ğ’™ğ‘–âˆˆâ„ğ’…is uninterpretable 
Feature representation 
ğ‘¥â€²ğ‘–âˆˆ0, 1 is interpretable 
Image
Text
ğ’™ğ‘–: a tensor with three color channels per pixel
0/1 indicates the absence/presence of a patch of pixels
ğ’™ğ‘–: a high-dimensional vector (word embedding)
Image
Text
0/1 indicates the absence/presence of a word
(bag-of-words representation)
LIME: Local Interpretable Model-Agnostic Explanations

--------------------------------------------------------------------------------
[End of Page 23]

24
â€¢ Interpretable data representations
Neural network ğ‘“
Linear model ğ‘”
ğ’™= ğ’™1, ğ’™2, â‹¯, ğ’™ğ‘›
ğ’™â€² = ğ‘¥â€²1, ğ‘¥â€²2, â‹¯, ğ‘¥â€²ğ‘
â‰ˆ
Text
a
good
movie
ğ’™
ğ’™1
ğ’™2
ğ’™3
Vocab
â‹®
a
â‹®
good
â‹®
movie
â‹®
ğ’™â€²
â‹®(0)
1
â‹®
1
â‹®
1
â‹®
LIME: Local Interpretable Model-Agnostic Explanations

--------------------------------------------------------------------------------
[End of Page 24]

25
â€¢ Sampling for local exploration
Need more samples to fit a local linear model
ğ’™â€² = 0, â‹¯, 1, â‹¯, 1, â‹¯, 1, â‹¯, 1, â‹¯, 0, 1, â‹¯, 0 ğ‘
It
is
a fantastic movie
LIME: Local Interpretable Model-Agnostic Explanations

--------------------------------------------------------------------------------
[End of Page 25]

26
â€¢ Sampling for local exploration
Need more samples to fit a local linear model
ğ’™â€² = 0, â‹¯, 1, â‹¯, 1, â‹¯, 1, â‹¯, 1, â‹¯, 0, 1, â‹¯, 0 ğ‘
It
is
a fantastic movie
Randomly sample nonzero elements
ğ’›1â€² = 0, â‹¯, 0, â‹¯, 0, â‹¯, 1, â‹¯, 0, â‹¯, 0, 1, â‹¯, 0 ğ‘
a
movie
LIME: Local Interpretable Model-Agnostic Explanations

--------------------------------------------------------------------------------
[End of Page 26]

27
â€¢ Sampling for local exploration
Need more samples to fit a local linear model
ğ’™â€² = 0, â‹¯, 1, â‹¯, 1, â‹¯, 1, â‹¯, 1, â‹¯, 0, 1, â‹¯, 0 ğ‘
It
is
a fantastic movie
Randomly sample nonzero elements
ğ’›1â€² = 0, â‹¯, 0, â‹¯, 0, â‹¯, 1, â‹¯, 0, â‹¯, 0, 1, â‹¯, 0 ğ‘
ğ’›2â€² = 0, â‹¯, 0, â‹¯, 0, â‹¯, 0, â‹¯, 1, â‹¯, 0, 1, â‹¯, 0 ğ‘
a
movie
fantastic movie
LIME: Local Interpretable Model-Agnostic Explanations

--------------------------------------------------------------------------------
[End of Page 27]

28
â€¢ Sampling for local exploration
Need more samples to fit a local linear model
ğ’™â€² = 0, â‹¯, 1, â‹¯, 1, â‹¯, 1, â‹¯, 1, â‹¯, 0, 1, â‹¯, 0 ğ‘
It
is
a fantastic movie
Randomly sample nonzero elements
ğ’›1â€² = 0, â‹¯, 0, â‹¯, 0, â‹¯, 1, â‹¯, 0, â‹¯, 0, 1, â‹¯, 0 ğ‘
ğ’›2â€² = 0, â‹¯, 0, â‹¯, 0, â‹¯, 0, â‹¯, 1, â‹¯, 0, 1, â‹¯, 0 ğ‘
a
movie
fantastic movie
ğ’›ğ‘€â€² = 0, â‹¯, 0, â‹¯, 0, â‹¯, 0, â‹¯, 1, â‹¯, 0, 0, â‹¯, 0 ğ‘
â‹®
fantastic
LIME: Local Interpretable Model-Agnostic Explanations

--------------------------------------------------------------------------------
[End of Page 28]

29
â€¢ Sampling for local exploration
Need more samples to fit a local linear model
ğ’™â€² = 0, â‹¯, 1, â‹¯, 1, â‹¯, 1, â‹¯, 1, â‹¯, 0, 1, â‹¯, 0 ğ‘
It
is
a fantastic movie
Randomly sample nonzero elements
ğ’›1â€² = 0, â‹¯, 0, â‹¯, 0, â‹¯, 1, â‹¯, 0, â‹¯, 0, 1, â‹¯, 0 ğ‘
ğ’›2â€² = 0, â‹¯, 0, â‹¯, 0, â‹¯, 0, â‹¯, 1, â‹¯, 0, 1, â‹¯, 0 ğ‘
a
movie
fantastic movie
ğ’›ğ‘€â€² = 0, â‹¯, 0, â‹¯, 0, â‹¯, 0, â‹¯, 1, â‹¯, 0, 0, â‹¯, 0 ğ‘
â‹®
fantastic
What are the labels of 
these pseudo examples?
LIME: Local Interpretable Model-Agnostic Explanations

--------------------------------------------------------------------------------
[End of Page 29]

30
â€¢ Sampling for local exploration
Labeling pseudo examples with neural network ğ‘“
ğ’›1â€²
ğ’›2â€²
ğ’›ğ‘€â€²
â‹®
ğ’›1
ğ’›2
ğ’›ğ‘€
â‹®
ğ‘“ğ’›1
ğ‘“ğ’›2
ğ‘“ğ’›ğ‘€
Negative
Positive
Positive
LIME: Local Interpretable Model-Agnostic Explanations

--------------------------------------------------------------------------------
[End of Page 30]

31
â€¢ Sampling for local exploration
Labeling pseudo examples with neural network ğ‘“
ğ’›1â€²
ğ’›2â€²
ğ’›ğ‘€â€²
â‹®
ğ’›1
ğ’›2
ğ’›ğ‘€
â‹®
ğ‘“ğ’›1
ğ‘“ğ’›2
ğ‘“ğ’›ğ‘€
Negative
Positive
Positive
a movie
fantastic movie
LIME: Local Interpretable Model-Agnostic Explanations

--------------------------------------------------------------------------------
[End of Page 31]

32
â€¢ Sampling for local exploration
Penalize noisy examples 
Distance between ğ’™and ğ’›ğ‘š
ğœ‹ğ’™ğ’›ğ‘š= ğ‘’(âˆ’ğ·ğ’™,ğ’›ğ‘š2/ğœ2)
ğ·: cosine distance (for text), ğ¿2 distance (for image)
LIME: Local Interpretable Model-Agnostic Explanations

--------------------------------------------------------------------------------
[End of Page 32]

33
â€¢ Sparse linear explanation
Fitting a local linear model 
ğ‘”(ğ’›â€²) â‰ˆğ‘“(ğ’›)
ğ’›ğ‘šâ€², ğ‘“ğ’›ğ‘š
ğ‘š=1,â‹¯,ğ‘€
ğ‘”ğ’›â€² = ğ’˜ğ‘‡ğ’›â€²
LIME: Local Interpretable Model-Agnostic Explanations

--------------------------------------------------------------------------------
[End of Page 33]

34
â€¢ Sparse linear explanation
Fitting a local linear model 
ğ‘”(ğ’›â€²) â‰ˆğ‘“(ğ’›)
ğ’›ğ‘šâ€², ğ‘“ğ’›ğ‘š
ğ‘š=1,â‹¯,ğ‘€
Objective
min â„’ğ‘“, ğ‘”
â„’ğ‘“, ğ‘”= à·ğœ‹ğ’™ğ’›(ğ‘“ğ’›âˆ’ğ‘”(ğ’›â€²))2
ğ‘”ğ’›â€² = ğ’˜ğ‘‡ğ’›â€²
LIME: Local Interpretable Model-Agnostic Explanations

--------------------------------------------------------------------------------
[End of Page 34]

35
â€¢ Sparse linear explanation
Fitting a local linear model 
ğ‘”(ğ’›â€²) â‰ˆğ‘“(ğ’›)
ğ’›ğ‘šâ€², ğ‘“ğ’›ğ‘š
ğ‘š=1,â‹¯,ğ‘€
Objective
min â„’ğ‘“, ğ‘”+ Î©(ğ‘”)
â„’ğ‘“, ğ‘”= à·ğœ‹ğ’™ğ’›(ğ‘“ğ’›âˆ’ğ‘”(ğ’›â€²))2
Restricting complexity (the 
number of nonzero weights)
ğ‘”ğ’›â€² = ğ’˜ğ‘‡ğ’›â€²
LIME: Local Interpretable Model-Agnostic Explanations

--------------------------------------------------------------------------------
[End of Page 35]

36
â€¢ Sparse linear explanation
Extracting feature importance scores
ğ’˜à·œğ‘¦ğ‘‡
-
à·œğ‘¦: model prediction on the original example
-
Local explanation: ğ‘¤à·œğ‘¦,ğ’™1, â‹¯, ğ‘¤à·œğ‘¦,ğ’™ğ‘›
LIME: Local Interpretable Model-Agnostic Explanations

--------------------------------------------------------------------------------
[End of Page 36]

37
Question?

--------------------------------------------------------------------------------
[End of Page 37]

38
â€¢ Explaining each example individually, not the whole dataset (locally faithful)
â€¢ May not work for highly non-linear models
LIME: Local Interpretable Model-Agnostic Explanations

--------------------------------------------------------------------------------
[End of Page 38]

39
â€¢ Example: Deep networks for image classification
Model: Googleâ€™s pre-trained Inception neural network
Top 3 predicted classes
Electric guitar
Acoustic guitar
Labrador
The explanations enhance trust in the model, as it acts in a reasonable manner
LIME: Local Interpretable Model-Agnostic Explanations

--------------------------------------------------------------------------------
[End of Page 39]

40
Question?

--------------------------------------------------------------------------------
[End of Page 40]

41
Submodular Pick for Explaining Models
Single explanation is not 
sufficient to evaluate and assess 
trust in the model as a whole
Providing a global understanding
of the model by explaining a set 
of individual instances

--------------------------------------------------------------------------------
[End of Page 41]

42
Submodular Pick for Explaining Models
Single explanation is not 
sufficient to evaluate and assess 
trust in the model as a whole
Providing a global understanding
of the model by explaining a set 
of individual instances
How to select these 
instances judiciously?

--------------------------------------------------------------------------------
[End of Page 42]

43
Submodular Pick for Explaining Models
â€¢
Budget ğµ: the number of explanations users are willing to look at in order to understand a model
A set of instances ğ‘‹
select
ğµinstances (diverse, representative)

--------------------------------------------------------------------------------
[End of Page 43]

44
Submodular Pick for Explaining Models
â€¢
Budget ğµ: the number of explanations users are willing to look at in order to understand a model
A set of instances ğ‘‹
select
ğµinstances (diverse, representative)
ïƒ˜Submodular pick (SP) algorithm
ğ‘¤1,1 ğ‘¤1,2
ğ‘¤2,2 ğ‘¤2,3
ğ‘¤3,2 ğ‘¤3,3
ğ‘¤4,2
ğ‘¤4,4
ğ‘¤5,4 ğ‘¤5,5
ğ’™(1)
Explanation matrix ğ‘Š
ğ’™(2)
ğ’™(3)
ğ’™(4)
ğ’™(5)
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
ğ‘¥5
-
Each row represents an instance
-
Each column represents a feature
-
Each value represents a local importance
ğ’™(1) contains two features

--------------------------------------------------------------------------------
[End of Page 44]

45
Submodular Pick for Explaining Models
â€¢
Budget ğµ: the number of explanations users are willing to look at in order to understand a model
A set of instances ğ‘‹
select
ğµinstances (diverse, representative)
ïƒ˜Submodular pick (SP) algorithm
ğ‘¤1,1 ğ‘¤1,2
ğ‘¤2,2 ğ‘¤2,3
ğ‘¤3,2 ğ‘¤3,3
ğ‘¤4,2
ğ‘¤4,4
ğ‘¤5,4 ğ‘¤5,5
ğ’™(1)
Explanation matrix ğ‘Š
ğ’™(2)
ğ’™(3)
ğ’™(4)
ğ’™(5)
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
ğ‘¥5
ï±Select instances that cover 
important features (ğ‘¥2)
ğ¼2: global importance of ğ‘¥2

--------------------------------------------------------------------------------
[End of Page 45]

46
Submodular Pick for Explaining Models
â€¢
Budget ğµ: the number of explanations users are willing to look at in order to understand a model
A set of instances ğ‘‹
select
ğµinstances (diverse, representative)
ïƒ˜Submodular pick (SP) algorithm
ğ‘¤1,1 ğ‘¤1,2
ğ‘¤2,2 ğ‘¤2,3
ğ‘¤3,2 ğ‘¤3,3
ğ‘¤4,2
ğ‘¤4,4
ğ‘¤5,4 ğ‘¤5,5
ğ’™(1)
Explanation matrix ğ‘Š
ğ’™(2)
ğ’™(3)
ğ’™(4)
ğ’™(5)
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
ğ‘¥5
ï±Select instances that cover 
important features (ğ‘¥2)
ï±Avoid selecting instances with similar 
explanations (redundant features)
If ğ’™(2) is selected, 
there is no need to 
select ğ’™(3)

--------------------------------------------------------------------------------
[End of Page 46]

47
Submodular Pick for Explaining Models
â€¢
Budget ğµ: the number of explanations users are willing to look at in order to understand a model
A set of instances ğ‘‹
select
ğµinstances (diverse, representative)
ïƒ˜Submodular pick (SP) algorithm
ğ‘¤1,1 ğ‘¤1,2
ğ‘¤2,2 ğ‘¤2,3
ğ‘¤3,2 ğ‘¤3,3
ğ‘¤4,2
ğ‘¤4,4
ğ‘¤5,4 ğ‘¤5,5
ğ’™(1)
Explanation matrix ğ‘Š
ğ’™(2)
ğ’™(3)
ğ’™(4)
ğ’™(5)
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
ğ‘¥5
ï±Select instances that cover 
important features (ğ‘¥2)
ï±Avoid selecting instances with similar 
explanations (redundant features)
ï±Select less instances, while covering 
more features (ğ’™(2) and ğ’™(5))

--------------------------------------------------------------------------------
[End of Page 47]

48
Submodular Pick for Explaining Models
ïƒ˜Submodular pick (SP) algorithm
Algorithm Submodular pick (SP) algorithm
Require: Instances ğ‘‹, Budget ğµ
for all ğ’™(ğ‘–) in ğ‘‹do
ğ’˜(ğ‘–) â†ğ¿ğ¼ğ‘€ğ¸(ğ’™(ğ‘–))
Construct the explanation matrix

--------------------------------------------------------------------------------
[End of Page 48]

49
Submodular Pick for Explaining Models
ïƒ˜Submodular pick (SP) algorithm
Algorithm Submodular pick (SP) algorithm
Require: Instances ğ‘‹, Budget ğµ
for all ğ’™(ğ‘–) in ğ‘‹do
ğ’˜(ğ‘–) â†ğ¿ğ¼ğ‘€ğ¸(ğ’™(ğ‘–))
Construct the explanation matrix
for ğ‘—âˆˆ1, â‹¯, ğ‘do
ğ¼ğ‘—â†
à·
ğ‘–=1
ğ‘‹
ğ‘¤ğ‘–,ğ‘—
Compute global feature importance

--------------------------------------------------------------------------------
[End of Page 49]

50
Submodular Pick for Explaining Models
ïƒ˜Submodular pick (SP) algorithm
Algorithm Submodular pick (SP) algorithm
Require: Instances ğ‘‹, Budget ğµ
for all ğ’™(ğ‘–) in ğ‘‹do
ğ’˜(ğ‘–) â†ğ¿ğ¼ğ‘€ğ¸(ğ’™(ğ‘–))
Construct the explanation matrix
for ğ‘—âˆˆ1, â‹¯, ğ‘do
ğ¼ğ‘—â†
à·
ğ‘–=1
ğ‘‹
ğ‘¤ğ‘–,ğ‘—
Compute global feature importance
ğ‘‰â†{}
while ğ‘‰< ğµdo
ğ‘‰â†ğ‘‰â‹ƒğ’™(ğ‘–âˆ—)
Return ğ‘‰
ğ‘–âˆ—= argmax
ğ‘–
ğ‘ğ‘‰â‹ƒğ’™ğ‘–, ğ‘Š, ğ¼
c ğ‘‰, ğ‘Š, ğ¼= Ïƒğ‘—=1
ğ‘
1 âˆƒğ‘–âˆˆğ‘‰:ğ‘¤ğ‘–,ğ‘—>0 ğ¼ğ‘—
Greedily add examples that maximize the coverage gain 
The coverage computes the total global importance of the 
features that appear in at least one instance in a set V

--------------------------------------------------------------------------------
[End of Page 50]

51
SP-LIME
â€¢ Compute global feature importance based on local feature importance 
from LIME
â€¢ Provide a global understanding of the model by selecting a set of 
representative instances

--------------------------------------------------------------------------------
[End of Page 51]

52
Question?

--------------------------------------------------------------------------------
[End of Page 52]

53
Perturbation-based methods
â€¢ LIME 
â€¢ SHAP
(Ribeiro et al., KDD, 2016)
(Lundberg and Lee, NIPS, 2017)

--------------------------------------------------------------------------------
[End of Page 53]

54
Black Box
Input
Output
ğ’™
ğ‘¦
Explaining Black-box Model
ğ‘¥1
ğ‘¥2
ğ‘¥ğ‘›
â‹¯
(Prediction probability ğ‘ƒğ‘¦)

--------------------------------------------------------------------------------
[End of Page 54]

55
Black Box
Input
Output
ğ’™
ğ‘¦
Explaining Black-box Model
ğ‘¥1
ğ‘¥2
ğ‘¥ğ‘›
â‹¯
(Prediction probability ğ‘ƒğ‘¦)
ğ‘¥1
ğ‘¥2
ğ‘¥ğ‘›
â‹¯
ğ‘ƒğ‘¦â€²
Importance of ğ‘¥ğ‘–
ğ‘ƒğ‘¦âˆ’ğ‘ƒğ‘¦â€²

--------------------------------------------------------------------------------
[End of Page 55]

56
Black Box
Input
Output
ğ’™
ğ‘¦
Explaining Black-box Model
ğ‘¥1
ğ‘¥2
ğ‘¥ğ‘›
â‹¯
(Prediction probability ğ‘ƒğ‘¦)
ğ‘¥1
ğ‘¥2
ğ‘¥ğ‘›
â‹¯
ğ‘ƒğ‘¦â€²
Importance of ğ‘¥ğ‘–
ğ‘ƒğ‘¦âˆ’ğ‘ƒğ‘¦â€²
ğ‘¥1
ğ‘¥2
ğ‘¥ğ‘›
â‹¯
ğ‘ƒğ‘¦â€²â€²
ğ‘ƒğ‘¦âˆ’ğ‘ƒğ‘¦â€²â€²

--------------------------------------------------------------------------------
[End of Page 56]

57
Black Box
Input
Output
ğ’™
ğ‘¦
Explaining Black-box Model
ğ‘¥1
ğ‘¥2
ğ‘¥ğ‘›
â‹¯
(Prediction probability ğ‘ƒğ‘¦)
ğ‘¥1
ğ‘¥2
ğ‘¥ğ‘›
â‹¯
ğ‘ƒğ‘¦â€²
Importance of ğ‘¥ğ‘–
ğ‘ƒğ‘¦âˆ’ğ‘ƒğ‘¦â€²
ğ‘¥1
ğ‘¥2
ğ‘¥ğ‘›
â‹¯
ğ‘ƒğ‘¦â€²â€²
ğ‘ƒğ‘¦âˆ’ğ‘ƒğ‘¦â€²â€²
â‹¯
â‹¯
â‹¯
[Leave-one-out, (Li et al., 2016)]

--------------------------------------------------------------------------------
[End of Page 57]

58
Leave-one-out
â€¢ Sentiment classification
Text
Confidence
Word importance
The movie is interesting 
0.98
The movie is interesting 
0.95
The
The movie is interesting 
0.87
movie
The movie is interesting 
0.96
is
The movie is interesting 
0.61
interesting
Model prediction: positive
0.03
0.11
0.02
0.37

--------------------------------------------------------------------------------
[End of Page 58]

59
â€¢ Leave ONE feature out at each step 
Text
Confidence
Word importance
The movie is interesting and impressive  
0.97
0.95
0.96
interesting
The movie is interesting and impressive  
The movie is interesting and impressive  
impressive
Leave-one-out
Feature importance may be misleading
0.02
0.01

--------------------------------------------------------------------------------
[End of Page 59]

60
â€¢ Leave ONE feature out at each step 
Text
Confidence
Word importance
The movie is interesting and impressive  
0.97
0.95
0.96
interesting
The movie is interesting and impressive  
The movie is interesting and impressive  
impressive
Leave-one-out
Feature importance may be misleading
0.02
0.01
Need a better way to quantify 
feature importance

--------------------------------------------------------------------------------
[End of Page 60]

61
SHAP
A unified approach to interpreting model predictions
Scott M. Lundberg, Su-In Lee
(NIPS, 2017)

--------------------------------------------------------------------------------
[End of Page 61]

62
SHAP
â€¢ Shapley value 
[Shapley, 1953]
Game
Payoff
Player 1
Player 2
Player 3
Player 4

--------------------------------------------------------------------------------
[End of Page 62]

63
SHAP
â€¢ Shapley value 
[Shapley, 1953]
Game
Payoff
Player 1
Player 2
Player 3
Player 4
?

--------------------------------------------------------------------------------
[End of Page 63]

64
SHAP
â€¢ Shapley value 
[Shapley, 1953]
Coalitions
Payoff
â‹¯
(23)
ğ‘ƒ1
ğ‘ƒ2
ğ‘ƒ3
ğ‘ƒ4
ğ‘ƒ5
â‹¯

--------------------------------------------------------------------------------
[End of Page 64]

65
SHAP
â€¢ Shapley value 
[Shapley, 1953]
Coalitions
â‹¯
(23)
ğ‘ƒ1
ğ‘ƒ2
ğ‘ƒ3
ğ‘ƒ4
ğ‘ƒ5
â‹¯
ğ‘ƒ1â€²
ğ‘ƒ2â€²
ğ‘ƒ3â€²
ğ‘ƒ4â€²
ğ‘ƒ5â€²
Payoff

--------------------------------------------------------------------------------
[End of Page 65]

66
SHAP
â€¢ Shapley value 
[Shapley, 1953]
Coalitions
â‹¯
(23)
ğ‘ƒ1
ğ‘ƒ2
ğ‘ƒ3
ğ‘ƒ4
ğ‘ƒ5
â‹¯
ğ‘ƒ1â€²
ğ‘ƒ2â€²
ğ‘ƒ3â€²
ğ‘ƒ4â€²
ğ‘ƒ5â€²
Payoff
Marginal contribution
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ†ğ‘ƒ1
âˆ†ğ‘ƒ2
âˆ†ğ‘ƒ3
âˆ†ğ‘ƒ4
âˆ†ğ‘ƒ5

--------------------------------------------------------------------------------
[End of Page 66]

67
SHAP
â€¢ Shapley value 
[Shapley, 1953]
Coalitions
â‹¯
ğ‘ƒ1
ğ‘ƒ2
ğ‘ƒ3
ğ‘ƒ4
ğ‘ƒ5
â‹¯
ğ‘ƒ1â€²
ğ‘ƒ2â€²
ğ‘ƒ3â€²
ğ‘ƒ4â€²
ğ‘ƒ5â€²
Payoff
Marginal contribution
âˆ’
âˆ’
âˆ’
âˆ’
âˆ’
âˆ†ğ‘ƒ1
âˆ†ğ‘ƒ2
âˆ†ğ‘ƒ3
âˆ†ğ‘ƒ4
âˆ†ğ‘ƒ5
Contribution= Ïƒ âˆ†ğ‘ƒğ‘–
(23)

--------------------------------------------------------------------------------
[End of Page 67]

68
SHAP
â€¢ Shapley value 
[Shapley, 1953]
Game
Payoff
Player 1
Player 2
Player 3
Player 4

--------------------------------------------------------------------------------
[End of Page 68]

69
SHAP
â€¢ Shapley value 
[Shapley, 1953]
Model ğ‘“
Prediction
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
ğœ™1
ğœ™2
ğœ™3
ğœ™4

--------------------------------------------------------------------------------
[End of Page 69]

70
SHAP
â€¢ Shapley value 
[Shapley, 1953]
ğœ™ğ‘–=
à·
ğ‘†âŠ†ğ¹\{ğ‘–}
ğ‘†!
ğ¹âˆ’ğ‘†âˆ’1 !
ğ¹!
ğ‘“ğ‘†âˆª{ğ‘–} ğ‘¥ğ‘†âˆª{ğ‘–} âˆ’ğ‘“ğ‘†ğ‘¥ğ‘†
ğ¹
ğ‘†
ğ‘–
1
â‹¯
Marginal contribution of ğ‘¥ğ‘–given ğ‘†
ğ¹\{ğ‘–}
2
3
4

--------------------------------------------------------------------------------
[End of Page 70]

71
SHAP
â€¢ Shapley value 
[Shapley, 1953]
Weighted by the permutations of features
ğ¹
ğ‘†
ğ‘–
1
â‹¯
ğ¹\{ğ‘–}
2
3
4
ğœ™ğ‘–=
à·
ğ‘†âŠ†ğ¹\{ğ‘–}
ğ‘†!
ğ¹âˆ’ğ‘†âˆ’1 !
ğ¹!
ğ‘“ğ‘†âˆª{ğ‘–} ğ‘¥ğ‘†âˆª{ğ‘–} âˆ’ğ‘“ğ‘†ğ‘¥ğ‘†
ğ¹!
ğ¹âˆ’ğ‘†âˆ’1 !
ğ‘†!

--------------------------------------------------------------------------------
[End of Page 71]

72
SHAP
â€¢ SHapley Additive exPlanation (SHAP) 
ğ‘”ğ‘§â€² â‰ˆğ‘“â„ğ‘¥ğ‘§â€²
ğ‘”ğ‘§â€² = ğœ™0 + à·
ğ‘–=1
ğ‘
ğœ™ğ‘–ğ‘§ğ‘–â€²
ğ‘§â€² â‰ˆğ‘¥â€²
ğ‘¥= â„ğ‘¥ğ‘¥â€²
Interpretable input
Additive feature attribution method
Original input

--------------------------------------------------------------------------------
[End of Page 72]

73
SHAP
â€¢ SHapley Additive exPlanation (SHAP) 
ğ‘”ğ‘§â€² â‰ˆğ‘“â„ğ‘¥ğ‘§â€²
ğ‘”ğ‘§â€² = ğœ™0 + à·
ğ‘–=1
ğ‘
ğœ™ğ‘–ğ‘§ğ‘–â€²
ğ‘§â€² â‰ˆğ‘¥â€²
ğ‘¥= â„ğ‘¥ğ‘¥â€²
Interpretable input
Additive feature attribution method
Original input
LIME is a special case, but not optimal
ğ‘”ğ‘§â€² = à·
ğ‘–=1
ğ‘
ğ‘¤ğ‘–ğ‘§ğ‘–â€²

--------------------------------------------------------------------------------
[End of Page 73]

74
SHAP
â€¢ SHapley Additive exPlanation (SHAP) 
ğ‘”ğ‘§â€² â‰ˆğ‘“â„ğ‘¥ğ‘§â€²
ğ‘”ğ‘§â€² = ğœ™0 + à·
ğ‘–=1
ğ‘
ğœ™ğ‘–ğ‘§ğ‘–â€²
ğ‘§â€² â‰ˆğ‘¥â€²
ğ‘¥= â„ğ‘¥ğ‘¥â€²
Interpretable input
Additive feature attribution method
Original input
ï±Property 1: Local accuracy
ğ‘“ğ‘¥= ğ‘”ğ‘¥â€² = ğœ™0 + à·
ğ‘–=1
ğ‘
ğœ™ğ‘–ğ‘¥ğ‘–â€²
ğœ™0 = â„ğ‘¥0

--------------------------------------------------------------------------------
[End of Page 74]

75
SHAP
â€¢ SHapley Additive exPlanation (SHAP) 
ğ‘”ğ‘§â€² â‰ˆğ‘“â„ğ‘¥ğ‘§â€²
ğ‘”ğ‘§â€² = ğœ™0 + à·
ğ‘–=1
ğ‘
ğœ™ğ‘–ğ‘§ğ‘–â€²
ğ‘§â€² â‰ˆğ‘¥â€²
ğ‘¥= â„ğ‘¥ğ‘¥â€²
Interpretable input
Additive feature attribution method
Original input
ï±Property 2: Missingness
ğ‘¥ğ‘–
â€² = 0
âŸ¹
ğœ™ğ‘–= 0
Missingness constrains features missing in the 
original input to have no attributed impact

--------------------------------------------------------------------------------
[End of Page 75]

76
SHAP
â€¢ SHapley Additive exPlanation (SHAP) 
ğ‘”ğ‘§â€² â‰ˆğ‘“â„ğ‘¥ğ‘§â€²
ğ‘”ğ‘§â€² = ğœ™0 + à·
ğ‘–=1
ğ‘
ğœ™ğ‘–ğ‘§ğ‘–â€²
ğ‘§â€² â‰ˆğ‘¥â€²
ğ‘¥= â„ğ‘¥ğ‘¥â€²
Interpretable input
Additive feature attribution method
Original input
ï±Property 3: Consistency
For any two models ğ‘“1 and ğ‘“2, if ğ‘“1 â„ğ‘¥ğ‘§â€²
âˆ’ğ‘“1 â„ğ‘¥ğ‘§â€²\ğ‘–
â‰¥ğ‘“2 â„ğ‘¥ğ‘§â€²
âˆ’ğ‘“2 â„ğ‘¥ğ‘§â€²\ğ‘–
ğ‘§ğ‘–
â€² = 0
for all inputs ğ‘§â€² âˆˆ0, 1 ğ‘, then ğœ™ğ‘–ğ‘“1, ğ‘¥â‰¥ğœ™ğ‘–ğ‘“2, ğ‘¥

--------------------------------------------------------------------------------
[End of Page 76]

77
SHAP
â€¢ SHapley Additive exPlanation (SHAP) 
ğ‘”ğ‘§â€² â‰ˆğ‘“â„ğ‘¥ğ‘§â€²
ğ‘”ğ‘§â€² = ğœ™0 + à·
ğ‘–=1
ğ‘
ğœ™ğ‘–ğ‘§ğ‘–â€²
ğ‘§â€² â‰ˆğ‘¥â€²
ğ‘¥= â„ğ‘¥ğ‘¥â€²
Interpretable input
Additive feature attribution method
Original input
Only Shapley value satisfies all the three properties
ğœ™ğ‘–ğ‘“, ğ‘¥= à·
ğ‘§â€²âŠ†ğ‘¥â€²
ğ‘§â€² ! ğ‘âˆ’ğ‘§â€² âˆ’1 !
ğ‘!
ğ‘“â„ğ‘¥ğ‘§â€²
âˆ’ğ‘“â„ğ‘¥ğ‘§â€²\ğ‘–
Contains a subset of non-zero entries in ğ‘¥â€²

--------------------------------------------------------------------------------
[End of Page 77]

78
SHAP
ğœ™ğ‘–ğ‘“, ğ‘¥= à·
ğ‘§â€²âŠ†ğ‘¥â€²
ğ‘§â€² ! ğ‘âˆ’ğ‘§â€² âˆ’1 !
ğ‘!
ğ‘“â„ğ‘¥ğ‘§â€²
âˆ’ğ‘“â„ğ‘¥ğ‘§â€²\ğ‘–
Challenge
Computational complexity
ğ‘‚(2ğ‘›)
â€¢ SHapley Additive exPlanation (SHAP) 

--------------------------------------------------------------------------------
[End of Page 78]

79
SHAP
â€¢ SHapley Additive exPlanation (SHAP) 
Model-agnostic approximations
Model-type-specific approximations
-
Shapley sampling values
-
Kernel SHAP
-
Linear SHAP
-
Low-Order SHAP
-
Max SHAP
-
Deep SHAP

--------------------------------------------------------------------------------
[End of Page 79]

80
SHAP
â€¢ SHapley Additive exPlanation (SHAP) 
Model-agnostic approximations
Model-type-specific approximations
-
Shapley sampling values
-
Kernel SHAP
-
Linear SHAP
-
Low-Order SHAP
-
Max SHAP
-
Deep SHAP
Initialize the number of samples ğ‘€
ğœ™ğ‘–âŸµ0
for ğ‘šâˆˆ1, â‹¯, ğ‘€do
ğœ™ğ‘–âŸµğœ™ğ‘–+ ğ‘§â€² ! ğ‘âˆ’ğ‘§â€² âˆ’1 !
ğ‘!
ğ‘“â„ğ‘¥ğ‘§â€²
âˆ’ğ‘“â„ğ‘¥ğ‘§â€²\ğ‘–
Sample ğ‘§â€² âŠ†ğ‘¥â€²

--------------------------------------------------------------------------------
[End of Page 80]

81
SHAP
â€¢ SHapley Additive exPlanation (SHAP) 
Model-agnostic approximations
Model-type-specific approximations
-
Shapley sampling values
-
Kernel SHAP
-
Linear SHAP
-
Low-Order SHAP
-
Max SHAP
-
Deep SHAP
Î© ğ‘”= 0
ğœ‹ğ‘¥â€² ğ‘§â€² =
(ğ‘âˆ’1)
(ğ‘ğ‘â„ğ‘œğ‘œğ‘ ğ‘’ğ‘§â€² ) ğ‘§â€² (ğ‘âˆ’ğ‘§â€² )
â„’ğ‘“, ğ‘”= à·ğœ‹ğ‘¥â€² ğ‘§â€² (ğ‘“â„ğ‘¥ğ‘§â€²
âˆ’ğ‘”(ğ‘§â€²))2
Linear LIME + Shapley values
The solutions would be consistent with properties 1-3

--------------------------------------------------------------------------------
[End of Page 81]

82
SHAP
â€¢ SHapley Additive exPlanation (SHAP) 
Model-agnostic approximations
Model-type-specific approximations
-
Shapley sampling values
-
Kernel SHAP
-
Linear SHAP
-
Low-Order SHAP
-
Max SHAP
-
Deep SHAP
Faster model-specific methods
SHAP values can be approximated directly 
from the modelâ€™s weight coefficients

--------------------------------------------------------------------------------
[End of Page 82]

83
Question?

--------------------------------------------------------------------------------
[End of Page 83]

84
Reference
â€¢
Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. "" Why should i trust you?" Explaining the predictions 
of any classifier." Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and 
data mining. 2016.
â€¢
Lundberg, Scott M., and Su-In Lee. "A unified approach to interpreting model predictions." Proceedings of the 
31st international conference on neural information processing systems. 2017.
â€¢
Li, Jiwei, Will Monroe, and Dan Jurafsky. "Understanding neural networks through representation 
erasure." arXiv preprint arXiv:1612.08220 (2016).
â€¢
Lloyd S Shapley. 1953. A value for n-person games. Contributions to the Theory of Games, 2(28).

--------------------------------------------------------------------------------
[End of Page 84]