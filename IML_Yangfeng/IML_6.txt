CS 4501/6501 Interpretable Machine Learning
Hanjie Chen, Yangfeng Ji
Department of Computer Science
University of Virginia
{hc9mx, yangfeng}@virginia.edu
1
Post-hoc explanations: gradient/attention-based methods

--------------------------------------------------------------------------------
[End of Page 1]

2
Explaining Black-box Model
Black Box
Input
Output
ğ’™
ğ’š
Perturbation-based methods
Interpreter
â€¢
Model-agnostic (black-box)
â€¢
Perturbing the input and observing 
model prediction change
â€¢
Extracting relationships between 
input features and the output

--------------------------------------------------------------------------------
[End of Page 2]

3
Explaining Black-box Model
Black Box
Input
Output
ğ’™
ğ’š
Perturbation-based methods
Interpreter
â€¢
Model-agnostic (black-box)
â€¢
Perturbing the input and observing 
model prediction change
â€¢
Extracting relationships between 
input features and the output
â€¢
Applicable to any black-box models
â€¢
Computational complexity

--------------------------------------------------------------------------------
[End of Page 3]

4
Explaining Black-box Model
Model
Input
Output
ğ’™
ğ’š
Additional information from the model
Interpreter
â€¢
Model-dependent (white-box)
â€¢
Additional information: gradients, 
attentions
â€¢
Simple, fast, efficient
â€¢
Not applicable if no such 
information available
(gradients, attentions)

--------------------------------------------------------------------------------
[End of Page 4]

5
Explaining Black-box Model
â€¢ Gradient-based methods
â€¢ Attention-based methods

--------------------------------------------------------------------------------
[End of Page 5]

6
Gradient-based Explanation
The gradient of a function ğ‘“on ğ’™âˆˆâ„! is
âˆ‡ğ‘“ğ’™=
ğœ•ğ‘“
ğœ•ğ‘¥"
â‹®
ğœ•ğ‘“
ğœ•ğ‘¥!
Source: https://towardsdatascience.com/basics-gradient-input-as-explanation-bca79bb80de0

--------------------------------------------------------------------------------
[End of Page 6]

7
Gradient-based Explanation
The gradient of a function ğ‘“on ğ’™âˆˆâ„! is
âˆ‡ğ‘“ğ’™=
ğœ•ğ‘“
ğœ•ğ‘¥"
â‹®
ğœ•ğ‘“
ğœ•ğ‘¥!
Source: https://towardsdatascience.com/basics-gradient-input-as-explanation-bca79bb80de0
The derivative #$
#%! indicates how 
much ğ‘“will change when ğ‘¥&
increases a little bit

--------------------------------------------------------------------------------
[End of Page 7]

8
Gradient-based Explanation
Model ğ‘“
â‹®
ğ‘¥"
ğ‘¥'
ğ‘¥!
ğ’™
ğ‘“ğ’™
Gradient
ğœ•ğ‘“
ğœ•ğ‘¥"
ğœ•ğ‘“
ğœ•ğ‘¥!
â‹®
â‹®

--------------------------------------------------------------------------------
[End of Page 8]

9
Gradient-based Explanation
Model ğ‘“
â‹®
ğ‘¥"
ğ‘¥'
ğ‘¥!
ğ’™
ğ‘“ğ’™
Gradient
ğœ•ğ‘“
ğœ•ğ‘¥"
ğœ•ğ‘“
ğœ•ğ‘¥!
â‹®
â‹®
Feature importance
The influence of â€œtiny changeâ€ to the 
feature on the model prediction

--------------------------------------------------------------------------------
[End of Page 9]

10
Gradient-based Explanation
Model ğ‘“
â‹®
ğ‘¥"
ğ‘¥'
ğ‘¥!
ğ’™
ğ‘“ğ’™
Gradient
ğœ•ğ‘“
ğœ•ğ‘¥"
ğœ•ğ‘“
ğœ•ğ‘¥!
â‹®
â‹®
Ã¼ One backpropagation
Ã¼ Simple, fast

--------------------------------------------------------------------------------
[End of Page 10]

11
Gradient-based Explanation
ğ‘¥"
ğ‘¥'
ğ’™
ğ‘¥" is more important than ğ‘¥'
âˆ‡!!ğ‘“
âˆ‡!"ğ‘“
Ã¼ Changing ğ‘¥! can flip the model prediction
Ã¼ Changing ğ‘¥" would not influence the 
model prediction

--------------------------------------------------------------------------------
[End of Page 11]

12
Gradient-based Explanation
Problem 1: saturated outputs lead to unintuitive gradients
(Shrikumar et al., 2017)
ğ‘¦= ,ğ‘¥" + ğ‘¥',
ğ‘¤â„ğ‘’ğ‘›ğ‘¥" + ğ‘¥' < 1
1,
ğ‘¤â„ğ‘’ğ‘›ğ‘¥" + ğ‘¥' â‰¥1
ğ‘¥" + ğ‘¥'
ğ‘¦
0
1
2
1
ğ‘¥! = 1, ğ‘¥" = 1
The gradient on ğ‘¥! or ğ‘¥" is 
0, but that does not mean 
neither is important

--------------------------------------------------------------------------------
[End of Page 12]

13
Gradient-based Explanation
Problem 2: discontinuous gradients (e.g., thresholding) are problematic
(Shrikumar et al., 2017)
ğ‘¥
ğ‘¦
0
10
ğ‘¦= ğ‘šğ‘ğ‘¥0, ğ‘¥âˆ’10
The gradient changes dramatically

--------------------------------------------------------------------------------
[End of Page 13]

14
Gradient-based Explanation
Problem 2: discontinuous gradients (e.g., thresholding) are problematic
(Shrikumar et al., 2017)
ğ‘¥
ğ‘¦
0
10
ğ‘¦= ğ‘šğ‘ğ‘¥0, ğ‘¥âˆ’10
The gradient changes dramatically
Need to replace â€œReluâ€ with 
â€œSoftplusâ€ activation

--------------------------------------------------------------------------------
[End of Page 14]

15
Gradient-based Explanation
Problem 3: input gradient is sensitive to slight perturbations
ğ‘¥"
ğ‘¥'
ğ’™

--------------------------------------------------------------------------------
[End of Page 15]

16
Gradient-based Explanation
Problem 3: input gradient is sensitive to slight perturbations
(Smilkov et al., 2017)
Input gradients are misleading, resulting in a noisy saliency map

--------------------------------------------------------------------------------
[End of Page 16]

17
Gradient-based Explanation
Do NOT rely on a single gradient calculation
(Smilkov et al., 2017)
â€¢ SmoothGrad: add gaussian noise to 
inputs and average the gradients
Source: EMNLP 2020 Tutorial on Interpreting Predictions of NLP Models

--------------------------------------------------------------------------------
[End of Page 17]

18
Gradient-based Explanation
Do NOT rely on a single gradient calculation
(Smilkov et al., 2017)
â€¢ SmoothGrad: add gaussian noise to 
inputs and average the gradients
Source: EMNLP 2020 Tutorial on Interpreting Predictions of NLP Models
â€¢ Integrated Gradients: average gradients 
along a path from baseline to the input
(Sundararajan et al., 2017)

--------------------------------------------------------------------------------
[End of Page 18]

19
Gradient-based Explanation
Do NOT rely on a single gradient calculation
(Smilkov et al., 2017)
â€¢ SmoothGrad: add gaussian noise to 
inputs and average the gradients
Source: EMNLP 2020 Tutorial on Interpreting Predictions of NLP Models
â€¢ Integrated Gradients: average gradients 
along a path from baseline to the input
(Sundararajan et al., 2017)

--------------------------------------------------------------------------------
[End of Page 19]

20
IG
Axiomatic Attribution for Deep Networks
Mukund Sundararajan, Ankur Taly, Qiqi Yan
(ICML, 2017)

--------------------------------------------------------------------------------
[End of Page 20]

21
Two Fundamental Axioms
â€¢ Sensitivity
For every input and baseline that differ in one feature but have different 
predictions then the differing feature should be given a non-zero attribution
Input
a
clever
piece
of
cinema
ğ’™!
ğ’™"
ğ’™#
ğ’™$
ğ’™%
Baseline
a
clever
piece
of
cinema
ğ’™!
ğ’™#
ğ’™$
ğ’™%
Prediction
Positive
Negative
ğ‘" = 0.46
clever
Attribution
(ğ‘" â‰ 0)

--------------------------------------------------------------------------------
[End of Page 21]

22
Two Fundamental Axioms
â€¢ Sensitivity
Gradients violate Sensitivity
ğ‘¦= ,ğ‘¥,
ğ‘¤â„ğ‘’ğ‘›ğ‘¥< 1
1,
ğ‘¤â„ğ‘’ğ‘›ğ‘¥â‰¥1
ğ‘¥
ğ‘¦
0
1
2
1
Input
ğ‘¥= 2
Baseline
ğ‘¥= 0
The output changes 1, while the 
gradient method gives attribution 
of 0 to ğ‘¥
Output
ğ‘¦= 1
ğ‘¦= 0

--------------------------------------------------------------------------------
[End of Page 22]

23
Two Fundamental Axioms
â€¢ Implementation invariance
The attributions are always identical for two functionally equivalent networks
The outputs of two networks are 
equal for all inputs, despite having 
very different implementations
ğ‘“â„" ğ‘¥
= ğ‘“â„' ğ‘¥

--------------------------------------------------------------------------------
[End of Page 23]

24
Two Fundamental Axioms
â€¢ Implementation invariance
The attributions are always identical for two functionally equivalent networks
Gradients are invariant to implementation
The chain-rule for gradients is essentially about implementation invariance: 
ğœ•ğ‘“
ğœ•ğ‘¥= ğœ•ğ‘“
ğœ•â„; ğœ•â„
ğœ•ğ‘”; ğœ•ğ‘”
ğœ•ğ‘¥
ğ‘¥
ğ‘“ğ‘¥
ğ‘”
â„

--------------------------------------------------------------------------------
[End of Page 24]

25
Two Fundamental Axioms
â€¢ Implementation invariance
The attributions are always identical for two functionally equivalent networks
Gradients are invariant to implementation
The chain-rule for gradients is essential for implementation invariance: 
ğœ•ğ‘“
ğœ•ğ‘¥= ğœ•ğ‘“
ğœ•â„; ğœ•â„
ğœ•ğ‘”; ğœ•ğ‘”
ğœ•ğ‘¥
ğ‘¥
ğ‘“ğ‘¥
ğ‘”
â„

--------------------------------------------------------------------------------
[End of Page 25]

26
Two Fundamental Axioms
â€¢ Implementation invariance
The attributions are always identical for two functionally equivalent networks
Gradients are invariant to implementation
The chain-rule for gradients is essential for implementation invariance: 
ğœ•ğ‘“
ğœ•ğ‘¥= ğœ•ğ‘“
ğœ•â„; ğœ•â„
ğœ•ğ‘”; ğœ•ğ‘”
ğœ•ğ‘¥
ğ‘¥
ğ‘“ğ‘¥
ğ‘”
â„
Some methods (e.g., LRP and DeepLift) do 
not satisfy the implementation invariance 

--------------------------------------------------------------------------------
[End of Page 26]

27
IG
â€¢ Integrated Gradients
ğ‘“: neural network
ğ’™âˆˆâ„!: input
ğ’™â€² âˆˆâ„! : baseline
(e.g., black image, zero 
embedding vector)
ğ’™â€²
ğ’™
Get samples along the straight line from ğ’™â€² to ğ’™
ğ’™& + ğ›¼ğ’™âˆ’ğ’™â€™
ğ›¼âˆˆ0, 1

--------------------------------------------------------------------------------
[End of Page 27]

28
IG
â€¢ Integrated Gradients
ğ‘“: neural network
(e.g., black image, zero 
embedding vector)
ğ’™â€²
ğ’™
Compute gradients at all points along the path 
ğ’™& + ğ›¼ğ’™âˆ’ğ’™â€™
ğ›¼âˆˆ0, 1
ğ’™âˆˆâ„!: input
ğ’™â€² âˆˆâ„! : baseline

--------------------------------------------------------------------------------
[End of Page 28]

29
IG
â€¢ Integrated Gradients
ğ‘“: neural network
(e.g., black image, zero 
embedding vector)
ğ’™â€²
ğ’™
Cumulate these gradients
ğ’™& + ğ›¼ğ’™âˆ’ğ’™â€™
ğ›¼âˆˆ0, 1
ğ¼ğº' ğ’™= ğ‘¥' âˆ’ğ‘¥'â€² Ã— 7
()*
!
ğœ•ğ‘“ğ’™& + ğ›¼ğ’™âˆ’ğ’™â€™
ğœ•ğ‘¥'
ğ‘‘ğ›¼
On the ğ‘–+, dimension
ğ’™âˆˆâ„!: input
ğ’™â€² âˆˆâ„! : baseline

--------------------------------------------------------------------------------
[End of Page 29]

30
IG
â€¢ Integrated Gradients
Axiom: completeness
The attributions add up to the difference between the output of ğ‘“at the input ğ’™
and the baseline ğ’™â€²
<
')!
-
ğ¼ğº' ğ’™= ğ‘“ğ’™âˆ’ğ‘“ğ’™â€²

--------------------------------------------------------------------------------
[End of Page 30]

31
IG
â€¢ Integrated Gradients
Axiom: completeness
The attributions add up to the difference between the output of ğ‘“at the input ğ’™
and the baseline ğ’™â€²
<
')!
-
ğ¼ğº' ğ’™= ğ‘“ğ’™âˆ’ğ‘“ğ’™â€²
Sensitivity: for every input and 
baseline that differ in one feature 
but have different predictions 
then the differing feature should 
be given a non-zero attribution

--------------------------------------------------------------------------------
[End of Page 31]

32
IG
â€¢ Integrated Gradients
Axiom: completeness
The attributions add up to the difference between the output of ğ‘“at the input ğ’™
and the baseline ğ’™â€²
<
')!
-
ğ¼ğº' ğ’™= ğ‘“ğ’™âˆ’ğ‘“ğ’™â€²
Sensitivity: for every input and 
baseline that differ in one feature 
but have different predictions 
then the differing feature should 
be given a non-zero attribution
Sensitivity
Implementation invariance

--------------------------------------------------------------------------------
[End of Page 32]

33
IG
â€¢ Integrated Gradients
Axiom: completeness
The attributions add up to the difference between the output of ğ‘“at the input ğ’™
and the baseline ğ’™â€²
<
')!
-
ğ¼ğº' ğ’™= ğ‘“ğ’™âˆ’ğ‘“ğ’™â€²
ğ‘“ğ’™â€² â‰ˆ0
Shapley
ğ‘”ğ‘§= ğœ™* + âˆ‘')!
-
ğœ™'ğ‘§'

--------------------------------------------------------------------------------
[End of Page 33]

34
Question?

--------------------------------------------------------------------------------
[End of Page 34]

35
IG
â€¢ Uniqueness of Integrated Gradients
ğ’™â€²
ğ’™
Each path yields a different attribution method
ğ‘ƒğ‘ğ‘¡â„ğ¼ğº' ğ’™= 7
()*
!
ğœ•ğ‘“ğ›¾ğ›¼
ğœ•ğ›¾' ğ›¼
ğœ•ğ›¾' ğ›¼
ğœ•ğ›¼
ğ‘‘ğ›¼
ğ›¾ğ›¼: path function, ğ›¾0 = ğ’™â€², ğ›¾1 = ğ’™
IG is the straight path:
ğ›¾ğ›¼= ğ’™" + ğ›¼ğ’™âˆ’ğ’™â€™

--------------------------------------------------------------------------------
[End of Page 35]

36
IG
â€¢ Uniqueness of Integrated Gradients
ğ’™â€²
ğ’™
Each path yields a different attribution method
ğ‘ƒğ‘ğ‘¡â„ğ¼ğº' ğ’™= 7
()*
!
ğœ•ğ‘“ğ›¾ğ›¼
ğœ•ğ›¾' ğ›¼
ğœ•ğ›¾' ğ›¼
ğœ•ğ›¼
ğ‘‘ğ›¼
ğ›¾ğ›¼: path function, ğ›¾0 = ğ’™â€², ğ›¾1 = ğ’™
Sensitivity
Implementation invariance

--------------------------------------------------------------------------------
[End of Page 36]

37
IG
â€¢ Uniqueness of Integrated Gradients
ğ’™â€²
ğ’™
Why the straightline path chosen by integrated gradients is canonical?
Ã¼ The simplest path
Ã¼ Preserving symmetry
For all inputs and baselines that have identical 
values for symmetric variables, the symmetric 
variables receive identical attributions
Swapping the two variables 
does not change the function
ğ‘“ğ‘¥, ğ‘¦= ğ‘“(ğ‘¦, ğ‘¥)

--------------------------------------------------------------------------------
[End of Page 37]

38
IG
â€¢ Uniqueness of Integrated Gradients
ğ’™â€²
ğ’™
Why the straightline path chosen by integrated gradients is canonical?
Ã¼ The simplest path
Ã¼ Preserving symmetry
For all inputs and baselines that have identical 
values for symmetric variables, the symmetric 
variables receive identical attributions
Example
ğ‘™ğ‘œğ‘”ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘_ğ‘Ÿğ‘’ğ‘”ğ‘Ÿğ‘’ğ‘ ğ‘ ğ‘–ğ‘œğ‘›(ğ‘¥# + ğ‘¥$)
Input: ğ‘¥# = ğ‘¥$ = 1
Baseline: ğ‘¥# = ğ‘¥$ = 0
ğ´ğ‘¡ğ‘¡ğ‘Ÿ(ğ‘¥#) = ğ´ğ‘¡ğ‘¡ğ‘Ÿ(ğ‘¥$)

--------------------------------------------------------------------------------
[End of Page 38]

39
IG
â€¢ Uniqueness of Integrated Gradients
ğ’™â€²
ğ’™
Why the straightline path chosen by integrated gradients is canonical?
Ã¼ The simplest path
Ã¼ Preserving symmetry
For all inputs and baselines that have identical 
values for symmetric variables, the symmetric 
variables receive identical attributions
Example
ğ‘™ğ‘œğ‘”ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘_ğ‘Ÿğ‘’ğ‘”ğ‘Ÿğ‘’ğ‘ ğ‘ ğ‘–ğ‘œğ‘›(ğ‘¥# + ğ‘¥$)
Input: ğ‘¥# = ğ‘¥$ = 1
Baseline: ğ‘¥# = ğ‘¥$ = 0
ğ´ğ‘¡ğ‘¡ğ‘Ÿ(ğ‘¥#) = ğ´ğ‘¡ğ‘¡ğ‘Ÿ(ğ‘¥$)
Theorem: IG is the unique 
path method that is 
symmetry-preserving

--------------------------------------------------------------------------------
[End of Page 39]

40
IG
â€¢ Applying Integrated Gradients
The integral of integrated gradients can be efficiently approximated via a summation
ğ¼ğº' ğ’™â‰ˆğ‘¥' âˆ’ğ‘¥'â€² Ã— <
.)!
/ ğœ•ğ‘“ğ’™& + ğ‘˜
ğ‘šğ’™âˆ’ğ’™â€™
ğœ•ğ‘¥'
Ã— 1
ğ‘š
ğ‘š: the number of steps

--------------------------------------------------------------------------------
[End of Page 40]

41
IG
â€¢ Applications of Integrated Gradients
Task: object recognition
Model: GoogleNet
Dataset: ImageNet
Integrated gradients 
are better at reflecting 
distinctive features of 
the input image

--------------------------------------------------------------------------------
[End of Page 41]

42
Question?

--------------------------------------------------------------------------------
[End of Page 42]

43
Explaining Black-box Model
â€¢ Gradient-based methods
â€¢ Attention-based methods

--------------------------------------------------------------------------------
[End of Page 43]

44
Attention
What is attention?
Source: https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-
mechanism-deep-learning/
In psychology, attention is the cognitive process of selectively concentrating on one or a few 
things while ignoring others

--------------------------------------------------------------------------------
[End of Page 44]

45
Attention
What is attention?
Source: https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-
mechanism-deep-learning/
In psychology, attention is the cognitive process of selectively concentrating on one or a few 
things while ignoring others
The attention mechanism 
for neural networks is to 
mimic human brain actions 
in a simplified manner

--------------------------------------------------------------------------------
[End of Page 45]

46
Attention
Light up natural language procession (NLP)
Transformer
(Vaswani et al., 2017)
BERT
(Devlin et al., 2018)
GPT
(Radford et al., 2018)

--------------------------------------------------------------------------------
[End of Page 46]

47
Attention
Context vector: a good summary of the input
Hidden layer
â‹®
ğ‘¥"
ğ‘¥'
ğ‘¥!
â‹®
ğ’‰"
ğ’‰'
ğ’‰!
â¨
ğ’„C
ğ’”C
â‹®
â‹®
ğ’”CD"
ğ‘¦CD"
ğ‘¦C
Input
Output
Encoder 
hidden states
Decoder 
hidden states

--------------------------------------------------------------------------------
[End of Page 47]

48
Attention
Context vector: a good summary of the input
Hidden layer
â‹®
ğ‘¥"
ğ‘¥'
ğ‘¥!
â‹®
ğ’‰"
ğ’‰'
ğ’‰!
â¨
ğ’„C
ğ’„+ = <
')!
-
ğ›¼+'ğ’‰'
ğ›¼+' = ğ‘ğ‘™ğ‘–ğ‘”ğ‘›ğ‘¦+, ğ‘¥'
=
ğ‘’ğ‘¥ğ‘ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ’”+0!, ğ’‰'
âˆ‘.)!
-
ğ‘’ğ‘¥ğ‘ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ’”+0!, ğ’‰.
ğ’”C
â‹®
â‹®
ğ’”CD"
ğ‘¦CD"
ğ‘¦C
Context vector for output ğ‘¦+
How well ğ‘¦+ and ğ‘¥' are aligned   
Softmax of some predefined 
alignment score
Input
Output
Encoder 
hidden states
Decoder 
hidden states

--------------------------------------------------------------------------------
[End of Page 48]

49
Attention
Context vector: a good summary of the input
Hidden layer
â‹®
ğ‘¥"
ğ‘¥'
ğ‘¥!
â‹®
ğ’‰"
ğ’‰'
ğ’‰!
â¨
ğ’„C
ğ’„+ = <
')!
-
ğ›¼+'ğ’‰'
ğ›¼+' = ğ‘ğ‘™ğ‘–ğ‘”ğ‘›ğ‘¦+, ğ‘¥'
=
ğ‘’ğ‘¥ğ‘ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ’”+0!, ğ’‰'
âˆ‘.)!
-
ğ‘’ğ‘¥ğ‘ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ’”+0!, ğ’‰.
ğ’”C
â‹®
â‹®
ğ’”CD"
ğ‘¦CD"
ğ‘¦C
Context vector for output ğ‘¦+
How well ğ‘¦+ and ğ‘¥' are aligned   
Softmax of some predefined 
alignment score
Can be parametrized by 
a feed-forward network 
jointly trained with other 
parts of the model
Input
Output
Encoder 
hidden states
Decoder 
hidden states

--------------------------------------------------------------------------------
[End of Page 49]

50
Attention
ğ‘¥"
ğ‘¥'
ğ‘¥!
ğ‘¦CD"
ğ‘¦C
Input
Output
The attention weights ğ›¼C& somehow indicate how much of each input 
feature contributes to each output
â‹®
â‹®
â‹®
â‹®
ğ›¼C"
ğ›¼C'
ğ›¼C!
Ã¼ Simple, fast
Ã¼ No additional computation

--------------------------------------------------------------------------------
[End of Page 50]

51
Attention
Self-attention mechanism 
Self-attention
â‹®
ğ’™"
ğ’™'
ğ’™!
â‹®
Input or hidden 
representations
ğ’™"â€²
ğ’™'â€²
ğ’™!â€²
With context 
information

--------------------------------------------------------------------------------
[End of Page 51]

52
Attention
Self-attention mechanism 
Self-attention
â‹®
ğ’™"
ğ’™'
ğ’™!
ğ’™"â€²
With context 
information
Input or hidden 
representations

--------------------------------------------------------------------------------
[End of Page 52]

53
Attention
Self-attention mechanism 
ğ’™"
ğ’™'
ğ’™E
Query: ğ’’
Key: ğ’Œ
Value: ğ’—
ğ’™F
ğ’Œ"
ğ’’"
ğ’’! = ğ‘Š1ğ’™!
ğ’Œ! = ğ‘Š.ğ’™!
ğ’Œ" = ğ‘Š.ğ’™"
ğ’Œ# = ğ‘Š.ğ’™#
ğ’Œ$ = ğ‘Š.ğ’™$
ğ’Œ'
ğ’ŒF
ğ’ŒE

--------------------------------------------------------------------------------
[End of Page 53]

54
Attention
Self-attention mechanism 
ğ’™"
ğ’™'
ğ’™E
Query: ğ’’
Key: ğ’Œ
Value: ğ’—
ğ’™F
ğ’Œ"
ğ’’"
ğ’’! = ğ‘Š1ğ’™!
ğ’Œ! = ğ‘Š.ğ’™!
ğ’Œ" = ğ‘Š.ğ’™"
ğ’Œ# = ğ‘Š.ğ’™#
ğ’Œ$ = ğ‘Š.ğ’™$
ğ’Œ'
ğ’ŒF
ğ’ŒE
ğœ¶"
ğœ¶'
ğœ¶F
ğœ¶E

--------------------------------------------------------------------------------
[End of Page 54]

55
Attention
Self-attention mechanism 
ğ’™"
ğ’™'
ğ’™E
Query: ğ’’
Key: ğ’Œ
Value: ğ’—
ğ’™F
ğ’Œ"
ğ’’"
ğ’’! = ğ‘Š1ğ’™!
ğ’Œ! = ğ‘Š.ğ’™!
ğ’Œ" = ğ‘Š.ğ’™"
ğ’Œ# = ğ‘Š.ğ’™#
ğ’Œ$ = ğ‘Š.ğ’™$
ğ’Œ'
ğ’ŒF
ğ’ŒE
ğœ¶"
ğœ¶'
ğœ¶F
ğœ¶E
Softmax
ğœ¶"â€²
ğœ¶'â€²
ğœ¶Fâ€²
ğœ¶Eâ€²

--------------------------------------------------------------------------------
[End of Page 55]

56
Attention
Self-attention mechanism 
ğ’™"
ğ’™'
ğ’™E
Query: ğ’’
Key: ğ’Œ
Value: ğ’—
ğ’™F
ğ’Œ"
ğ’’"
ğ’’! = ğ‘Š1ğ’™!
ğ’Œ! = ğ‘Š.ğ’™!
ğ’Œ" = ğ‘Š.ğ’™"
ğ’Œ# = ğ‘Š.ğ’™#
ğ’Œ$ = ğ‘Š.ğ’™$
ğ’Œ'
ğ’ŒF
ğ’ŒE
ğœ¶"
ğœ¶'
ğœ¶F
ğœ¶E
Softmax
ğœ¶"â€²
ğœ¶'â€²
ğœ¶Fâ€²
ğœ¶Eâ€²
Sum to 1
Attention weight

--------------------------------------------------------------------------------
[End of Page 56]

57
Attention
Self-attention mechanism 
ğ’™"
ğ’™'
ğ’™E
Query: ğ’’
Key: ğ’Œ
Value: ğ’—
ğ’™F
ğ’Œ"
ğ’’"
ğ’’! = ğ‘Š1ğ’™!
ğ’Œ! = ğ‘Š.ğ’™!
ğ’Œ" = ğ‘Š.ğ’™"
ğ’Œ# = ğ‘Š.ğ’™#
ğ’Œ$ = ğ‘Š.ğ’™$
ğ’Œ'
ğ’ŒF
ğ’ŒE
ğœ¶"â€²
ğœ¶'â€²
ğœ¶Fâ€²
ğœ¶Eâ€²
ğ’—"
ğ’—'
ğ’—F
ğ’—E
ğ’—! = ğ‘Š2ğ’™!
ğ’—" = ğ‘Š2ğ’™"
ğ’—# = ğ‘Š2ğ’™#
ğ’—$ = ğ‘Š2ğ’™$

--------------------------------------------------------------------------------
[End of Page 57]

58
Attention
Self-attention mechanism 
ğ’™"
ğ’™'
ğ’™E
Query: ğ’’
Key: ğ’Œ
Value: ğ’—
ğ’™F
ğ’Œ"
ğ’’"
ğ’’! = ğ‘Š1ğ’™!
ğ’Œ! = ğ‘Š.ğ’™!
ğ’Œ" = ğ‘Š.ğ’™"
ğ’Œ# = ğ‘Š.ğ’™#
ğ’Œ$ = ğ‘Š.ğ’™$
ğ’Œ'
ğ’ŒF
ğ’ŒE
ğœ¶"â€²
ğœ¶'â€²
ğœ¶Fâ€²
ğœ¶Eâ€²
ğ’—"
ğ’—'
ğ’—F
ğ’—E
ğ’—! = ğ‘Š2ğ’™!
ğ’—" = ğ‘Š2ğ’™"
ğ’—# = ğ‘Š2ğ’™#
ğ’—$ = ğ‘Š2ğ’™$
â¨‚
â¨‚
â¨‚
â¨‚
ğ’™"â€²

--------------------------------------------------------------------------------
[End of Page 58]

59
Attention
Self-attention mechanism 
Self-attention
â‹®
ğ’™"
ğ’™'
ğ’™!
â‹®
Input or hidden 
representations
ğ’™"â€²
ğ’™'â€²
ğ’™!â€²
With context 
information
ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›ğ¾, ğ‘‰, ğ‘„= ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ğ‘„ğ¾%
ğ‘›
ğ‘‰

--------------------------------------------------------------------------------
[End of Page 59]

60
Attention
Composite embeddings based on attentions
Source: https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1

--------------------------------------------------------------------------------
[End of Page 60]

61
Attention
Consider the last attention layer for model interpretation
BERT
Transformer
Multi-Head Attention

--------------------------------------------------------------------------------
[End of Page 61]

62
Question?

--------------------------------------------------------------------------------
[End of Page 62]

63
Is Attention Interpretable?
Sofia Serrano, Noah A. Smith
(ACL, 2019)

--------------------------------------------------------------------------------
[End of Page 63]

64
Attention weights can be highly inconsistent with model prediction
Attention for Explanation
ğ‘¥"
ğ‘¥'
ğ‘¥!
Input
â‹®
Attention
ğ‘"
ğ‘'
ğ‘!
â‹®
Sum to 1

--------------------------------------------------------------------------------
[End of Page 64]

65
Intermediate Representation Erasure 
â€¢
Explanation ğ¼: a ranking of importance of the attention layerâ€™s input representations
â€¢
Exam the impact of some contextualized inputs to an attention layer, ğ¼â€² âŠ‚ğ¼, on the modelâ€™s output

--------------------------------------------------------------------------------
[End of Page 65]

66
Intermediate Representation Erasure 
â€¢
Explanation ğ¼: a ranking of importance of the attention layerâ€™s input representations
â€¢
Exam the impact of some contextualized inputs to an attention layer, ğ¼â€² âŠ‚ğ¼, on the modelâ€™s output
â€¢
Running the model twice: once without any modification, once with the attention weights of ğ¼â€²
zeroed out

--------------------------------------------------------------------------------
[End of Page 66]

67
Intermediate Representation Erasure 
Evaluate model prediction change
â€¢
Jensen-Shannon (JS) divergence between 
output distributions ğ‘and ğ‘3&
â€¢
Difference between the argmaxes of ğ‘and ğ‘3&
(decision flip) 
ğ½ğ‘†ğ‘ƒğ‘„= 1
2 ğ¾ğ¿ğ‘ƒğ‘€+ 1
2 ğ¾ğ¿ğ‘„ğ‘€
ğ‘€= 1
2 ğ‘ƒ+ 1
2 ğ‘„

--------------------------------------------------------------------------------
[End of Page 67]

68
Single Attention Weight Importance
Remove the component ğ‘–âˆ—âˆˆğ¼with the highest attention weight ğ›¼'âˆ—
Comparison: a random component ğ‘Ÿdrawn from ğ¼
ğ½ğ‘†ğ‘, ğ‘&âˆ—
ğ½ğ‘†ğ‘, ğ‘'

--------------------------------------------------------------------------------
[End of Page 68]

69
Single Attention Weight Importance
Remove the component ğ‘–âˆ—âˆˆğ¼with the highest attention weight ğ›¼'âˆ—
Comparison: a random component ğ‘Ÿdrawn from ğ¼
ğ½ğ‘†ğ‘, ğ‘&âˆ—
ğ½ğ‘†ğ‘, ğ‘'
Indicate how important ğ‘–âˆ—
is wrt ğ‘Ÿ. Intuitively, if 
âˆ‡ğ›¼= ğ›¼'âˆ—âˆ’ğ›¼5 is larger, 
âˆ‡ğ½ğ‘†should be larger.
âˆ‡ğ½ğ‘†= ğ½ğ‘†ğ‘, ğ‘'âˆ—
âˆ’ğ½ğ‘†ğ‘, ğ‘5

--------------------------------------------------------------------------------
[End of Page 69]

70
Single Attention Weight Importance
Remove the component ğ‘–âˆ—âˆˆğ¼with the highest attention weight ğ›¼'âˆ—
Comparison: a random component ğ‘Ÿdrawn from ğ¼
ğ½ğ‘†ğ‘, ğ‘&âˆ—
ğ½ğ‘†ğ‘, ğ‘'
âˆ‡ğ½ğ‘†= ğ½ğ‘†ğ‘, ğ‘'âˆ—
âˆ’ğ½ğ‘†ğ‘, ğ‘5
âˆ‡ğ½ğ‘†
âˆ‡ğ›¼= ğ›¼&âˆ—âˆ’ğ›¼'
Ã¼ If ğ‘–âˆ—is more important, âˆ‡ğ½ğ‘†is larger
Ã¼ When âˆ‡ğ½ğ‘†is small (close to 0), âˆ‡ğ›¼tends 
to be small
(ğ‘–âˆ—and ğ‘Ÿare nearly â€œtiedâ€ in attention) 

--------------------------------------------------------------------------------
[End of Page 70]

71
Single Attention Weight Importance
Remove the component ğ‘–âˆ—âˆˆğ¼with the highest attention weight ğ›¼'âˆ—
Comparison: a random component ğ‘Ÿdrawn from ğ¼
ğ½ğ‘†ğ‘, ğ‘&âˆ—
ğ½ğ‘†ğ‘, ğ‘'
âˆ‡ğ½ğ‘†= ğ½ğ‘†ğ‘, ğ‘'âˆ—
âˆ’ğ½ğ‘†ğ‘, ğ‘5
âˆ‡ğ½ğ‘†
âˆ‡ğ›¼= ğ›¼&âˆ—âˆ’ğ›¼'
Ã¼ If ğ‘–âˆ—is more important, âˆ‡ğ½ğ‘†is larger
Ã¼ When âˆ‡ğ½ğ‘†is small (close to 0), âˆ‡ğ›¼tends 
to be small
(ğ‘–âˆ—and ğ‘Ÿare nearly â€œtiedâ€ in attention) 
Ã¼ When âˆ‡ğ›¼is about 0.4, âˆ‡ğ½ğ‘†is still close to 0 
How much the attention 
weight can express the 
importance of a feature?

--------------------------------------------------------------------------------
[End of Page 71]

72
Single Attention Weight Importance
Decision flips caused by zeroing attention
Remove the component ğ‘–âˆ—âˆˆğ¼with the highest attention weight ğ›¼'âˆ—
Comparison: a random component ğ‘Ÿdrawn from ğ¼
Intuitively, upper-right values 
should be much larger than 
lower-left values

--------------------------------------------------------------------------------
[End of Page 72]

73
Single Attention Weight Importance
Decision flips caused by zeroing attention
Remove the component ğ‘–âˆ—âˆˆğ¼with the highest attention weight ğ›¼'âˆ—
Comparison: a random component ğ‘Ÿdrawn from ğ¼
Ã¼ Upper-right values are larger than lower-left 
values (removing ğ‘–âˆ—is easier to flip decision)

--------------------------------------------------------------------------------
[End of Page 73]

74
Single Attention Weight Importance
Decision flips caused by zeroing attention
Remove the component ğ‘–âˆ—âˆˆğ¼with the highest attention weight ğ›¼'âˆ—
Comparison: a random component ğ‘Ÿdrawn from ğ¼
Ã¼ Upper-right values are larger than lower-left 
values (removing ğ‘–âˆ—is easier to flip decision)
Ã¼ In most cases (lower-right values), erasing 
ğ‘–âˆ—does not change the decision
The highest attention weight indicates 
the most important feature?

--------------------------------------------------------------------------------
[End of Page 74]

75
Single Attention Weight Importance
ğ¼
ğ›¼"
ğ›¼'
ğ›¼F
ğ›¼E
â‹®
ğ›¼!
(descending order of importance)

--------------------------------------------------------------------------------
[End of Page 75]

76
Single Attention Weight Importance
ğ¼
ğ›¼"
ğ›¼'
ğ›¼F
ğ›¼E
â‹®
ğ›¼!
(descending order of importance)
ğ¼
ğ›¼"
ğ›¼'
ğ›¼F
ğ›¼E
â‹®
ğ›¼!
(descending order of importance)
Intuitively, the top items in a truly useful 
ranking of importance would comprise a 
minimal necessary set of information for 
making the modelâ€™s decision

--------------------------------------------------------------------------------
[End of Page 76]

77
Importance of Sets of Attention Weights
Test how multiple attention weights perform together as importance predictors
Erasing representations from the top of the ranking downward until the modelâ€™s decision changes
ğ¼
ğ›¼"
ğ›¼'
ğ›¼F
ğ›¼E
â‹®
ğ›¼!
Prediction change

--------------------------------------------------------------------------------
[End of Page 77]

78
Importance of Sets of Attention Weights
Test how multiple attention weights perform together as importance predictors
Erasing representations from the top of the ranking downward until the modelâ€™s decision changes
ğ¼
ğ›¼"
ğ›¼'
ğ›¼F
ğ›¼E
â‹®
ğ›¼!
Prediction change
ğ¼"
ğ›¼"
ğ›¼'
ğ›¼F
ğ›¼E
â‹®
ğ›¼!
Prediction change
(Alternative rankings 
of importance)
Attention may not be a good 
interpretation method

--------------------------------------------------------------------------------
[End of Page 78]

79
Importance of Sets of Attention Weights
Baselines
â€¢
Random rankings
â€¢
Gradients
â€¢
Gradients Ã— Attentions
Ã¼ Both a high attention weight and 
a high calculated gradient indicate 
an important component
Fractions of original components removed before first 
decision flip under different importance rankings

--------------------------------------------------------------------------------
[End of Page 79]

80
Lipton (2016) describes a model as â€œtransparentâ€:
a person can contemplate the entire model at once
Explanations are concise
Attention suggests a large part of features as â€œimportantâ€

--------------------------------------------------------------------------------
[End of Page 80]

81
Question?

--------------------------------------------------------------------------------
[End of Page 81]

82
Reference
â€¢
Shrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. "Learning important features through propagating 
activation differences." International conference on machine learning. PMLR, 2017.
â€¢
Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. "Axiomatic attribution for deep networks." International 
conference on machine learning. PMLR, 2017.
â€¢
Serrano, Sofia, and Noah A. Smith. "Is attention interpretable?." arXiv preprint arXiv:1906.03731 (2019).
â€¢
Smilkov, Daniel, et al. "Smoothgrad: removing noise by adding noise." arXiv preprint arXiv:1706.03825 (2017).
â€¢
Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems 30 (2017).
â€¢
Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." arXiv
preprint arXiv:1810.04805 (2018).
â€¢
Radford, Alec, et al. "Improving language understanding by generative pre-training." (2018).

--------------------------------------------------------------------------------
[End of Page 82]