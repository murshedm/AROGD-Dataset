CS 4501/6501 Interpretable Machine Learning
Jiefeng Chen
Department of Computer Sciences
University of Wisconsin - Madison
jchen662@wisc.edu
1
Guest Lecture: Robust Attribution Regularization

--------------------------------------------------------------------------------
[End of Page 1]

2
Overview
â€¢ What is attribution? 
â€¢ How to compute attribution? 
â€¢ What is the vulnerability issue of attribution methods? 
â€¢ Why should attribution methods be robust? 
â€¢ How to enforce attribution robustness?
â€¢ What are the benefits of attribution robustness?

--------------------------------------------------------------------------------
[End of Page 2]

3
Overview
â€¢ What is attribution? 
â€¢ How to compute attribution? 
â€¢ What is the vulnerability issue of attribution methods? 
â€¢ Why should attribution methods be robust? 
â€¢ How to enforce attribution robustness?
â€¢ What are the benefits of attribution robustness?

--------------------------------------------------------------------------------
[End of Page 3]

4
What is Attribution?
According to Merriam-Webster: â€œattributionâ€ means 
â€œto explain (something) by indicating a causeâ€. 

--------------------------------------------------------------------------------
[End of Page 4]

5
Human Learning
Do you see a dog?
Yes.

--------------------------------------------------------------------------------
[End of Page 5]

6
Human Learning
What makes you think so?
Because I see this!
Correct
Wrong

--------------------------------------------------------------------------------
[End of Page 6]

7
Machine Learning
Machine learning is like human learning. 
Training Data
Learning 
Algorithm
Model
â€¦â€¦
â€¦â€¦
Dog
Test Data

--------------------------------------------------------------------------------
[End of Page 7]

8
Deep Neural Networks
A neural network with some level of complexity, usually at least two 
layers, qualifies as a Deep Neural Network (DNN). 
â€¦ â€¦
â€¦
â€¦
â€¦
â€¦
Outdoor
Indoor
Outdoor

--------------------------------------------------------------------------------
[End of Page 8]

9
Deep Learning Breakthroughs
Image Classification
Machine Translation
Game Playing

--------------------------------------------------------------------------------
[End of Page 9]

10
Deep Learning Challenges
â€¢ Blackbox: not too much understanding/interpretation
â€¢ Vulnerable to adversarial examples
Model
Dog
classified as
Stop Sign
97% confidence
classified as
Max Speed 100
98% confidence
+
=

--------------------------------------------------------------------------------
[End of Page 10]

11
Why DNN Models are Vulnerable? 
Deep Neural Networks (DNNs) may use spurious correlation for prediction. 
Training Data
Test Data
How will the 
DNN behave?

--------------------------------------------------------------------------------
[End of Page 11]

12
Attribution in Machine Learning
Attribution: attributing the prediction of a DNN to its input features.
Attributions for Question Classification
Attributions for Image Classification 

--------------------------------------------------------------------------------
[End of Page 12]

13
Formal Definition of Attribution
Suppose we have a function ğ¹: ğ‘…! â†’[0,1] that represents a deep network, 
and an input ğ‘¥= ğ‘¥", â€¦ , ğ‘¥! âˆˆğ‘…!. An attribution of the prediction at input ğ‘¥
relative to a baseline input ğ‘¥â€² is a vector ğ´# ğ‘¥, ğ‘¥$ = ğ‘", â€¦ , ğ‘! âˆˆğ‘…! where 
ğ‘% is the contribution of ğ‘¥% to the prediction ğ¹(ğ‘¥).
Model
ğ¹(ğ‘¥)
Cabbage Butterfly
Score: 0.996
ğ‘¥
ğ¹
ğ‘¥â€™
ğ´# ğ‘¥, ğ‘¥$

--------------------------------------------------------------------------------
[End of Page 13]

14
A note on the baseline
â€¢ The need for a baseline is central to any explanation method. In a 
sense, it is the counterfactual for causal reasoning. 
â€¢ The network must have a truly neutral prediction at the baseline input.

--------------------------------------------------------------------------------
[End of Page 14]

15
Question?

--------------------------------------------------------------------------------
[End of Page 15]

16
Overview
â€¢ What is attribution? 
â€¢ How to compute attribution? 
â€¢ What is the vulnerability issue of attribution methods? 
â€¢ Why should attribution methods be robust? 
â€¢ How to enforce attribution robustness?
â€¢ What are the benefits of attribution robustness?

--------------------------------------------------------------------------------
[End of Page 16]

17
Common Attribution Methods
â€¢ Gradients
â€¢ DeepLift
â€¢ Layer-wise Relevance Propagation (LRP)
â€¢ Deconvolutional Networks
â€¢ Guided Back-propagation
â€¢ Integrated Gradients

--------------------------------------------------------------------------------
[End of Page 17]

18
How to evaluate an attribution method?
Sundararajan et al.â€™s Approach:
â€¢ Define a set of reasonable axioms for attribution methods. 
â€¢ Check if the attribution method satisfies them.  
Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. "Axiomatic attribution for deep networks." ICML, 2017.

--------------------------------------------------------------------------------
[End of Page 18]

19
Sensitivity Axioms
â€¢ Sensitivity: If starting from baseline, varying a variable changes the 
output, then the variable should receive some attribution.
â€¢ Insensitivity: A variable that has no effect on the output gets no 
attribution.
Pure gradients do not satisfy Sensitivity 
when predictions saturate.

--------------------------------------------------------------------------------
[End of Page 19]

20
Functional Axioms
â€¢ Implementation Invariance: Two functionally equivalent networks 
have identical attributions for all inputs and baseline. 
â€¢ Linearity: If the function ğ¹is a linear combination of two functions ğ¹", 
ğ¹&, then the attributions for ğ¹are a linear combination of the 
attributions for ğ¹", ğ¹&. 
â€¢ Symmetry: If a function is symmetric across two input variables then 
the variables should receive identical attribution. 

--------------------------------------------------------------------------------
[End of Page 20]

21
An Accounting Axiom
Completeness: Sum(attributions) = F(input) - F(baseline).
Break down the predicted click through rate (pCTR) of an ad like:
â€¢ 55% of pCTR is because itâ€™s at position 1.
â€¢ 25% is due to its domain (a popular one).
â€¦

--------------------------------------------------------------------------------
[End of Page 21]

22
Integrated Gradients (IG)
â€¢ The integrated gradient (IG) along the ğ‘–ğ‘¡â„dimension for an input ğ‘¥and 
baseline ğ‘¥â€™ is defined as follows: 
ğ¼ğº% ğ‘¥â‰”ğ‘¥% âˆ’ğ‘¥%
$ Ã— :
'()
"
ğœ•ğ¹(ğ‘¥$ + ğ›¼Ã—(ğ‘¥âˆ’ğ‘¥â€²))
ğœ•ğ‘¥%
ğ‘‘ğ›¼
â€¢ IG satisfies the completeness axiom: if F is differentiable almost 
everywhere, then âˆ‘%("
!
ğ¼ğº% ğ‘¥= ğ¹ğ‘¥âˆ’ğ¹(ğ‘¥â€²).
Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. "Axiomatic attribution for deep networks." ICML, 2017.

--------------------------------------------------------------------------------
[End of Page 22]

23
Theoretical Result
Theorem: Integrated Gradients is the unique method satisfying:
â€¢ Sensitivity, Insensitivity
â€¢ Implementation Invariance, Linearity, Symmetry
â€¢ Completeness
up to the errors from approximating integration.

--------------------------------------------------------------------------------
[End of Page 23]

24
Implementation of IG
â€¢ The integral of IG can be efficiently approximated via a summation:
ğ¼ğº%
*++,-. ğ‘¥â‰”ğ‘¥% âˆ’ğ‘¥%
$ Ã— @
/("
0
ğœ•ğ¹ğ‘¥$ + ğ‘˜
ğ‘šÃ— ğ‘¥âˆ’ğ‘¥$
ğœ•ğ‘¥%
Ã— 1
ğ‘š
Here, m is the number of steps in the Riemman approximation of the integral. 
â€¢ Step-size ğ‘š: check if completeness holds. If not, increase ğ‘š.
â€¢ Baseline ğ‘¥$: select ğ‘¥$ that leads to a near-zero score. 

--------------------------------------------------------------------------------
[End of Page 24]

25
TensorFlow Implementation of IG

--------------------------------------------------------------------------------
[End of Page 25]

26
Applications of IG
We can use IG for Diabetic Retinopathy Prediction where feature importance 
explanations are important for specialists to build trust in the networkâ€™s predictions. 
Attribution for Diabetic Retinopathy grade prediction from a retinal fundus image.

--------------------------------------------------------------------------------
[End of Page 26]

27
Question?

--------------------------------------------------------------------------------
[End of Page 27]

28
Overview
â€¢ What is attribution? 
â€¢ How to compute attribution? 
â€¢ What is the vulnerability issue of attribution methods? 
â€¢ Why should attribution methods be robust? 
â€¢ How to enforce attribution robustness?
â€¢ What are the benefits of attribution robustness?

--------------------------------------------------------------------------------
[End of Page 28]

29
Fragile Interpretation
As Ghorbani et al. convincingly demonstrated, for existing DNNs, one can generate 
minimal input perturbations that substantially change model attributions, while 
keeping their (correct) predictions intact. 
Ghorbani, Amirata, Abubakar Abid, and James Zou. "Interpretation of neural networks is fragile." AAAI 2019.

--------------------------------------------------------------------------------
[End of Page 29]

30
Attribution Attack Objective
For a given neural network ğ’©with fixed weights and a test data point ğ‘¥1, the 
feature importance method produce an interpretation ğ¼(ğ‘¥1; ğ’©), which is a 
vector of normalized feature scores. The attribution attack objective is: 
arg max
2
ğ’Ÿ(ğ¼ğ‘¥1; ğ’©, ğ¼(ğ‘¥1 + ğ›¿; ğ’©))
ğ‘ ğ‘¢ğ‘ğ‘—ğ‘’ğ‘ğ‘¡ğ‘¡ğ‘œ:
ğ›¿
3 â‰¤ğœ–
ğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘¥1 + ğ›¿; ğ’©= ğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘¥1; ğ’©)
where ğ’Ÿ(â‹…) measures the change in interpretation and ğœ–> 0 constrains the 
norm of the perturbation. 

--------------------------------------------------------------------------------
[End of Page 30]

31
Solving Attribution Attack Objective
Algorithm Iterative Feature Importance Attacks
Input: test image ğ‘¥!, perturbation budget ğœ–, normalized feature importance function I(6), number of 
iterations P, step size ğ›¼. 
Define a dissimilarity function D to measure the change between interpretations of two images:
ğ·ğ‘¥!, ğ‘¥=
âˆ’âˆ‘"âˆˆ$ ğ¼ğ‘¥"
âˆ‘"âˆˆ% ğ¼ğ‘¥"
ğ¶ğ‘¥âˆ’ğ¶ğ‘¥!
&
where B is the set of the k largest dimensions of ğ¼(ğ‘¥!), A is the target region of the input image in 
targeted attack, and ğ¶(â‹…) is the center of feature importance mass. 
Initialize ğ‘¥" = ğ‘¥!
for ğ‘âˆˆ1, â€¦ , ğ‘ƒdo
Perturb the test image: ğ‘¥# = ğ‘¥#$% + ğ›¼â‹…ğ‘ ğ‘–ğ‘”ğ‘›(âˆ‡&ğ·(ğ‘¥!, ğ‘¥#$%))
If needed, clip the perturbed input to satisfy: ğ‘¥# âˆ’ğ‘¥!
' â‰¤ğœ–
end for
Among {ğ‘¥%, â€¦ , ğ‘¥(}, return the element with the largest value for the dissimilarity function and the 
same prediction as the original test image. 
for top-k attack
for targeted attack
for mass-center attack

--------------------------------------------------------------------------------
[End of Page 31]

32
Metrics for interpretation similarity
â€¢ Spearmanâ€™s correlation: use the rank correlation to compare the 
similarity between two attributions.
â€¢ Top-k intersection: compute the size of intersection of the k most 
important features of the two attributions divided by k. 
The indices of the 5 most important features 
of attribution A is {0, 1, 2, 3, 4} while the 
indices of the 5 most important features of 
attribution B is {3, 4, 5, 6, 7}. Then the top-5 
intersection of A and B is 0.4. 

--------------------------------------------------------------------------------
[End of Page 32]

33
Top-k and Mass-center Attack Results

--------------------------------------------------------------------------------
[End of Page 33]

34
Targeted Attack Results

--------------------------------------------------------------------------------
[End of Page 34]

35
Question?

--------------------------------------------------------------------------------
[End of Page 35]

36
Overview
â€¢ What is attribution? 
â€¢ How to compute attribution? 
â€¢ What is the vulnerability issue of attribution methods? 
â€¢ Why should attribution methods be robust? 
â€¢ How to enforce attribution robustness?
â€¢ What are the benefits of attribution robustness?

--------------------------------------------------------------------------------
[End of Page 36]

37
Why we want robust attributions?
Model attributions are facts about model behaviors. While robust 
attribution does not necessarily mean that the attribution is correct, a 
model with brittle attribution can never be trusted.

--------------------------------------------------------------------------------
[End of Page 37]

38
Importance of Attribution Robustness
In safety critical applications, the users need to check the attribution to see 
whether the modelâ€™s predictions could be trusted. If the attributions are 
brittle, the users will find it difficult to trust the model. 

--------------------------------------------------------------------------------
[End of Page 38]

39
Overview
â€¢ What is attribution? 
â€¢ How to compute attribution? 
â€¢ What is the vulnerability issue of attribution methods? 
â€¢ Why should attribution methods be robust? 
â€¢ How to enforce attribution robustness?
â€¢ What are the benefits of attribution robustness?

--------------------------------------------------------------------------------
[End of Page 39]

40
Robust Prediction Correlates with Robust Attribution: Why?
original image, 
normally trained model
perturbed image,
normally trained model
Empirical results demonstrate that if the model has robust prediction, 
usually it also has robust attribution. 

--------------------------------------------------------------------------------
[End of Page 40]

41
Robust Prediction Correlates with Robust Attribution: Why?
perturbed image,
robustly trained model
original image, 
robustly trained model
Empirical results demonstrate that if the model has robust prediction, 
usually it also has robust attribution. 

--------------------------------------------------------------------------------
[End of Page 41]

42
Adversarial Training (AT) 
Training for robust prediction: find a model that predicts the same label 
for all perturbed inputs around the training input. 
min
)
ğ”¼ğ’™,+ âˆ¼-
max
ğ’™!âˆˆ/(ğ’™) â„“ğ’™â€², ğ‘¦; ğœƒ
Perturbed input
Allowed perturbations
Madry, Aleksander, et al. â€œTowards deep learning models resistant to adversarial attacks.â€ ICLR 2018

--------------------------------------------------------------------------------
[End of Page 42]

43
Solving AT Objective
We use projected gradient descent (PGD) to solve the inner maximization 
problem and then use stochastic gradient descent (SGD) to optimize the 
model parameters. 
min
O
ğ”¼ğ’™,Q âˆ¼S
max
ğ’™!âˆˆT(ğ’™) â„“ğ’™â€², ğ‘¦; ğœƒ
ğ’™1U" = Î T ğ’™(ğ’™1 + ğ›¼â‹…ğ‘ ğ‘–ğ‘”ğ‘›(âˆ‡ğ’™â„“(ğ’™1, ğ‘¦; ğœƒ)))
ğ’™) = ğ’™
ğ’™$ = ğ’™V
ğœƒ$ = ğœƒâˆ’ğœ‚â‹…âˆ‡Oâ„“(ğ’™$, ğ‘¦; ğœƒ)
PGD
SGD

--------------------------------------------------------------------------------
[End of Page 43]

44
Robust Attribution Regularization (RAR)
Training for robust attribution: find a model that can get similar attributions 
for all perturbed inputs around the training input.
min
)
ğ”¼ğ’™,+ âˆ¼- â„“ğ’™, ğ‘¦; ğœƒ+ ğœ†â‹…RAR
RAR = max
ğ’™!âˆˆ/(ğ’™) ğ‘ (IG(ğ’™, ğ’™â€²))
Perturbed input
Allowed perturbations
Chen, Jiefeng, et al. "Robust attribution regularization." NeurIPS 2019.

--------------------------------------------------------------------------------
[End of Page 44]

45
Robust Attribution Regularization (RAR)
Training for robust attribution: find a model that can get similar attributions 
for all perturbed inputs around the training input.
min
)
ğ”¼ğ’™,+ âˆ¼- â„“ğ’™, ğ‘¦; ğœƒ+ ğœ†â‹…RAR
RAR = max
ğ’™!âˆˆ/(ğ’™) ğ‘ (IG(ğ’™, ğ’™â€²))
Size function
Integrated Gradient
Chen, Jiefeng, et al. "Robust attribution regularization." NeurIPS 2019.

--------------------------------------------------------------------------------
[End of Page 45]

46
IG in RAR
â€¢ The IG function for RAR is defined as: 
ğ¼ğº% ğ‘¥, ğ‘¥â€² â‰”ğ‘¥%
$ âˆ’ğ‘¥% Ã— :
'()
"
ğœ•â„“Q(ğ‘¥+ ğ›¼Ã— ğ‘¥â€² âˆ’ğ‘¥)
ğœ•ğ‘¥%
$
ğ‘‘ğ›¼
where â„“Q ğ‘¥= â„“(ğ‘¥, ğ‘¦; ğœƒ) is the loss function. The input ğ‘¥is regarded as the 
baseline. 
â€¢ From the axiom of Completeness, we have 
@
%("
W
ğ¼ğº% ğ‘¥, ğ‘¥â€² = â„“ğ‘¥$, ğ‘¦; ğœƒâˆ’â„“(ğ‘¥, ğ‘¦; ğœƒ)
â€¢ In implementation, we use summation approximation of IG: 
ğ¼ğº%
*++,-. ğ‘¥, ğ‘¥â€² â‰”ğ‘¥%
$ âˆ’ğ‘¥% Ã— @
/("
0
ğœ•â„“Q ğ‘¥+ ğ‘˜
ğ‘šÃ— ğ‘¥$ âˆ’ğ‘¥
ğœ•ğ‘¥%
$
Ã— 1
ğ‘š

--------------------------------------------------------------------------------
[End of Page 46]

47
Connection to Robust Prediction
â€¢ Robust attribution regularization: 
â€¢ If ğœ†= 1 and ğ‘ â‹…= ğ‘ ğ‘¢ğ‘š(â‹…), then RAR becomes the Adversarial 
Training objective for robust prediction: 
min
O
ğ”¼ğ’™,Q âˆ¼S
max
ğ’™!âˆˆX(ğ’™,Y) â„“(ğ’™$, ğ‘¦; ğœƒ)
simply by the Completeness of IG.
min
)
ğ”¼ğ’™,+ âˆ¼- â„“ğ’™, ğ‘¦; ğœƒ+ ğœ†âˆ—RAR
RAR = max
ğ’™!âˆˆ/(ğ’™) ğ‘ (IG(ğ’™, ğ’™â€²))
Madry, Aleksander, et al. â€œTowards deep learning models resistant to adversarial attacks.â€ ICLR 2018

--------------------------------------------------------------------------------
[End of Page 47]

48
When the two coincide?
Theorem. Consider the special case of one-layer neural networks, where 
the loss function takes the form of â„“ğ’™, ğ‘¦; ğ’˜= ğ‘”(âˆ’ğ‘¦ğ’˜, ğ’™). Suppose ğ‘”
is nonnegative, differentiable, non-decreasing, and convex. Then for ğœ†=
1, ğ‘ â‹…=
â‹…", and â„“3 neighborhood, RAR training objective reduces to 
adversarial training objective: 
@
%("
0
max
ğ’™"
!Zğ’™" #[Y ğ‘”(âˆ’ğ‘¦% ğ’˜, ğ’™%
$ )
= @
%("
0
ğ‘”(âˆ’ğ‘¦% ğ’˜, ğ’™% + ğœ–ğ’˜")
(Adversarial training objective)
(soft-margin)

--------------------------------------------------------------------------------
[End of Page 48]

49
When the two coincide?
For the special case of one-layer neural networks (linear function), the 
robust attribution instantiation (ğ‘ (â‹…) = â€– â‹…â€–") and the robust prediction 
instantiation (ğ‘ (â‹…) = ğ‘ ğ‘¢ğ‘š(â‹…)) coincide, and both reduce to soft max-
margin training. 

--------------------------------------------------------------------------------
[End of Page 49]

50
Connection to Robust Prediction
â€¢ Robust attribution regularization: 
â€¢ If ğœ†= ğœ†â€²/ğœ–\ and ğ‘ â‹…=
â‹…"
\ with approximate IG, then RAR becomes 
the Input Gradient Regularization for robust prediction: 
min
O
ğ”¼ğ’™,Q âˆ¼S â„“ğ’™, ğ‘¦; ğœƒ+ ğœ†â€² âˆ‡ğ’™â„“ğ’™, ğ‘¦; ğœƒ
\
\
min
)
ğ”¼ğ’™,+ âˆ¼- â„“ğ’™, ğ‘¦; ğœƒ+ ğœ†âˆ—RAR
RAR = max
ğ’™!âˆˆ/(ğ’™) ğ‘ (IG(ğ’™, ğ’™â€²))
Ross, Andrew, and Finale Doshi-Velez. â€œImproving the adversarial robustness and interpretability of deep 
neural networks by regularizing their input gradients.â€ AAAI 2018.

--------------------------------------------------------------------------------
[End of Page 50]

51
Instantiations of RAR
â€¢ IG-NORM: if we pick ğ‘ â‹…=
â‹…", then this gives
min
O
ğ”¼ğ’™,Q âˆ¼S â„“ğ’™, ğ‘¦; ğœƒ+ ğœ†â‹…
max
ğ’™!âˆˆX ğ’™,Y
IG(ğ’™, ğ’™â€²) "
â€¢ IG-SUM-NORM: if we define ğ‘ â‹…= ğ‘ ğ‘¢ğ‘šâ‹…+ ğ›½â‹…", where ğ›½â‰¥0 is a 
regularization parameter, and set ğœ†= 1, then this gives
min
O
ğ”¼ğ’™,Q âˆ¼S
max
ğ’™!âˆˆX ğ’™,Y â„“ğ’™$, ğ‘¦; ğœƒ+ ğ›½â‹…IG(ğ’™, ğ’™â€²) "

--------------------------------------------------------------------------------
[End of Page 51]

52
Solving RAR Objective
Given (ğ’™, ğ‘¦) at time step ğ‘¡during training, we have the following two steps:
1) Attack step: we run PGD on (ğ’™, ğ‘¦) to find ğ’™âˆ—that produces a large 
inner max term (i.e., IG ğ’™, ğ’™âˆ—
" for IG-NORM and â„“ğ’™âˆ—, ğ‘¦; ğœƒ+
ğ›½â‹…IG ğ’™, ğ’™âˆ—
" for IG-SUM-NORM). 
2) Gradient step: fixing ğ’™âˆ—, we can then compute the gradient of the 
corresponding loss with respect to ğœƒ, and then update the model.   

--------------------------------------------------------------------------------
[End of Page 52]

53
Difficulty of Optimization
Due to the summation approximation of IG, we have first order terms in 
the training objective. It forces us to compute second derivatives, which 
may not be numerically stable for deep networks. 
min
'
ğ”¼ğ’™,* âˆ¼, â„“ğ’™, ğ‘¦; ğœƒ+ ğœ†â‹…
max
ğ’™!âˆˆ- ğ’™,.
IG/00123(ğ’™, ğ’™â€²) 4
ğ¼ğº"
/00123 ğ‘¥, ğ‘¥â€² â‰”ğ‘¥"
5 âˆ’ğ‘¥" Ã— E
674
8
ğœ•â„“* ğ‘¥+ ğ‘˜
ğ‘šÃ— ğ‘¥5 âˆ’ğ‘¥
ğœ•ğ‘¥"
5
Ã— 1
ğ‘š

--------------------------------------------------------------------------------
[End of Page 53]

54
Experiments: Qualitative
Flower dataset

--------------------------------------------------------------------------------
[End of Page 54]

55
Experiments: Qualitative
MNIST dataset

--------------------------------------------------------------------------------
[End of Page 55]

56
Experiments: Qualitative
Fashion-MNIST dataset

--------------------------------------------------------------------------------
[End of Page 56]

57
Experiments: Qualitative
GTSRB dataset

--------------------------------------------------------------------------------
[End of Page 57]

58
Experiments: Quantitative
â€¢ Metrics for attribution robustness: 
1. Kendallâ€™s tau rank order correlation.
2. Top-K intersection.
Original Image Attribution Map
Perturbed Image Attribution Map
Top-1000 Intersection: 0.1%          
Kendallâ€™s Correlation: 0.2607

--------------------------------------------------------------------------------
[End of Page 58]

59
Experiments: Quantitative
Flower
MNIST
Fashion-MINST
GTSRB

--------------------------------------------------------------------------------
[End of Page 59]

60
Prediction Accuracy of Different Models
Dataset
Approach
Nat. Acc.
Adv. Acc.
MNIST
NATURAL
99.17%
0.00%
AT
98.40%
92.47%
IG-NORM
98.74%
81.43%
IG-SUM-NORM
98.34%
88.17%
Fashion-MNIST
NATURAL
90.86%
0.01%
AT
85.73%
73.01%
IG-NORM
85.13%
65.95%
IG-SUM-NORM
85.44%
70.26% 
GTSRB
NATURAL
98.57%
21.05%
AT
97.59%
83.24%
IG-NORM
97.02%
75.24%
IG-SUM-NORM
95.68%
77.12%
Flower
NATURAL
86.76%
0.00%
AT
83.82%
41.91%
IG-NORM
85.29%
24.26%
IG-SUM-NORM
82.35%
47.06%

--------------------------------------------------------------------------------
[End of Page 60]

61
Empirical Observations
Our main findings can be summarized as follows:
1. Compared with naturally trained models, RAR only results in a very 
small drop in test accuracy;
2. Our method gives significantly better attribution robustness, as 
measured by correlation analyses;
3. Our models yield comparable prediction robustness (sometimes even 
better), compared with adversarially trained models (for robust 
prediction), while consistently improving attribution robustness;
4. Intriguingly, RAR leads to much more human aligned attribution.

--------------------------------------------------------------------------------
[End of Page 61]

62
Overview
â€¢ What is attribution? 
â€¢ How to compute attribution? 
â€¢ What is the vulnerability issue of attribution methods? 
â€¢ Why should attribution methods be robust? 
â€¢ How to enforce attribution robustness?
â€¢ What are the benefits of attribution robustness?

--------------------------------------------------------------------------------
[End of Page 62]

63
Benefits of Attribution Robustness
â€¢ Robust attribution correlates with robust prediction.
â€¢ Robust attribution leads to more human-aligned attribution.
â€¢ Robust attribution may help tackle spurious correlations.

--------------------------------------------------------------------------------
[End of Page 63]

64
Question?

--------------------------------------------------------------------------------
[End of Page 64]

65
Reference
â€¢ Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. "Axiomatic attribution for deep 
networks." International conference on machine learning. PMLR, 2017.
â€¢ Ghorbani, Amirata, Abubakar Abid, and James Zou. "Interpretation of neural 
networks is fragile." Proceedings of the AAAI conference on artificial intelligence. 
Vol. 33. No. 01. 2019.
â€¢ Madry, Aleksander, et al. "Towards Deep Learning Models Resistant to Adversarial 
Attacks." International Conference on Learning Representations. 2018.
â€¢ Chen, Jiefeng, et al. "Robust attribution regularization." Advances in Neural 
Information Processing Systems 32 (2019).

--------------------------------------------------------------------------------
[End of Page 65]