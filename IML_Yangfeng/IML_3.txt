CS 4501/6501 Interpretable Machine Learning
Hanjie Chen, Yangfeng Ji
Department of Computer Science
University of Virginia
{hc9mx, yangfeng}@virginia.edu
1
Interpretable Generalized Additive Models 

--------------------------------------------------------------------------------
[End of Page 1]

2
Interpretability
â€¢
Three parameters (ğ‘¤1, ğ‘¤2, ğ‘¤3)
â€¢
ğ‘¦â€² = ğ‘¤1ğ‘¥1 + ğ‘¤2ğ‘¥2 + ğ‘¤3ğ‘¥3
â€¢
Contributions:
ğ‘¥1: ğ‘¤1ğ‘¥1
ğ‘¥2: ğ‘¤2ğ‘¥2
ğ‘¥3: ğ‘¤3ğ‘¥3
â€¢
Millions of parameters
â€¢
ğ’šâ€² = ğ‘“ğ’˜ğ’™(complex transformations)
â€¢
Model decision-making and feature 
attributions are unclear
Bad performance
Good interpretability 
Good performance 
Bad interpretability 

--------------------------------------------------------------------------------
[End of Page 2]

3
Trade-off
ğ‘¥1
ğ‘¥2
ğ‘¥ğ‘›
â‹¯
Input Layer
Hidden Layers
Output Layer
â‹¯
â‹¯
â‹¯
ğ‘¦
The information of input features is mixed

--------------------------------------------------------------------------------
[End of Page 3]

4
Trade-off
ğ‘¥1
ğ‘¥2
ğ‘¥ğ‘›
â‹¯
Input Layer
Hidden Layers
Output Layer
+
â‹¯
â‹¯
â‹¯
â‹¯
ğ‘¦
Keep the information of individual features â€œlocallyâ€

--------------------------------------------------------------------------------
[End of Page 4]

5
Trade-off
Interpretability
ğ‘¥1
ğ‘¥2
ğ‘¥ğ‘›
â‹¯
Input Layer
Hidden Layers
Output Layer
+
â‹¯
â‹¯
â‹¯
â‹¯
ğ‘¦
Keep the information of individual features â€œlocallyâ€
Users can understand 
the contributions of 
individual features

--------------------------------------------------------------------------------
[End of Page 5]

6
Trade-off
Generalized additive models (GAMs)
ğ‘”ğ‘¦= ğ‘“1 ğ‘¥1 + ğ‘“2 ğ‘¥2 + â‹¯+ ğ‘“ğ‘›ğ‘¥ğ‘›
â€¢ Permit complex relationships between individual features (ğ‘¥ğ‘–) and the 
target (ğ‘”ğ‘¦)
â€¢ Exclude complex interactions between features

--------------------------------------------------------------------------------
[End of Page 6]

7
GAM
Generalized additive models (GAMs)
ğ‘”ğ‘¦= ğ‘“1 ğ‘¥1 + ğ‘“2 ğ‘¥2 + â‹¯+ ğ‘“ğ‘›ğ‘¥ğ‘›
â€¢ ğ‘”âˆ™: link function
-
Identity: ğ‘”ğ‘¦= ğ‘¦
Regression
-
Logistic function: ğ‘”ğ‘¦represents the probability on a class             Classification
ğ¿
1 + ğ‘’âˆ’ğ‘˜(ğ‘¥âˆ’ğ‘¥0)
(ğ¿= 1, ğ‘˜= 1, ğ‘¥0 = 0)

--------------------------------------------------------------------------------
[End of Page 7]

8
GAM
Generalized additive models (GAMs)
ğ‘”ğ‘¦= ğ‘“1 ğ‘¥1 + ğ‘“2 ğ‘¥2 + â‹¯+ ğ‘“ğ‘›ğ‘¥ğ‘›
â€¢ ğ‘“ğ‘–âˆ™: shape function
-
Splines
ğ‘¥2
ğ‘¥
sin ğ‘¥
ğ‘¥
ğ‘¥
ğ‘¥
ğ‘¦

--------------------------------------------------------------------------------
[End of Page 8]

9
GAM
Generalized additive models (GAMs)
ğ‘”ğ‘¦= ğ‘“1 ğ‘¥1 + ğ‘“2 ğ‘¥2 + â‹¯+ ğ‘“ğ‘›ğ‘¥ğ‘›
â€¢ ğ‘“ğ‘–âˆ™: shape function
-
Binary Trees
ğ‘¥< 5
yes
no
ğ‘¦= 1
ğ‘¦= âˆ’1
ğ‘¦
ğ‘¥

--------------------------------------------------------------------------------
[End of Page 9]

10
GAM
Generalized additive models (GAMs)
ğ‘”ğ‘¦= ğ‘“1 ğ‘¥1 + ğ‘“2 ğ‘¥2 + â‹¯+ ğ‘“ğ‘›ğ‘¥ğ‘›
â€¢ ğ‘“ğ‘–âˆ™: shape function
-
Binary Trees
ğ‘¥< 5
yes
no
ğ‘¦= 1
ğ‘¦= 0
ğ‘¥< 7
yes
no
ğ‘¦= âˆ’1
ğ‘¦
ğ‘¥

--------------------------------------------------------------------------------
[End of Page 10]

11
GAM
Generalized additive models (GAMs)
ğ‘”ğ‘¦= ğ‘“1 ğ‘¥1 + ğ‘“2 ğ‘¥2 + â‹¯+ ğ‘“ğ‘›ğ‘¥ğ‘›
â€¢ ğ‘“ğ‘–âˆ™: shape function
-
Binary Trees
ğ‘¥< 5
yes
no
ğ‘¦= 1
ğ‘¦= 0
ğ‘¥< 7
yes
no
ğ‘¦= âˆ’1
For interpretability, we 
control tree complexity 
(nodes, leaves, depth) 
ğ‘¦
ğ‘¥

--------------------------------------------------------------------------------
[End of Page 11]

12
GAM
Generalized additive models (GAMs)
ğ‘”ğ‘¦= ğ‘“1 ğ‘¥1 + ğ‘“2 ğ‘¥2 + â‹¯+ ğ‘“ğ‘›ğ‘¥ğ‘›
â€¢ ğ‘“ğ‘–âˆ™: shape function
-
Bagged Trees (reduce the variance)
1
ğµ(
+ â‹¯+
)

--------------------------------------------------------------------------------
[End of Page 12]

13
GAM
Generalized additive models (GAMs)
ğ‘”ğ‘¦= ğ‘“1 ğ‘¥1 + ğ‘“2 ğ‘¥2 + â‹¯+ ğ‘“ğ‘›ğ‘¥ğ‘›
â€¢ Training
-
Shape functions for individual features
-
Learning methods

--------------------------------------------------------------------------------
[End of Page 13]

14
Learning GAM
â€¢ Gradient Boosting
-
Learning tree or tree ensemble shape functions
Algorithm Gradient Boosting for GAM
1.
2.
3.
4.
5.
6.
ğ‘“ğ‘—â†0, ğ‘—= 1, â‹¯, ğ‘›
for ğ‘š= 1, â‹¯, ğ‘€do
for ğ‘—= 1, â‹¯, ğ‘›do
â„›â†ğ‘¥ğ‘–ğ‘—, ğ‘¦ğ‘–âˆ’à·
ğ‘˜
ğ‘“ğ‘˜
ğ‘–=1
ğ‘
Learning shape function S: ğ‘¥ğ‘—â†’ğ‘¦using â„›as training data
ğ‘“ğ‘—â†ğ‘“ğ‘—+ ğ‘†
Initialize all shape functions as zero
Loop over M iterations
Loop over all features
Calculate residuals
Learn the one-dimensional 
function to predict the residuals
Update the shape function

--------------------------------------------------------------------------------
[End of Page 14]

15
Learning GAM
â€¢ Gradient Boosting
-
Learning tree or tree ensemble shape functions
Algorithm Gradient Boosting for GAM
1.
2.
3.
4.
5.
6.
ğ‘“ğ‘—â†0, ğ‘—= 1, â‹¯, ğ‘›
for ğ‘š= 1, â‹¯, ğ‘€do
for ğ‘—= 1, â‹¯, ğ‘›do
Learning shape function S: ğ‘¥ğ‘—â†’ğ‘¦using â„›as training data
ğ‘“ğ‘—â†ğ‘“ğ‘—+ ğ‘†
Initialize all shape functions as zero
Loop over M iterations
Loop over all features
Learn the one-dimensional 
function to predict the residuals
Update the shape function
â„›â†ğ‘¥ğ‘–ğ‘—, ğ‘¦ğ‘–âˆ’à·
ğ‘˜
ğ‘“ğ‘˜
ğ‘–=1
ğ‘
Calculate residuals

--------------------------------------------------------------------------------
[End of Page 15]

16
Learning GAM
â€¢ Gradient Boosting
ğ‘–
ğ‘¥1
ğ‘¥2
â‹¯
ğ‘¥ğ‘—
â‹¯
ğ‘¥ğ‘›
ğ‘¦
1
ğ‘¥11
ğ‘¥12
â‹¯
ğ‘¥1ğ‘—
â‹¯
ğ‘¥1ğ‘›
ğ‘¦1
2
ğ‘¥21
ğ‘¥22
â‹¯
ğ‘¥2ğ‘—
â‹¯
ğ‘¥2ğ‘›
ğ‘¦2
â‹®
â‹®
â‹®
â‹®
â‹®
â‹®
â‹®
â‹®
ğ‘
ğ‘¥ğ‘1
ğ‘¥ğ‘2
â‹¯
ğ‘¥ğ‘ğ‘—
â‹¯
ğ‘¥ğ‘ğ‘›
ğ‘¦ğ‘
Training data 
ğ’™ğ‘–, ğ‘¦ğ‘–
ğ‘–=1
ğ‘
ğ’™2

--------------------------------------------------------------------------------
[End of Page 16]

17
Learning GAM
â€¢ Gradient Boosting
ğ‘–
ğ‘¥1
ğ‘¥2
â‹¯
ğ‘¥ğ‘—
â‹¯
ğ‘¥ğ‘›
ğ‘¦
1
ğ‘¥11
ğ‘¥12
â‹¯
ğ‘¥1ğ‘—
â‹¯
ğ‘¥1ğ‘›
ğ‘¦1
2
ğ‘¥21
ğ‘¥22
â‹¯
ğ‘¥2ğ‘—
â‹¯
ğ‘¥2ğ‘›
ğ‘¦2
â‹®
â‹®
â‹®
â‹®
â‹®
â‹®
â‹®
â‹®
ğ‘
ğ‘¥ğ‘1
ğ‘¥ğ‘2
â‹¯
ğ‘¥ğ‘ğ‘—
â‹¯
ğ‘¥ğ‘ğ‘›
ğ‘¦ğ‘
Training data 
ğ’™ğ‘–, ğ‘¦ğ‘–
ğ‘–=1
ğ‘

--------------------------------------------------------------------------------
[End of Page 17]

18
Learning GAM
â€¢ Gradient Boosting
ğ‘–
ğ‘¥1
ğ‘¥2
â‹¯
ğ‘¥ğ‘—
â‹¯
ğ‘¥ğ‘›
ğ‘¦
1
ğ‘¥11
ğ‘¥12
â‹¯
ğ‘¥1ğ‘—
â‹¯
ğ‘¥1ğ‘›
ğ‘¦1
2
ğ‘¥21
ğ‘¥22
â‹¯
ğ‘¥2ğ‘—
â‹¯
ğ‘¥2ğ‘›
ğ‘¦2
â‹®
â‹®
â‹®
â‹®
â‹®
â‹®
â‹®
â‹®
ğ‘
ğ‘¥ğ‘1
ğ‘¥ğ‘2
â‹¯
ğ‘¥ğ‘ğ‘—
â‹¯
ğ‘¥ğ‘ğ‘›
ğ‘¦ğ‘
Training data 
ğ’™ğ‘–, ğ‘¦ğ‘–
ğ‘–=1
ğ‘

--------------------------------------------------------------------------------
[End of Page 18]

19
Learning GAM
â€¢ Gradient Boosting
ğ‘–
ğ‘¥1
ğ‘¥2
â‹¯
ğ‘¥ğ‘—
â‹¯
ğ‘¥ğ‘›
ğ‘¦
1
ğ‘¥11
ğ‘¥12
â‹¯
ğ‘¥1ğ‘—
â‹¯
ğ‘¥1ğ‘›
ğ‘¦1
2
ğ‘¥21
ğ‘¥22
â‹¯
ğ‘¥2ğ‘—
â‹¯
ğ‘¥2ğ‘›
ğ‘¦2
â‹®
â‹®
â‹®
â‹®
â‹®
â‹®
â‹®
â‹®
ğ‘
ğ‘¥ğ‘1
ğ‘¥ğ‘2
â‹¯
ğ‘¥ğ‘ğ‘—
â‹¯
ğ‘¥ğ‘ğ‘›
ğ‘¦ğ‘
Training data 
ğ’™ğ‘–, ğ‘¦ğ‘–
ğ‘–=1
ğ‘
ğ‘¦1 âˆ’à·
ğ‘˜
ğ‘“ğ‘˜
Residuals
ğ‘“ğ‘—
ğ‘¦2 âˆ’à·
ğ‘˜
ğ‘“ğ‘˜
ğ‘¦ğ‘âˆ’à·
ğ‘˜
ğ‘“ğ‘˜
â‹®
(errors made by the 
current model)

--------------------------------------------------------------------------------
[End of Page 19]

20
Learning GAM
â€¢ Gradient Boosting
Update ğ‘“ğ‘—based on 
ğ‘¥ğ‘–ğ‘—, ğ‘¦ğ‘–âˆ’Ïƒğ‘˜ğ‘“ğ‘˜
ğ‘–=1
ğ‘
-
Learn a shape function S that fits: ğ‘¥â†’ğ‘¦
-
Update ğ‘“ğ‘—â†ğ‘“ğ‘—+ ğ‘†
ğ‘¥
ğ‘¦

--------------------------------------------------------------------------------
[End of Page 20]

21
Learning GAM
â€¢ Gradient Boosting
Example
ğ‘¥
ğ‘¦
1
8
5
5
3
8
9
7
ğ‘¥< 4
yes
no
ğ‘¦= 7
ğ‘¦= 3
ğ‘¥< 6
yes
no
ğ‘¦= 2
Residuals
8 âˆ’7 = 1
5 âˆ’3 = 2
8 âˆ’7 = 1
7 âˆ’2 = 5
ğ‘“ğ‘—

--------------------------------------------------------------------------------
[End of Page 21]

22
Learning GAM
â€¢ Gradient Boosting
Example
ğ‘¥
ğ‘¦
1
8
5
5
3
8
9
7
ğ‘¥< 4
yes
no
ğ‘¦= 7
ğ‘¦= 3
ğ‘¥< 6
yes
no
ğ‘¦= 2
Residuals
8 âˆ’7 = 1
5 âˆ’3 = 2
8 âˆ’7 = 1
7 âˆ’2 = 5
ğ‘“ğ‘—

--------------------------------------------------------------------------------
[End of Page 22]

23
Learning GAM
â€¢ Gradient Boosting
Example
ğ‘¥
ğ‘¦
1
8
5
5
3
8
9
7
ğ‘¥< 4
yes
no
1
2
ğ‘¥< 6
yes
no
5
Residuals
8 âˆ’7 = 1
5 âˆ’3 = 2
8 âˆ’7 = 1
7 âˆ’2 = 5
ğ‘†

--------------------------------------------------------------------------------
[End of Page 23]

24
Learning GAM
â€¢ Gradient Boosting
Example
ğ‘¥
ğ‘¦
1
8
5
5
3
8
9
7
Residuals
8 âˆ’(7 + 1) = 0
5 âˆ’3 + 2 = 0
8 âˆ’7 + 1 = 0
7 âˆ’2 + 5 = 0
+
ğ‘†
ğ‘“ğ‘—
Update ğ‘“ğ‘—â†ğ‘“ğ‘—+ ğ‘†

--------------------------------------------------------------------------------
[End of Page 24]

25
Learning GAM
â€¢ Gradient Boosting
Example
ğ‘¥
ğ‘¦
1
8
5
5
3
8
9
7
Residuals
8 âˆ’(7 + 1) = 0
5 âˆ’3 + 2 = 0
8 âˆ’7 + 1 = 0
7 âˆ’2 + 5 = 0
+
ğ‘†
ğ‘“ğ‘—
Update ğ‘“ğ‘—â†ğ‘“ğ‘—+ ğ‘†
Do we learn a 
perfect model?

--------------------------------------------------------------------------------
[End of Page 25]

26
Learning GAM
The model fits training data too well
Source: http://scott.fortmann-roe.com/docs/BiasVariance.html
We have low bias, but 
probably have high variance

--------------------------------------------------------------------------------
[End of Page 26]

27
Learning GAM
â€¢ Gradient Boosting
Example
ğ‘¥
ğ‘¦
1
8
5
5
3
8
9
7
Residuals
8 âˆ’(7 + 0.1 Ã— 1) = 0.9
5 âˆ’3 + 0.1 Ã— 2 = 1.8
8 âˆ’7 + 0.1 Ã— 1 = 0.9
7 âˆ’2 + 0.1 Ã— 5 = 4.5
+
ğ‘†
ğ‘“ğ‘—
Update ğ‘“ğ‘—â†ğ‘“ğ‘—+ ğ›¾Ã— ğ‘†
Add a learning rate to scale the 
contribution of the new tree
ğ›¾Ã—

--------------------------------------------------------------------------------
[End of Page 27]

28
Learning GAM
â€¢ Gradient Boosting
-
Learning tree or tree ensemble shape functions
Algorithm Gradient Boosting for GAM
1.
2.
3.
4.
5.
6.
ğ‘“ğ‘—â†0, ğ‘—= 1, â‹¯, ğ‘›
for ğ‘š= 1, â‹¯, ğ‘€do
for ğ‘—= 1, â‹¯, ğ‘›do
Learning shape function S: ğ‘¥ğ‘—â†’ğ‘¦using â„›as training data
ğ‘“ğ‘—â†ğ‘“ğ‘—+ ğ‘†
Initialize all shape functions as zero
Loop over M iterations
Loop over all features
Learn the one-dimensional 
function to predict the residuals
Update the shape function
â„›â†ğ‘¥ğ‘–ğ‘—, ğ‘¦ğ‘–âˆ’à·
ğ‘˜
ğ‘“ğ‘˜
ğ‘–=1
ğ‘
Calculate residuals

--------------------------------------------------------------------------------
[End of Page 28]

29
Learning GAM
ğ‘¥1
ğ‘¥2
ğ‘¥ğ‘—
ğ‘¥ğ‘›
â‹¯
â‹¯
ğ‘š= 1

--------------------------------------------------------------------------------
[End of Page 29]

30
Learning GAM
ğ‘¥1
ğ‘¥2
ğ‘¥ğ‘—
ğ‘¥ğ‘›
â‹¯
â‹¯
ğ‘š= 1
ğ‘š= 2

--------------------------------------------------------------------------------
[End of Page 30]

31
Learning GAM
ğ‘¥1
ğ‘¥2
ğ‘¥ğ‘—
ğ‘¥ğ‘›
â‹¯
â‹¯
ğ‘š= 1
ğ‘š= 2
â‹¯
ğ‘š= ğ‘€

--------------------------------------------------------------------------------
[End of Page 31]

32
Learning GAM
ğ‘¥1
ğ‘¥2
ğ‘¥ğ‘—
ğ‘¥ğ‘›
â‹¯
â‹¯
ğ‘š= 1
ğ‘š= 2
â‹¯
ğ‘š= ğ‘€

--------------------------------------------------------------------------------
[End of Page 32]

33
Question?

--------------------------------------------------------------------------------
[End of Page 33]

34
Learning GAM
â€¢ Backfitting
-
Learning tree or tree ensemble shape functions
Algorithm Backfitting for GAM
1.
2.
3.
4.
5.
6.
ğ‘“ğ‘—â†0, ğ‘—= 1, â‹¯, ğ‘›
Learn ğ‘“1 using the training set 
ğ‘¥ğ‘–1, ğ‘¦ğ‘–
ğ‘–=1
ğ‘
for ğ‘—= 2, â‹¯, ğ‘›do
â„›â†ğ‘¥ğ‘–ğ‘—, ğ‘¦ğ‘–âˆ’à·
ğ‘˜=1
ğ‘—âˆ’1
ğ‘“ğ‘˜
ğ‘–=1
ğ‘
Learning shape function S: ğ‘¥ğ‘—â†’ğ‘¦using â„›as training data
ğ‘“ğ‘—â†ğ‘†
Initialize all shape functions as zero
Loop over rest features
Calculate residuals
Learn the one-dimensional 
function to predict the residuals
Update the shape function
7. Retrain ğ‘“1 based on the residuals of other ğ‘›âˆ’1 shape functions 

--------------------------------------------------------------------------------
[End of Page 34]

35
Learning GAM
â€¢ Least Squares
-
Learning spline shape functions
-
Reducing to fitting a linear model 
ğ’š= ğ‘¿ğœ·
ğ‘¿ğ’Š= ğ‘¥ğ‘–12,
ğ‘¥ğ‘–2, â‹¯, sin ğ‘¥ğ‘–ğ‘›
ğœ·= ğ›½1, ğ›½2, â‹¯, ğ›½ğ‘›ğ‘‡
ğ‘–ğ‘¡â„example
Objective
min ğ’šâˆ’ğ‘¿ğœ·
2
ğ‘”ğ‘¦= ğ›½1ğ‘¥1
2 + ğ›½2 ğ‘¥2 + â‹¯+ ğ›½ğ‘›sin ğ‘¥ğ‘›

--------------------------------------------------------------------------------
[End of Page 35]

36
Learning GAM
â€¢ Least Squares
-
Learning spline shape functions
-
Reducing to fitting a linear model 
ğ’š= ğ‘¿ğœ·
ğ‘¿ğ’Š= ğ‘¥ğ‘–12,
ğ‘¥ğ‘–2, â‹¯, sin ğ‘¥ğ‘–ğ‘›
ğœ·= ğ›½1, ğ›½2, â‹¯, ğ›½ğ‘›ğ‘‡
ğ‘–ğ‘¡â„example
Objective
min ğ’šâˆ’ğ‘¿ğœ·
2
ğ‘”ğ‘¦= ğ›½1ğ‘¥1
2 + ğ›½2 ğ‘¥2 + â‹¯+ ğ›½ğ‘›sin ğ‘¥ğ‘›
Simple, but not flexible

--------------------------------------------------------------------------------
[End of Page 36]

37
Summary
Generalized additive models (GAMs)
ğ‘”ğ‘¦= ğ‘“1 ğ‘¥1 + ğ‘“2 ğ‘¥2 + â‹¯+ ğ‘“ğ‘›ğ‘¥ğ‘›
â€¢ Training
-
Shape functions for individual features: splines, trees, ensembles of trees
-
Learning methods: Least Squares, Gradient Boosting, Backfitting

--------------------------------------------------------------------------------
[End of Page 37]

38
Summary
Generalized additive models (GAMs)
ğ‘”ğ‘¦= ğ‘“1 ğ‘¥1 + ğ‘“2 ğ‘¥2 + â‹¯+ ğ‘“ğ‘›ğ‘¥ğ‘›
â€¢ Training
-
Shape functions for individual features: splines, trees, ensembles of trees
-
Learning methods: Least Squares, Gradient Boosting, Backfitting
â€¢ Interpretability
ğ‘¥1
ğ‘¥ğ‘›
â‹¯
ğ‘“1 ğ‘¥1
ğ‘“ğ‘›ğ‘¥ğ‘›
â‹¯
contributions

--------------------------------------------------------------------------------
[End of Page 38]

39
Application
â€¢ Dataset: â€œConcreteâ€ (Blast Furnace Slag, Fly Ash, Superplasticizerâ€¦)
â€¢ Models: 
Lou, Yin, Rich Caruana, and Johannes Gehrke. "Intelligible models for classification and regression." Proceedings of the 
18th ACM SIGKDD international conference on Knowledge discovery and data mining. 2012.
â€¢ Task: predicting the compressive strength of concrete

--------------------------------------------------------------------------------
[End of Page 39]

40
Empirical Results
â€¢ GAMs perform better than linear or logistic regression 
(without feature shaping)
Lou, Yin, Rich Caruana, and Johannes Gehrke. "Intelligible models for classification and regression." Proceedings of the 
18th ACM SIGKDD international conference on Knowledge discovery and data mining. 2012.
â€¢ Tree-based shaping methods are more accurate 
than spline-based methods
â€¢ Bagged-trees with 2-4 leaves as shape functions in 
combination with gradient boosting as learning 
method perform better
â€¢ Controlling the complexity of trees can avoid 
overfitting 
(2 leaves)
(16 leaves)

--------------------------------------------------------------------------------
[End of Page 40]

41
Interpretation
Shapes of features for the â€œConcreteâ€ dataset (versus the compressive strength 
of concrete)
Lou, Yin, Rich Caruana, and Johannes Gehrke. "Intelligible models for classification and regression." Proceedings of the 
18th ACM SIGKDD international conference on Knowledge discovery and data mining. 2012.
(Splines)
(Bagged 
trees)

--------------------------------------------------------------------------------
[End of Page 41]

42
Question?

--------------------------------------------------------------------------------
[End of Page 42]

43
GA M
2
Limitation: GAMs do not consider feature dependency
ğ‘“1 ğ‘¥1 + ğ‘“2 ğ‘¥2
ğ‘“1 ğ‘¥1 + ğ‘“2 ğ‘¥2 + ğ‘¥1ğ‘¥2 + â‹¯
Gap
Model
Goal

--------------------------------------------------------------------------------
[End of Page 43]

44
GA M
2
Limitation: GAMs do not consider feature dependency
Add two-dimensional interactions
GA M
2
à·ğ‘“ğ‘–ğ‘¥ğ‘–+ à·ğ‘“ğ‘–,ğ‘—ğ‘¥ğ‘–, ğ‘¥ğ‘—
ğ‘“1 ğ‘¥1 + ğ‘“2 ğ‘¥2
ğ‘“1 ğ‘¥1 + ğ‘“2 ğ‘¥2 + ğ‘¥1ğ‘¥2 + â‹¯
Gap
Model
Goal

--------------------------------------------------------------------------------
[End of Page 44]

45
GA M
2
Definitions
â€¢ Dataset ğ·=
ğ’™ğ‘–, ğ‘¦ğ‘–
ğ‘–=1
ğ‘
â€¢ ğ’™ğ‘–= ğ‘¥ğ‘–1, â‹¯, ğ‘¥ğ‘–ğ‘›with ğ‘›features
â€¢ ğ‘¦ğ‘–is the response
â€¢ ğ’™= ğ‘¥1, â‹¯, ğ‘¥ğ‘›denote the features in the dataset
â€¢ ğ‘ˆ1 =
ğ‘–1 â‰¤ğ‘–â‰¤ğ‘›,ğ‘ˆ2 =
ğ‘–, ğ‘—1 â‰¤ğ‘–< ğ‘—â‰¤ğ‘›, ğ‘ˆ= ğ‘ˆ1 âˆªğ‘ˆ2, i.e., ğ‘ˆ
contains all indices for all features and pairs of features
â€¢ For any ğ‘¢âˆˆğ‘ˆ, let ğ»ğ‘¢denote the Hilbert space of ğ‘“ğ‘¢ğ‘¥ğ‘¢
â€¢ ğ»= Ïƒğ‘¢âˆˆğ‘ˆğ»ğ‘¢, ğ»1 = Ïƒğ‘¢âˆˆğ‘ˆ1 ğ»ğ‘¢, ğ»2 = Ïƒğ‘¢âˆˆğ‘ˆ2 ğ»ğ‘¢

--------------------------------------------------------------------------------
[End of Page 45]

46
GA M
2
GA M
2
ğ¹ğ’™= Ïƒğ‘¢âˆˆğ‘ˆğ‘“ğ‘¢ğ‘¥ğ‘¢
Objective
min
ğ¹âˆˆğ»ğ¸ğ¿(ğ‘¦, ğ¹ğ’™)
ğ¿: non-negative convex loss function
regression
classification
Squared loss
Cross-entropy loss
âˆ’ğ‘¦log ğ¹ğ’™âˆ’(1 âˆ’ğ‘¦) log(1 âˆ’ğ¹ğ’™)
(ğ‘¦âˆ’ğ¹ğ’™)ğŸ

--------------------------------------------------------------------------------
[End of Page 46]

47
GA M
2
â€¢ We have known how to learn shape functions for GAMs
â€¢ Applicable to two-dimensional shape functions ğ‘“ğ‘¢, ğ‘¢= {ğ‘–, ğ‘—}
Splines
ğ‘“1,2 = ğ‘¥1ğ‘¥2
ğ‘¥1
ğ‘¥2
Trees
ğ‘¥1 < 5
yes
no
ğ‘¦= 1
ğ‘¦= 0
ğ‘¥2 < 7
yes
no
ğ‘¦= âˆ’1
ğ‘¥2 < 3
yes
no
ğ‘¦= 2

--------------------------------------------------------------------------------
[End of Page 47]

48
GA M
2
Challenge
ğ‘›features
ğ‘‚(ğ‘›2) features interactions
How to find true 
feature interactions?
à·ğ‘“ğ‘–ğ‘¥ğ‘–+ à·ğ‘“ğ‘–,ğ‘—ğ‘¥ğ‘–, ğ‘¥ğ‘—

--------------------------------------------------------------------------------
[End of Page 48]

49
GA M
2
Algorithm 
1.
2.
3.
4.
5.
6.
ğ‘†â†âˆ…
ğ‘â†ğ‘ˆ2
While not converge do
The set of the selected pairs
GA M
2
7.
8.
9.
10.
ğ¹â†arg
min
ğ¹âˆˆğ»1+Ïƒğ‘¢âˆˆğ‘†ğ»ğ‘¢
1
2 ğ¸
ğ‘¦âˆ’ğ¹ğ’™
2
ğ‘…â†ğ‘¦âˆ’ğ¹ğ’™
for all ğ‘¢âˆˆğ‘do
ğ¹ğ‘¢â†ğ¸àµ£ğ‘…| ğ‘¥ğ‘¢]
ğ‘¢âˆ—â†arg min
ğ‘¢âˆˆğ‘
1
2 ğ¸(ğ‘…âˆ’ğ‘“ğ‘¢ğ‘¥ğ‘¢)2
ğ‘†â†ğ‘†âˆªğ‘¢âˆ—
ğ‘â†ğ‘âˆ’ğ‘¢âˆ—
The set of the remaining pairs

--------------------------------------------------------------------------------
[End of Page 49]

50
GA M
2
Algorithm 
1.
2.
3.
4.
5.
6.
ğ‘†â†âˆ…
ğ‘â†ğ‘ˆ2
While not converge do
The set of the selected pairs
GA M
2
7.
8.
9.
10.
ğ¹â†arg
min
ğ¹âˆˆğ»1+Ïƒğ‘¢âˆˆğ‘†ğ»ğ‘¢
1
2 ğ¸
ğ‘¦âˆ’ğ¹ğ’™
2
ğ‘…â†ğ‘¦âˆ’ğ¹ğ’™
for all ğ‘¢âˆˆğ‘do
ğ¹ğ‘¢â†ğ¸àµ£ğ‘…| ğ‘¥ğ‘¢]
ğ‘¢âˆ—â†arg min
ğ‘¢âˆˆğ‘
1
2 ğ¸(ğ‘…âˆ’ğ‘“ğ‘¢ğ‘¥ğ‘¢)2
ğ‘†â†ğ‘†âˆªğ‘¢âˆ—
ğ‘â†ğ‘âˆ’ğ‘¢âˆ—
The set of the remaining pairs
The best additive model ğ¹so far in Hilbert 
space ğ»1 + Ïƒğ‘¢âˆˆğ‘†ğ»ğ‘¢
Learning shape functions for all 
single features (ğ‘“ğ‘–ğ‘¥ğ‘–) and the 
selected feature pairs (ğ‘“ğ‘–,ğ‘—ğ‘¥ğ‘–, ğ‘¥ğ‘—). 
When ğ‘†= âˆ…, ğ¹is the GAM.

--------------------------------------------------------------------------------
[End of Page 50]

51
GA M
2
Algorithm 
1.
2.
3.
4.
5.
6.
ğ‘†â†âˆ…
ğ‘â†ğ‘ˆ2
While not converge do
The set of the selected pairs
GA M
2
7.
8.
9.
10.
ğ¹â†arg
min
ğ¹âˆˆğ»1+Ïƒğ‘¢âˆˆğ‘†ğ»ğ‘¢
1
2 ğ¸
ğ‘¦âˆ’ğ¹ğ’™
2
ğ‘…â†ğ‘¦âˆ’ğ¹ğ’™
for all ğ‘¢âˆˆğ‘do
ğ¹ğ‘¢â†ğ¸àµ£ğ‘…| ğ‘¥ğ‘¢]
ğ‘¢âˆ—â†arg min
ğ‘¢âˆˆğ‘
1
2 ğ¸(ğ‘…âˆ’ğ‘“ğ‘¢ğ‘¥ğ‘¢)2
ğ‘†â†ğ‘†âˆªğ‘¢âˆ—
ğ‘â†ğ‘âˆ’ğ‘¢âˆ—
The set of the remaining pairs
The best additive model ğ¹so far in Hilbert 
space ğ»1 + Ïƒğ‘¢âˆˆğ‘†ğ»ğ‘¢
Calculate residual

--------------------------------------------------------------------------------
[End of Page 51]

52
GA M
2
Algorithm 
1.
2.
3.
4.
5.
6.
ğ‘†â†âˆ…
ğ‘â†ğ‘ˆ2
While not converge do
The set of the selected pairs
GA M
2
7.
8.
9.
10.
ğ¹â†arg
min
ğ¹âˆˆğ»1+Ïƒğ‘¢âˆˆğ‘†ğ»ğ‘¢
1
2 ğ¸
ğ‘¦âˆ’ğ¹ğ’™
2
ğ‘…â†ğ‘¦âˆ’ğ¹ğ’™
for all ğ‘¢âˆˆğ‘do
ğ¹ğ‘¢â†ğ¸àµ£ğ‘…| ğ‘¥ğ‘¢]
ğ‘¢âˆ—â†arg min
ğ‘¢âˆˆğ‘
1
2 ğ¸(ğ‘…âˆ’ğ‘“ğ‘¢ğ‘¥ğ‘¢)2
ğ‘†â†ğ‘†âˆªğ‘¢âˆ—
ğ‘â†ğ‘âˆ’ğ‘¢âˆ—
The set of the remaining pairs
The best additive model ğ¹so far in Hilbert 
space ğ»1 + Ïƒğ‘¢âˆˆğ‘†ğ»ğ‘¢
Calculate residual
Build an interaction model on the residual
Loop over all remaining feature pairs
Learning a shape function 
for each feature pair

--------------------------------------------------------------------------------
[End of Page 52]

53
GA M
2
Algorithm 
1.
2.
3.
4.
5.
6.
ğ‘†â†âˆ…
ğ‘â†ğ‘ˆ2
While not converge do
The set of the selected pairs
GA M
2
7.
8.
9.
10.
ğ¹â†arg
min
ğ¹âˆˆğ»1+Ïƒğ‘¢âˆˆğ‘†ğ»ğ‘¢
1
2 ğ¸
ğ‘¦âˆ’ğ¹ğ’™
2
ğ‘…â†ğ‘¦âˆ’ğ¹ğ’™
for all ğ‘¢âˆˆğ‘do
ğ¹ğ‘¢â†ğ¸àµ£ğ‘…| ğ‘¥ğ‘¢]
ğ‘¢âˆ—â†arg min
ğ‘¢âˆˆğ‘
1
2 ğ¸(ğ‘…âˆ’ğ‘“ğ‘¢ğ‘¥ğ‘¢)2
ğ‘†â†ğ‘†âˆªğ‘¢âˆ—
ğ‘â†ğ‘âˆ’ğ‘¢âˆ—
The set of the remaining pairs
The best additive model ğ¹so far in Hilbert 
space ğ»1 + Ïƒğ‘¢âˆˆğ‘†ğ»ğ‘¢
Calculate residual
Build an interaction model on the residual
Loop over all remaining feature pairs
Select the best feature pair

--------------------------------------------------------------------------------
[End of Page 53]

54
GA M
2
Algorithm 
1.
2.
3.
4.
5.
6.
ğ‘†â†âˆ…
ğ‘â†ğ‘ˆ2
While not converge do
The set of the selected pairs
GA M
2
7.
8.
9.
10.
ğ¹â†arg
min
ğ¹âˆˆğ»1+Ïƒğ‘¢âˆˆğ‘†ğ»ğ‘¢
1
2 ğ¸
ğ‘¦âˆ’ğ¹ğ’™
2
ğ‘…â†ğ‘¦âˆ’ğ¹ğ’™
for all ğ‘¢âˆˆğ‘do
ğ¹ğ‘¢â†ğ¸àµ£ğ‘…| ğ‘¥ğ‘¢]
ğ‘¢âˆ—â†arg min
ğ‘¢âˆˆğ‘
1
2 ğ¸(ğ‘…âˆ’ğ‘“ğ‘¢ğ‘¥ğ‘¢)2
ğ‘†â†ğ‘†âˆªğ‘¢âˆ—
ğ‘â†ğ‘âˆ’ğ‘¢âˆ—
The set of the remaining pairs
The best additive model ğ¹so far in Hilbert 
space ğ»1 + Ïƒğ‘¢âˆˆğ‘†ğ»ğ‘¢
Calculate residual
Build an interaction model on the residual
Loop over all remaining feature pairs
Select the best feature pair
Put the best feature pair in ğ‘†
Remove that from ğ‘

--------------------------------------------------------------------------------
[End of Page 54]

55
GA M
2
Algorithm 
1.
2.
3.
4.
5.
6.
ğ‘†â†âˆ…
ğ‘â†ğ‘ˆ2
While not converge do
The set of the selected pairs
GA M
2
7.
8.
9.
10.
ğ¹â†arg
min
ğ¹âˆˆğ»1+Ïƒğ‘¢âˆˆğ‘†ğ»ğ‘¢
1
2 ğ¸
ğ‘¦âˆ’ğ¹ğ’™
2
ğ‘…â†ğ‘¦âˆ’ğ¹ğ’™
for all ğ‘¢âˆˆğ‘do
ğ¹ğ‘¢â†ğ¸àµ£ğ‘…| ğ‘¥ğ‘¢]
ğ‘¢âˆ—â†arg min
ğ‘¢âˆˆğ‘
1
2 ğ¸(ğ‘…âˆ’ğ‘“ğ‘¢ğ‘¥ğ‘¢)2
ğ‘†â†ğ‘†âˆªğ‘¢âˆ—
ğ‘â†ğ‘âˆ’ğ‘¢âˆ—
The set of the remaining pairs
The best additive model ğ¹so far in Hilbert 
space ğ»1 + Ïƒğ‘¢âˆˆğ‘†ğ»ğ‘¢
Calculate residual
Build an interaction model on the residual
Loop over all remaining feature pairs
Select the best feature pair
Put the best feature pair in ğ‘†
Remove that from ğ‘
ğ‘‚(ğ‘›2)
See a fast interaction 
detection algorithm in 
[Lou et al., 2013]

--------------------------------------------------------------------------------
[End of Page 55]

56
GA M
2
GAM
ğ‘¥1
ğ‘¥2
ğ‘¥ğ‘›
â‹¯
GA M
2
ğ‘†(selected pairs)
ğ‘(Remaining pairs)
(ğ‘¥ğ‘–, ğ‘¥ğ‘—)
Residuals
Select a feature pair 
that minimizes the 
residual
1
2
3

--------------------------------------------------------------------------------
[End of Page 56]

57
Question?

--------------------------------------------------------------------------------
[End of Page 57]

58
Application
â€œIntelligible models for healthcare: Predicting pneumonia risk and 
hospital 30-day readmissionâ€
Caruana et al., KDD 2015 

--------------------------------------------------------------------------------
[End of Page 58]

59
Background
â€¢
In the mid 90â€™s, a project was funded by Cost-Effective HealthCare (CEHC) to evaluate the 
application of machine learning to important problems in healthcare such as predicting 
pneumonia risk
â€¢
Goal: predict the probability of death (POD) for patients with pneumonia
â€¢
High-risk: patients could be admitted to the hospital
â€¢
Low-risk: patients were treated as outpatients

--------------------------------------------------------------------------------
[End of Page 59]

60
Background
â€¢
In the mid 90â€™s, a project was funded by Cost-Effective HealthCare (CEHC) to evaluate the 
application of machine learning to important problems in healthcare such as predicting 
pneumonia risk
â€¢
Goal: predict the probability of death (POD) for patients with pneumonia
â€¢
High-risk: patients could be admitted to the hospital
â€¢
Low-risk: patients were treated as outpatients
Models
Logistic regression
Rule-based learning
k-nearest neighbor
Neural networks
â€¦

--------------------------------------------------------------------------------
[End of Page 60]

61
Background
â€¢
In the mid 90â€™s, a project was funded by Cost-Effective HealthCare (CEHC) to evaluate the 
application of machine learning to important problems in healthcare such as predicting 
pneumonia risk
â€¢
Goal: predict the probability of death (POD) for patients with pneumonia
â€¢
High-risk: patients could be admitted to the hospital
â€¢
Low-risk: patients were treated as outpatients
Models
Logistic regression
Rule-based learning
k-nearest neighbor
Neural networks
â€¦
AUC=0.86 (Best performance)

--------------------------------------------------------------------------------
[End of Page 61]

62
Background
â€¢
In the mid 90â€™s, a project was funded by Cost-Effective HealthCare (CEHC) to evaluate the 
application of machine learning to important problems in healthcare such as predicting 
pneumonia risk
â€¢
Goal: predict the probability of death (POD) for patients with pneumonia
â€¢
High-risk: patients could be admitted to the hospital
â€¢
Low-risk: patients were treated as outpatients
Models
Logistic regression
Rule-based learning
k-nearest neighbor
Neural networks
â€¦
AUC=0.86 (Best performance)
AUC=0.77 (safer to use on patients)

--------------------------------------------------------------------------------
[End of Page 62]

63
Problem
Rule-based models are interpretable
if (chance_of_rain > 0.75) 
{ umbrella <- "yes" }
else { umbrella <- "no" }
The rule-based model learned a rule:
HasAsthama(x)  âŸ¹LowerRisk(x)

--------------------------------------------------------------------------------
[End of Page 63]

64
Problem
The rule-based model learned a rule:
HasAsthama(x)  âŸ¹LowerRisk(x)
counterintuitive

--------------------------------------------------------------------------------
[End of Page 64]

65
Problem
The rule-based model learned a rule:
HasAsthama(x)  âŸ¹LowerRisk(x)
counterintuitive
The model captures a true pattern in the training data: 
Patients: 
asthma + 
pneumonia 
Hospital (ICU)
The aggressive care 
lowered the risk of 
dying from pneumonia

--------------------------------------------------------------------------------
[End of Page 65]

66
Problem
The rule-based model learned a rule:
HasAsthama(x)  âŸ¹LowerRisk(x)
Asthmatics have much higher risk!
It would be dangerous if the model predicts low 
risk on patients who have not been hospitalized

--------------------------------------------------------------------------------
[End of Page 66]

67
Problem
How about other potential patterns?
Pregnancy  âŸ¹Lower Risk ?

--------------------------------------------------------------------------------
[End of Page 67]

68
Problem
How about other potential patterns?
Pregnancy  âŸ¹Lower Risk ?
MUST understand ML models in healthcare. 
Otherwise, models may hurt patients 
because of true patterns in data! 

--------------------------------------------------------------------------------
[End of Page 68]

69
Generalized Additive Models
â€¢ Better prediction performance than logistic regression 
(capture more data patterns)
â€¢ Interpretable

--------------------------------------------------------------------------------
[End of Page 69]

70
Case Study: Pneumonia Risk
â€¢ There are 46 features 
describing each patient
â€¢ Bagged trees with 
gradient boosting

--------------------------------------------------------------------------------
[End of Page 70]

71
Prediction Performance
AUC for different learning methods
AUC: Area Under the ROC Curve
ğ‘‡ğ‘ƒğ‘…=
ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ ğ¹ğ‘
ğ¹ğ‘ƒğ‘…=
ğ¹ğ‘ƒ
ğ¹ğ‘ƒ+ ğ‘‡ğ‘

--------------------------------------------------------------------------------
[End of Page 71]

72
Interpretation
Older people have 
higher risk
(Blood Urea Nitrogen)

--------------------------------------------------------------------------------
[End of Page 72]

73
Interpretation
GAMs also found the 
pattern: asthma lowers 
the risk
(Blood Urea Nitrogen)

--------------------------------------------------------------------------------
[End of Page 73]

74
Interpretation
GAMs also found the 
pattern: asthma lowers 
the risk
(Blood Urea Nitrogen)
Repair: eliminate this 
term

--------------------------------------------------------------------------------
[End of Page 74]

75
Interpretation
(Blood Urea Nitrogen)
Most patients have BUN=0 because, 
as in many medical datasets, if the 
variable is not measured or assumed 
normal it is coded as 0

--------------------------------------------------------------------------------
[End of Page 75]

76
Interpretation
(Blood Urea Nitrogen)
BUN levels below 30 appear to 
be low risk, while levels from 
50-200 indicate higher risk

--------------------------------------------------------------------------------
[End of Page 76]

77
Interpretation
(Blood Urea Nitrogen)
Having cancer significantly 
increases the risk of dying 
from pneumonia

--------------------------------------------------------------------------------
[End of Page 77]

78
Interpretation
(Blood Urea Nitrogen)
Chronic lung disease and a
history of chest pain both lower 
risk (similar problem as asthma)

--------------------------------------------------------------------------------
[End of Page 78]

79
Interpretation
-
Risk is highest for the youngest patients 
-
It declines for patients who acquire cancer 
later in life
-
For patients without cancer, risk rises as 
expected with age
Old people with high respiration 
rate have the highest risk

--------------------------------------------------------------------------------
[End of Page 79]

80
Takeaway
â€¢ If a model contains a modest number of terms (e.g., less than 50), it is 
best to show terms in the model to experts in the order they are most 
familiar with
â€¢ When the number of terms grows large, it is best to provide a well-defined 
ordering of the terms for a patient (from terms that increase risk most to 
terms that decrease risk most)

--------------------------------------------------------------------------------
[End of Page 80]

81
Question?

--------------------------------------------------------------------------------
[End of Page 81]

82
Reference
â€¢
Lou, Yin, Rich Caruana, and Johannes Gehrke. "Intelligible models for classification and regression." Proceedings of the 18th ACM 
SIGKDD international conference on Knowledge discovery and data mining. 2012.
â€¢
Lou, Yin, et al. "Accurate intelligible models with pairwise interactions." Proceedings of the 19th ACM SIGKDD international 
conference on Knowledge discovery and data mining. 2013.
â€¢
Caruana, Rich, et al. "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission." Proceedings 
of the 21th ACM SIGKDD international conference on knowledge discovery and data mining. 2015.

--------------------------------------------------------------------------------
[End of Page 82]