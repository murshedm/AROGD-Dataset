CS 4774 Machine Learning
Model Selection and Validation
Yangfeng Ji
Information and Language Processing Lab
Department of Computer Science
University of Virginia

--------------------------------------------------------------------------------
[End of Page 1]

Overview
1. Overview
2. Model Validation
3. Model Selection
4. Model Selection in Practice
1

--------------------------------------------------------------------------------
[End of Page 2]

Overview

--------------------------------------------------------------------------------
[End of Page 3]

Polynominals
Polynomial regression
(a) ğ‘‘= 1
(b) ğ‘‘= 3
(c) ğ‘‘= 15
3

--------------------------------------------------------------------------------
[End of Page 4]

Model Evaluation and Selection
Since we cannot compute the true error of any given hypothesis
â„âˆˆH
â–¶How to evaluate the performance for a given model?
â–¶How to select the best model among a few candidates?
4

--------------------------------------------------------------------------------
[End of Page 5]

Model Validation

--------------------------------------------------------------------------------
[End of Page 6]

Validation Set
The simplest way to estimate the true error of a predictor â„
â–¶Independently sample an additional set of examples ğ‘‰with size
ğ‘šğ‘£
ğ‘‰= {(ğ’™1, ğ‘¦1), . . . , (ğ’™ğ‘šğ‘£, ğ‘¦ğ‘šğ‘£)}
(1)
â–¶Evaluate the predictor â„on this validation set
ğ¿ğ‘‰(â„) = |{ğ‘–âˆˆ[ğ‘šğ‘£] : â„(ğ’™) â‰ ğ‘¦ğ‘–}|
ğ‘šğ‘£
.
(2)
Usually, ğ¿ğ‘‰(â„) is a good approximation to ğ¿D(â„)
6

--------------------------------------------------------------------------------
[End of Page 7]

Model Selection

--------------------------------------------------------------------------------
[End of Page 8]

Model Selection Procedure
Given the training set ğ‘†and the validation set ğ‘‰
â–¶For each model configuration ğ‘, find the best hypothesis â„ğ‘(ğ’™, ğ‘†)
â„ğ‘(ğ’™, ğ‘†) = argmin
â„â€²âˆˆHğ‘
ğ¿ğ‘†(â„â€²(ğ’™, ğ‘†))
(3)
8

--------------------------------------------------------------------------------
[End of Page 9]

Model Selection Procedure
Given the training set ğ‘†and the validation set ğ‘‰
â–¶For each model configuration ğ‘, find the best hypothesis â„ğ‘(ğ’™, ğ‘†)
â„ğ‘(ğ’™, ğ‘†) = argmin
â„â€²âˆˆHğ‘
ğ¿ğ‘†(â„â€²(ğ’™, ğ‘†))
(3)
â–¶With a collection of best models with different configurations
Hâ€² = {â„ğ‘1(ğ’™, ğ‘†), . . . , â„ğ‘ğ‘˜(ğ’™, ğ‘†)}, find the overall best hypothesis
â„(ğ’™, ğ‘†) = argmin
â„â€²âˆˆHâ€²
ğ¿ğ‘‰(â„â€²(ğ’™, ğ‘†))
(4)
8

--------------------------------------------------------------------------------
[End of Page 10]

Model Selection Procedure
Given the training set ğ‘†and the validation set ğ‘‰
â–¶For each model configuration ğ‘, find the best hypothesis â„ğ‘(ğ’™, ğ‘†)
â„ğ‘(ğ’™, ğ‘†) = argmin
â„â€²âˆˆHğ‘
ğ¿ğ‘†(â„â€²(ğ’™, ğ‘†))
(3)
â–¶With a collection of best models with different configurations
Hâ€² = {â„ğ‘1(ğ’™, ğ‘†), . . . , â„ğ‘ğ‘˜(ğ’™, ğ‘†)}, find the overall best hypothesis
â„(ğ’™, ğ‘†) = argmin
â„â€²âˆˆHâ€²
ğ¿ğ‘‰(â„â€²(ğ’™, ğ‘†))
(4)
â–¶It is similar to learn with the finite hypothesis space Hâ€²
8

--------------------------------------------------------------------------------
[End of Page 11]

Model Configuration/Hyperparameters
Consider polynomial regression
Hğ‘‘= {ğ‘¤0 + ğ‘¤1ğ‘¥+ Â· Â· Â· + ğ‘¤ğ‘‘ğ‘¥ğ‘‘: ğ‘¤0, ğ‘¤1, . . . , ğ‘¤ğ‘‘âˆˆâ„}
(5)
â–¶the degree of polynomials ğ‘‘
â–¶regularization coefficient ğœ†as in ğœ†Â· âˆ¥ğ’˜âˆ¥2
2
â–¶the bias term ğ‘¤0
9

--------------------------------------------------------------------------------
[End of Page 12]

Model Configuration/Hyperparameters
Consider polynomial regression
Hğ‘‘= {ğ‘¤0 + ğ‘¤1ğ‘¥+ Â· Â· Â· + ğ‘¤ğ‘‘ğ‘¥ğ‘‘: ğ‘¤0, ğ‘¤1, . . . , ğ‘¤ğ‘‘âˆˆâ„}
(5)
â–¶the degree of polynomials ğ‘‘
â–¶regularization coefficient ğœ†as in ğœ†Â· âˆ¥ğ’˜âˆ¥2
2
â–¶the bias term ğ‘¤0
Additional factors during learning
â–¶Optimization methods
â–¶Dimensionality of inputs, etc.
9

--------------------------------------------------------------------------------
[End of Page 13]

Limitation of Keeping a Validation Set
If the validation set is
â–¶small, then it could be biased and could not give a good
approximation to the true error
â–¶large, e.g., the same order of the training set, then we waste the
information if do not use the examples for training.
10

--------------------------------------------------------------------------------
[End of Page 14]

ğ‘˜-Fold Cross Validation
The basic procedure of ğ‘˜-fold cross validation:
â–¶Split the whole data set into ğ‘˜parts
Data
11

--------------------------------------------------------------------------------
[End of Page 15]

ğ‘˜-Fold Cross Validation
The basic procedure of ğ‘˜-fold cross validation:
â–¶Split the whole data set into ğ‘˜parts
â–¶For each model configuration, run the learning procedure ğ‘˜
times
â–¶Each time, pick one part as validation set and the rest as training set
Fold 1
Fold 2
Fold 3
Fold 4
Fold 5
11

--------------------------------------------------------------------------------
[End of Page 16]

ğ‘˜-Fold Cross Validation
The basic procedure of ğ‘˜-fold cross validation:
â–¶Split the whole data set into ğ‘˜parts
â–¶For each model configuration, run the learning procedure ğ‘˜
times
â–¶Each time, pick one part as validation set and the rest as training set
â–¶Take the average of ğ‘˜validation errors as the model error
Fold 1
Fold 2
Fold 3
Fold 4
Fold 5
11

--------------------------------------------------------------------------------
[End of Page 17]

Cross-Validation Algorithm
1: Input: (1) training set ğ‘†; (2) set of parameter values Î˜; (3)
learning algorithm ğ´, and (4) integer ğ‘˜
2: Partition ğ‘†into ğ‘†1, ğ‘†2, . . . , ğ‘†ğ‘˜
3: for ğœƒğ‘¡âˆˆÎ˜ do
4:
for ğ‘–= 1, . . . , ğ‘˜do
5:
â„ğ‘–,ğœƒğ‘¡= ğ´(ğ‘†\ğ‘†ğ‘–; ğœƒğ‘¡)
6:
end for
7:
Err(ğœƒğ‘¡) = 1
ğ‘˜
Ãğ‘˜
ğ‘–=1 ğ¿ğ‘†ğ‘–(â„ğ‘–,ğœƒğ‘¡)
8: end for
9: Output: Ë†ğœƒâ†argminğœƒğ‘¡âˆˆÎ˜ Err(ğœƒğ‘¡)
In practice, ğ‘˜is usually 5 or 10.
12

--------------------------------------------------------------------------------
[End of Page 18]

Train-Validation-Test Split
â–¶Training set: used for learning with a pre-selected hypothesis
space, such as
â–¶logistic regression for classification
â–¶polynomial regression with ğ‘‘= 15 and ğœ†= 0.1
â–¶Validation set: used for selecting the best hypothesis across
multiple hypothesis spaces
â–¶Similar to learning with a finite hypothesis space Hâ€²
â–¶Test set: only used for evaluating the overall best hypothesis
13

--------------------------------------------------------------------------------
[End of Page 19]

Train-Validation-Test Split
â–¶Training set: used for learning with a pre-selected hypothesis
space, such as
â–¶logistic regression for classification
â–¶polynomial regression with ğ‘‘= 15 and ğœ†= 0.1
â–¶Validation set: used for selecting the best hypothesis across
multiple hypothesis spaces
â–¶Similar to learning with a finite hypothesis space Hâ€²
â–¶Test set: only used for evaluating the overall best hypothesis
Typical splits on all available data
Train
Val
Test
13

--------------------------------------------------------------------------------
[End of Page 20]

Train-Validation-Test Split
â–¶Training set: used for learning with a pre-selected hypothesis
space, such as
â–¶logistic regression for classification
â–¶polynomial regression with ğ‘‘= 15 and ğœ†= 0.1
â–¶Validation set: used for selecting the best hypothesis across
multiple hypothesis spaces
â–¶Similar to learning with a finite hypothesis space Hâ€²
â–¶Test set: only used for evaluating the overall best hypothesis
Typical splits on all available data
Fold 1
Fold 2
Fold 3
Fold 4
Fold 5
Test
13

--------------------------------------------------------------------------------
[End of Page 21]

Model Selection in Practice

--------------------------------------------------------------------------------
[End of Page 22]

What To Do If A Learning Fails
There are many elements that can help fix the learning procedure
â–¶Get a larger sample
[Shalev-Shwartz and Ben-David, 2014, Page 151]
15

--------------------------------------------------------------------------------
[End of Page 23]

What To Do If A Learning Fails
There are many elements that can help fix the learning procedure
â–¶Get a larger sample
â–¶Change the hypothesis class by
â–¶Enlarging it
â–¶Reducing it
â–¶Completely changing it
â–¶Changing the parameters you consider
[Shalev-Shwartz and Ben-David, 2014, Page 151]
15

--------------------------------------------------------------------------------
[End of Page 24]

What To Do If A Learning Fails
There are many elements that can help fix the learning procedure
â–¶Get a larger sample
â–¶Change the hypothesis class by
â–¶Enlarging it
â–¶Reducing it
â–¶Completely changing it
â–¶Changing the parameters you consider
â–¶Change the feature representation of the data (usually domain
dependent)
[Shalev-Shwartz and Ben-David, 2014, Page 151]
15

--------------------------------------------------------------------------------
[End of Page 25]

What To Do If A Learning Fails
There are many elements that can help fix the learning procedure
â–¶Get a larger sample
â–¶Change the hypothesis class by
â–¶Enlarging it
â–¶Reducing it
â–¶Completely changing it
â–¶Changing the parameters you consider
â–¶Change the feature representation of the data (usually domain
dependent)
â–¶Change the optimization algorithm used to apply your learning
rule (lecture on optimization methods)
[Shalev-Shwartz and Ben-David, 2014, Page 151]
15

--------------------------------------------------------------------------------
[End of Page 26]

Error Decomposition Using Validation
With two additional terms
â–¶ğ¿ğ‘‰(â„ğ‘†): validation error
â–¶ğ¿ğ‘†(â„ğ‘†): empirical (or training) error
the true error of â„ğ‘†can be decomposed as
ğ¿D(â„ğ‘†) = (ğ¿D(â„ğ‘†) âˆ’ğ¿ğ‘‰(â„ğ‘†))
|                 {z                 }
(1)
+ (ğ¿ğ‘‰(â„ğ‘†) âˆ’ğ¿ğ‘†(â„ğ‘†))
|                {z                }
(2)
+ ğ¿ğ‘†(â„ğ‘†)
| {z }
(3)
â–¶Item (1) is bounded by the previous theorem
â–¶Item (2) is large: overfitting
â–¶Item (3) is large: underfitting
16

--------------------------------------------------------------------------------
[End of Page 27]

About Large ğ¿ğ‘†(â„ğ‘†)
Recall that â„ğ‘†is an ERM hypothesis, aka
â„ğ‘†âˆˆargmin
â„â€²âˆˆH
ğ¿ğ‘†(â„â€²)
(6)
17

--------------------------------------------------------------------------------
[End of Page 28]

About Large ğ¿ğ‘†(â„ğ‘†)
Recall that â„ğ‘†is an ERM hypothesis, aka
â„ğ‘†âˆˆargmin
â„â€²âˆˆH
ğ¿ğ‘†(â„â€²)
(6)
If ğ¿ğ‘†(â„ğ‘†) is large, it is possible that
1. the hypothesis space His not large enough
2. the hypothesis space is large enough, but your implementation
has some bugs
17

--------------------------------------------------------------------------------
[End of Page 29]

About Large ğ¿ğ‘†(â„ğ‘†)
Recall that â„ğ‘†is an ERM hypothesis, aka
â„ğ‘†âˆˆargmin
â„â€²âˆˆH
ğ¿ğ‘†(â„â€²)
(6)
If ğ¿ğ‘†(â„ğ‘†) is large, it is possible that
1. the hypothesis space His not large enough
2. the hypothesis space is large enough, but your implementation
has some bugs
Q: How to distinguish these two?
17

--------------------------------------------------------------------------------
[End of Page 30]

About Large ğ¿ğ‘†(â„ğ‘†)
Recall that â„ğ‘†is an ERM hypothesis, aka
â„ğ‘†âˆˆargmin
â„â€²âˆˆH
ğ¿ğ‘†(â„â€²)
(6)
If ğ¿ğ‘†(â„ğ‘†) is large, it is possible that
1. the hypothesis space His not large enough
2. the hypothesis space is large enough, but your implementation
has some bugs
Q: How to distinguish these two?
A: Find an existing simple baseline model
17

--------------------------------------------------------------------------------
[End of Page 31]

About Large ğ¿ğ‘‰(â„ğ‘†)
... with a small ğ¿ğ‘†(â„ğ‘†), it is possible that
1. the hypothesis space is too large
2. you may not have enough training examples
3. the hypothesis space is inappropriate
18

--------------------------------------------------------------------------------
[End of Page 32]

About Large ğ¿ğ‘‰(â„ğ‘†)
... with a small ğ¿ğ‘†(â„ğ‘†), it is possible that
1. the hypothesis space is too large
2. you may not have enough training examples
3. the hypothesis space is inappropriate
Comments
â–¶Issue 1 and 2 are easy to fix
â–¶Get more data if possible, or reduce the hypothesis space
â–¶How to distinguish issue 3 from 1 and 2?
18

--------------------------------------------------------------------------------
[End of Page 33]

Learning Curves
With different proportions of training examples, we can plot the
training and validation errors
(a)
Figure: Examples of learning curves [Shalev-Shwartz and Ben-David, 2014,
Page 153].
19

--------------------------------------------------------------------------------
[End of Page 34]

Learning Curves
With different proportions of training examples, we can plot the
training and validation errors
(a)
(b)
Figure: Examples of learning curves [Shalev-Shwartz and Ben-David, 2014,
Page 153].
19

--------------------------------------------------------------------------------
[End of Page 35]

Reference
Shalev-Shwartz, S. and Ben-David, S. (2014).
Understanding machine learning: From theory to algorithms.
Cambridge university press.
20

--------------------------------------------------------------------------------
[End of Page 36]