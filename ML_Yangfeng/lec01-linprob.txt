CS 4774 Machine Learning
Introduction
Yangfeng Ji
Information and Language Processing Lab
Department of Computer Science
University of Virginia

--------------------------------------------------------------------------------
[End of Page 1]

Course Instructor and Teaching Assistants
â–¶Instructor:
â–¶Yangfeng Ji
â–¶Office: Rice 510
â–¶Teaching assistants
â–¶Oishee Hoque
â–¶Aparna Kishore
â–¶Nusrat Mozumder
â–¶Sanchit Sinha
â–¶Dane Williamson
Office hours will be released on the course webpage.
1

--------------------------------------------------------------------------------
[End of Page 2]

Prerequisites
â–¶Programming and Algorithm
â–¶CS 2150 or CS 3100 with a grade of C- or better
2

--------------------------------------------------------------------------------
[End of Page 3]

Prerequisites
â–¶Programming and Algorithm
â–¶CS 2150 or CS 3100 with a grade of C- or better
â–¶Linear Algebra
â–¶Math 3350 or APMA 3080 or equivalent
2

--------------------------------------------------------------------------------
[End of Page 4]

Prerequisites
â–¶Programming and Algorithm
â–¶CS 2150 or CS 3100 with a grade of C- or better
â–¶Linear Algebra
â–¶Math 3350 or APMA 3080 or equivalent
â–¶Probability and Statistics
â–¶APMA 3100, APMA 3110, MATH 3100, or equivalent
2

--------------------------------------------------------------------------------
[End of Page 5]

Goal
The survey results (by Jan. 18, 12 PM)
3

--------------------------------------------------------------------------------
[End of Page 6]

Outline
This course will cover the basic materials on the following topics
1. Introduction to learning theory
2. Linear classification and regression
3. Model selection and validation
4. Boosting
5. Optimization methods
6. Neural networks (e.g., CNN, RNN, Auto-encoders, Transformers)
4

--------------------------------------------------------------------------------
[End of Page 7]

Outline (II)
The following topics will not be the emphasis of this course
â–¶Statistical modeling
â–¶e.g., parameter estimation, Bayesian statistics, graphical models
5

--------------------------------------------------------------------------------
[End of Page 8]

Outline (II)
The following topics will not be the emphasis of this course
â–¶Statistical modeling
â–¶e.g., parameter estimation, Bayesian statistics, graphical models
â–¶Machine learning engineering
â–¶e.g., how to implement a classifier from end to end
â–¶although, we will provide some demo code for illustration
purposes
5

--------------------------------------------------------------------------------
[End of Page 9]

Outline (II)
The following topics will not be the emphasis of this course
â–¶Statistical modeling
â–¶e.g., parameter estimation, Bayesian statistics, graphical models
â–¶Machine learning engineering
â–¶e.g., how to implement a classifier from end to end
â–¶although, we will provide some demo code for illustration
purposes
â–¶Advanced topics in machine learning
â–¶e.g., reinforcement learning, active learning, semi-supervised
learning, online learning
5

--------------------------------------------------------------------------------
[End of Page 10]

Textbooks
â–¶Shalev-Shwartz and Ben-David. Understanding Machine Learning:
From Theory to Algorithms. 2014
6

--------------------------------------------------------------------------------
[End of Page 11]

Textbooks
â–¶Shalev-Shwartz and Ben-David. Understanding Machine Learning:
From Theory to Algorithms. 2014
â–¶Goodfellow, Bengio, and Courville. Deep Learning. 2016
6

--------------------------------------------------------------------------------
[End of Page 12]

Reference Books
For students looking for additional reading materials
â–¶Bishop. Pattern Recognition and Machine Learning. 2006
â–¶Murphy. Machine Learning: A Probabilistic Perspective. 2012
â–¶Mohri, Rostamizadeh, and Talwalkar. Foundations of Machine
Learning. 2nd Edition. 2018
â–¶Hastie, Tibshirani, and Friedman. The Elements of Statistical
Learning (2nd Edition). 2009
7

--------------------------------------------------------------------------------
[End of Page 13]

Homework and Grading Policy
â–¶Homeworks (70%)
â–¶Five homework assignments, 14 points each
8

--------------------------------------------------------------------------------
[End of Page 14]

Homework and Grading Policy
â–¶Homeworks (70%)
â–¶Five homework assignments, 14 points each
â–¶In-class Quiz (10%)
â–¶For the instructor to get a better understanding of studentsâ€™
feedback on the lectures
â–¶1 point each
8

--------------------------------------------------------------------------------
[End of Page 15]

Homework and Grading Policy
â–¶Homeworks (70%)
â–¶Five homework assignments, 14 points each
â–¶In-class Quiz (10%)
â–¶For the instructor to get a better understanding of studentsâ€™
feedback on the lectures
â–¶1 point each
â–¶Final project (20%)
â–¶There are some pre-defined problems with datasets provided
â–¶Students will team up (3 â€“ 4 students per group) to solve one
problem
8

--------------------------------------------------------------------------------
[End of Page 16]

Grading Policy
The final grade is threshold-based instead of percentage-based
9

--------------------------------------------------------------------------------
[End of Page 17]

Late Penalty
â–¶Homework submission will be accepted up to 72 hours late, with
20% deduction per 24 hours on the points as a penalty
â–¶Submission will not be accepted if more than 72 hours late
â–¶Make sure not submit wrong files
â–¶it is students responsbility to make sure they submit the right and
complete files for each homework
â–¶It is usually better if students just turn in what they have in time
10

--------------------------------------------------------------------------------
[End of Page 18]

Violation of the Honor Code
Plagiarism, examples are
â–¶in a homework submission, copying answers from others directly
or some minor changes
â–¶in a report, copying texts from a published paper (including,
some minor changes)
â–¶in a code, using someone elseâ€™s functions/implementations
without acknowledging the contribution
11

--------------------------------------------------------------------------------
[End of Page 19]

Webpages
â–¶Course webpage
http://yangfengji.net/uva-ml-undergrad/
which contains all the information you need about this course.
â–¶Canvas
â–¶For homework releasing and grading
â–¶For announcement, online QA, discussion, etc.
12

--------------------------------------------------------------------------------
[End of Page 20]

Why Taking this Course?

--------------------------------------------------------------------------------
[End of Page 21]

Machine Learning Courses
14

--------------------------------------------------------------------------------
[End of Page 22]

An Example Course
Building logistic regression classifier
15

--------------------------------------------------------------------------------
[End of Page 23]

Another Example
Building a GPT model with Transformer
16

--------------------------------------------------------------------------------
[End of Page 24]

What is Missing?
17

--------------------------------------------------------------------------------
[End of Page 25]

What is this?
â–¶Whatâ€™s the definition of this classifier?
â–¶What if it does not work?
â–¶What are its limitations?
18

--------------------------------------------------------------------------------
[End of Page 26]

What is this?
â–¶Whatâ€™s the definition of this classifier?
â–¶What if it does not work?
â–¶What are its limitations?
In fact, if you explain how these parameters work and their effects, you can
skip at least one third of the class lectures.
18

--------------------------------------------------------------------------------
[End of Page 27]

Mathematical/Statistical Background
To understanding machine learning, we need some mathematical and
statistical knowledge
19

--------------------------------------------------------------------------------
[End of Page 28]

Now, letâ€™s have some fun!
Warning: you will see lots of mathematical notations.
20

--------------------------------------------------------------------------------
[End of Page 29]

Basic Linear Algebra

--------------------------------------------------------------------------------
[End of Page 30]

Linear Equations
Consider the following system of equations
4ğ‘¥1 âˆ’5ğ‘¥2 = âˆ’13
âˆ’2ğ‘¥1 + 3ğ‘¥2 = 9
(1)
In matrix notation, it can be written as a more compact from
Ağ’™= ğ’ƒ
(2)
with
A =

4
âˆ’5
âˆ’2
3

ğ’™=

ğ‘¥1
ğ‘¥2

ğ’ƒ=

âˆ’13
9

(3)
22

--------------------------------------------------------------------------------
[End of Page 31]

Basic Notations
A =

4
âˆ’5
âˆ’2
3

ğ’™=

ğ‘¥1
ğ‘¥2

ğ’ƒ=

âˆ’13
9

â–¶A âˆˆâ„ğ‘šÃ—ğ‘›: a matrix with ğ‘šrows and ğ‘›columns
â–¶The element on the ğ‘–-th row and the ğ‘—-th column is denoted as ğ‘ğ‘–,ğ‘—
â–¶ğ’™âˆˆâ„ğ‘›: a vector with ğ‘›entries. By convention, an ğ‘›-dimensional
vector is often thought of as matrix with ğ‘›rows and 1 column,
known as a column vector.
â–¶The ğ‘–-th element is denoted as ğ‘¥ğ‘–
23

--------------------------------------------------------------------------------
[End of Page 32]

Vector Norms
â–¶A norm of a vector âˆ¥ğ’™âˆ¥is informally a measure of the â€œlengthâ€ of
the vector.
â–¶Formally, a norm is any function ğ‘“: â„ğ‘›â†’â„that satisfies four
properties
1. ğ‘“(ğ’™) â‰¥0 for any ğ’™âˆˆâ„ğ‘›
2. ğ‘“(ğ’™) = 0 if and only if ğ’™= 0
3. ğ‘“(ğ‘ğ’™) = |ğ‘| Â· ğ‘“(ğ’™) for any ğ’™âˆˆâ„ğ‘›
4. ğ‘“(ğ’™+ ğ’š) â‰¤ğ‘“(ğ’™) + ğ‘“(ğ’š), for any ğ’™, ğ’šâˆˆâ„ğ‘›
24

--------------------------------------------------------------------------------
[End of Page 33]

â„“2 Norm
The â„“2 norm of a vector ğ’™âˆˆâ„ğ‘›is defined as
âˆ¥ğ’™âˆ¥2 =
v
t ğ‘›
Ã•
ğ‘–=1
ğ‘¥2
ğ‘–
(4)
x
y
ğ’™
âˆ¥ğ’™âˆ¥2
Question for Homework: prove â„“2 norm satisfies all four properties
25

--------------------------------------------------------------------------------
[End of Page 34]

â„“1 Norms
The â„“1 norm of a vector ğ’™âˆˆâ„ğ‘›is defined as
âˆ¥ğ’™âˆ¥1 =
ğ‘›
Ã•
ğ‘–=1
|ğ‘¥ğ‘–|
(5)
26

--------------------------------------------------------------------------------
[End of Page 35]

Plots
For a two-dimensional vector ğ’™= (ğ‘¥1, ğ‘¥2) âˆˆâ„2, which of the
following plot is âˆ¥ğ’™âˆ¥1 = 1?
ğ‘¥1
ğ‘¥2
(a)
ğ‘¥1
ğ‘¥2
(b)
ğ‘¥1
ğ‘¥2
(c)
27

--------------------------------------------------------------------------------
[End of Page 36]

Dot Product
The dot product of ğ’™, ğ’šâˆˆâ„ğ‘›is defined as
âŸ¨ğ’™, ğ’šâŸ©= ğ’™Tğ’š=
ğ‘›
Ã•
ğ‘–=1
ğ‘¥ğ‘–ğ‘¦ğ‘–
(6)
where ğ’™T is the transpose of ğ’™.
â–¶âˆ¥ğ’™âˆ¥2
2 = âŸ¨ğ’™, ğ’™âŸ©
â–¶If ğ’™= (0, 0, . . . ,
1
|{z}
ğ‘¥ğ‘–
, . . . , 0), then âŸ¨ğ’™, ğ’šâŸ©= ğ‘¦ğ‘–
â–¶If ğ’™is an unit vector (âˆ¥ğ’™âˆ¥2 = 1), then âŸ¨ğ’™, ğ’šâŸ©is the projection of ğ’š
on the direction of ğ’™
ğ’™
ğ’š
28

--------------------------------------------------------------------------------
[End of Page 37]

Cauchy-Schwarz Inequality
For all ğ’™, ğ’šâˆˆâ„ğ‘›
|âŸ¨ğ’™, ğ’šâŸ©| â‰¤âˆ¥ğ’™âˆ¥2âˆ¥ğ’šâˆ¥2
(7)
with equality if and only if ğ’™= ğ›¼ğ’šwith ğ›¼âˆˆâ„
Proof:
Let Ëœğ’™=
ğ’™
âˆ¥ğ’™âˆ¥2 and Ëœğ’š=
ğ’š
âˆ¥ğ’šâˆ¥2 , then Ëœğ’™and Ëœğ’šare both unit vectors.
Based on the geometric interpretation on the previous slide, we have
âŸ¨Ëœğ’™, Ëœğ’šâŸ©â‰¤1
(8)
if and only if Ëœğ’™= Ëœğ’š.
29

--------------------------------------------------------------------------------
[End of Page 38]

Matrix-Vector Multiplication
Given a matrix ğ‘¨and a vector ğ’™, their multiplication is equivalent to
performing a linear transformation on ğ’™
ğ‘¨ğ’™
(9)
For example, consider the following matrix
ğ‘¨=

0.5
0
0
2

(10)
and three vectors
â–¶ğ’™T
1 = [1, 2]
â–¶ğ’™T
2 = [2, 4]
â–¶ğ’™T
3 = [3, 6]
30

--------------------------------------------------------------------------------
[End of Page 39]

Matrix-Vector Multiplication
Given a matrix ğ‘¨and a vector ğ’™, their multiplication is equivalent to
performing a linear transformation on ğ’™
ğ‘¨ğ’™
(9)
For example, consider the following matrix
ğ‘¨=

0.5
0
0
2

(10)
and three vectors
â–¶ğ’™T
1 = [1, 2]
â–¶ğ’™T
2 = [2, 4]
â–¶ğ’™T
3 = [3, 6]
This is also what the function torch.nn.Linear means
30

--------------------------------------------------------------------------------
[End of Page 40]

Two Special Matrices
â–¶The identity matrix, denoted as I âˆˆâ„ğ‘›Ã—ğ‘›], is a square matrix with
ones on the diagonal and zeros everywhere else.
I =
ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
1
...
1
ï£¹ï£ºï£ºï£ºï£ºï£ºï£»
(11)
31

--------------------------------------------------------------------------------
[End of Page 41]

Two Special Matrices
â–¶The identity matrix, denoted as I âˆˆâ„ğ‘›Ã—ğ‘›], is a square matrix with
ones on the diagonal and zeros everywhere else.
I =
ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
1
...
1
ï£¹ï£ºï£ºï£ºï£ºï£ºï£»
(11)
â–¶A diagonal matrix, denoted as D = diag(ğ‘‘1, ğ‘‘2, . . . , ğ‘‘ğ‘›), is a
matrix where all non-diagonal elements are 0.
D =
ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
ğ‘‘1
...
ğ‘‘ğ‘›
ï£¹ï£ºï£ºï£ºï£ºï£ºï£»
(12)
31

--------------------------------------------------------------------------------
[End of Page 42]

Inverse
The inverse of a square matrix A âˆˆâ„ğ‘›Ã—ğ‘›is denoted as Aâˆ’1, which is
the unique matrix such that
ğ‘¨âˆ’1ğ‘¨= ğ‘°= ğ‘¨ğ‘¨âˆ’1
(13)
â–¶Non-square matrices do not have inverses (by definition)
â–¶Not all square matrices are invertible
â–¶The solution of the linear equations in Eq. (1) is ğ’™= Aâˆ’1ğ’ƒ
32

--------------------------------------------------------------------------------
[End of Page 43]

Inverse (II)
â–¶In matrix-vector multiplication, an inverse matrix ğ‘¨âˆ’1 will
reverse the linear transformation performed by ğ‘¨
ğ‘¨âˆ’1ğ‘¨ğ’™= ğ’™
(14)
33

--------------------------------------------------------------------------------
[End of Page 44]

Inverse (II)
â–¶In matrix-vector multiplication, an inverse matrix ğ‘¨âˆ’1 will
reverse the linear transformation performed by ğ‘¨
ğ‘¨âˆ’1ğ‘¨ğ’™= ğ’™
(14)
â–¶When a matrix ğ‘¨is not invertible, it means its linear
transformation is not reversible
ğ‘¨=

1
0
0
0

(15)
33

--------------------------------------------------------------------------------
[End of Page 45]

Orthogonal Matrices
â–¶Two vectors ğ’™, ğ’šâˆˆâ„ğ‘›are orthogonal if âŸ¨ğ’™, ğ’šâŸ©= 0
ğ’™
ğ’š
34

--------------------------------------------------------------------------------
[End of Page 46]

Orthogonal Matrices
â–¶Two vectors ğ’™, ğ’šâˆˆâ„ğ‘›are orthogonal if âŸ¨ğ’™, ğ’šâŸ©= 0
ğ’™
ğ’š
â–¶A square matrix U âˆˆâ„ğ‘›Ã—ğ‘›is orthogonal, if all its columns are
orthogonal to each other and normalized (orthonormal)
âŸ¨ğ’–ğ‘–, ğ’–ğ‘—âŸ©= 0, âˆ¥ğ’–ğ‘–âˆ¥= 1, âˆ¥ğ’–ğ‘—âˆ¥= 1
(16)
for ğ‘–, ğ‘—âˆˆ[ğ‘›] and ğ‘–â‰ ğ‘—
34

--------------------------------------------------------------------------------
[End of Page 47]

Orthogonal Matrices
â–¶Two vectors ğ’™, ğ’šâˆˆâ„ğ‘›are orthogonal if âŸ¨ğ’™, ğ’šâŸ©= 0
ğ’™
ğ’š
â–¶A square matrix U âˆˆâ„ğ‘›Ã—ğ‘›is orthogonal, if all its columns are
orthogonal to each other and normalized (orthonormal)
âŸ¨ğ’–ğ‘–, ğ’–ğ‘—âŸ©= 0, âˆ¥ğ’–ğ‘–âˆ¥= 1, âˆ¥ğ’–ğ‘—âˆ¥= 1
(16)
for ğ‘–, ğ‘—âˆˆ[ğ‘›] and ğ‘–â‰ ğ‘—
â–¶Furthermore, UTU = I = UUT, which further implies Uâˆ’1 = UT
34

--------------------------------------------------------------------------------
[End of Page 48]

A Special Case
Consider a matrix ğ‘¨as
ğ‘¨=

0
1
1
0

(17)
For any ğ’™T = [ğ‘¥1, ğ‘¥2], we have
ğ‘¨

ğ‘¥1
ğ‘¥2

=

ğ‘¥2
ğ‘¥1

(18)
35

--------------------------------------------------------------------------------
[End of Page 49]

A Special Case
Consider a matrix ğ‘¨as
ğ‘¨=

0
1
1
0

(17)
For any ğ’™T = [ğ‘¥1, ğ‘¥2], we have
ğ‘¨

ğ‘¥1
ğ‘¥2

=

ğ‘¥2
ğ‘¥1

(18)
The is a reflection operation. Operations like this are popularly used
in computer graphics.
35

--------------------------------------------------------------------------------
[End of Page 50]

Symmetric Matrices
A symmetric matrix A âˆˆâ„ğ‘›Ã—ğ‘›is defined as
AT = A
(19)
or, in other words,
ğ‘ğ‘–,ğ‘—= ğ‘ğ‘—,ğ‘–
âˆ€ğ‘–, ğ‘—âˆˆ[ğ‘›]
(20)
Comments
â–¶The identity matrix I is symmetric
â–¶A diagonal matrix is symmetric
ğ’™Tğ‘¨ğ’š
(21)
gives each dimension a different weight (importance) when
computing the similarity between ğ’™and ğ’š
36

--------------------------------------------------------------------------------
[End of Page 51]

Eigen Decomposition
Every symmetric matrix A can be decomposed as
A = UÎ›UT
(22)
with
â–¶Î› =
ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
ğœ†1
...
ğœ†ğ‘›
ï£¹ï£ºï£ºï£ºï£ºï£ºï£»
as a diagonal matrix
â–¶Q is an orthogonal matrix
37

--------------------------------------------------------------------------------
[End of Page 52]

Eigen Decomposition
Every symmetric matrix A can be decomposed as
A = UÎ›UT
(22)
with
â–¶Î› =
ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
ğœ†1
...
ğœ†ğ‘›
ï£¹ï£ºï£ºï£ºï£ºï£ºï£»
as a diagonal matrix
â–¶Q is an orthogonal matrix
â–¶Consider the similarity measurement
ğ’™TAğ’š= ğ’™TUÎ›UTğ’š= (UTğ’™)TÎ›UTğ’š
(23)
37

--------------------------------------------------------------------------------
[End of Page 53]

Eigen Decomposition
Every symmetric matrix A can be decomposed as
A = UÎ›UT
(22)
with
â–¶Î› =
ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
ğœ†1
...
ğœ†ğ‘›
ï£¹ï£ºï£ºï£ºï£ºï£ºï£»
as a diagonal matrix
â–¶Q is an orthogonal matrix
â–¶Consider the similarity measurement
ğ’™TAğ’š= ğ’™TUÎ›UTğ’š= (UTğ’™)TÎ›UTğ’š
(23)
Question for Homework: if a symmetric matrix A is invertible, show
Aâˆ’1 = UÎ›âˆ’1UT with Î›âˆ’1 = diag( 1
ğœ†1 , . . . , 1
ğœ†ğ‘›)
37

--------------------------------------------------------------------------------
[End of Page 54]

Symmetric Positive Semidefinite Matrices
A symmetric matrix P âˆˆâ„ğ‘›Ã—ğ‘›is positive semidefinite if and only if
ğ’™TPğ’™â‰¥0
(24)
for all ğ’™âˆˆâ„ğ‘›.
38

--------------------------------------------------------------------------------
[End of Page 55]

Symmetric Positive Semidefinite Matrices
A symmetric matrix P âˆˆâ„ğ‘›Ã—ğ‘›is positive semidefinite if and only if
ğ’™TPğ’™â‰¥0
(24)
for all ğ’™âˆˆâ„ğ‘›.
Eigen decomposition of P as
P = UÎ›UT
(25)
with Î› = diag(ğœ†1, . . . , ğœ†ğ‘›) and
ğœ†ğ‘–â‰¥0
(26)
38

--------------------------------------------------------------------------------
[End of Page 56]

Symmetric Positive Definite Matrices
A symmetric matrix P âˆˆâ„ğ‘›Ã—ğ‘›is positive definite if and only if
ğ’™TPğ’™> 0
(27)
for all ğ’™âˆˆâ„ğ‘›.
â–¶Eigen values of P, Î› = diag(ğœ†1, . . . , ğœ†ğ‘›) with
ğœ†ğ‘–> 0
(28)
39

--------------------------------------------------------------------------------
[End of Page 57]

Symmetric Positive Definite Matrices
A symmetric matrix P âˆˆâ„ğ‘›Ã—ğ‘›is positive definite if and only if
ğ’™TPğ’™> 0
(27)
for all ğ’™âˆˆâ„ğ‘›.
â–¶Eigen values of P, Î› = diag(ğœ†1, . . . , ğœ†ğ‘›) with
ğœ†ğ‘–> 0
(28)
Question for Homework: if one of the eigen values ğœ†ğ‘–< 0, show that
you can also find a vector ğ’™such that ğ’™TPğ’™< 0
39

--------------------------------------------------------------------------------
[End of Page 58]

Review
The identity matrix I is
â–¶a diagonal matrix?
â–¶a symmetric matrix?
â–¶an orthogonal matrix?
â–¶a positive (semi-)definite matrix?
40

--------------------------------------------------------------------------------
[End of Page 59]

Review
The identity matrix I is
â–¶a diagonal matrix? âœ“
â–¶a symmetric matrix? âœ“
â–¶an orthogonal matrix? âœ“
â–¶a positive (semi-)definite matrix? âœ“
40

--------------------------------------------------------------------------------
[End of Page 60]

Review of Probability Theory

--------------------------------------------------------------------------------
[End of Page 61]

What is Probability?
The probability of landing heads is 0.52
42

--------------------------------------------------------------------------------
[End of Page 62]

Two interpretations
Frequentist Probability represents the long-run frequency of an event
â–¶If we flip the coin many times, we expect it to land
heads about 52% times
43

--------------------------------------------------------------------------------
[End of Page 63]

Two interpretations
Frequentist Probability represents the long-run frequency of an event
â–¶If we flip the coin many times, we expect it to land
heads about 52% times
Bayesian Probability quantifies our (un)certainty about an event
â–¶We believe the coin is 52% of chance to land head
on the next toss
43

--------------------------------------------------------------------------------
[End of Page 64]

Bayesian Interpretation
Example scenarios of Bayesian interpretation of probability:
44

--------------------------------------------------------------------------------
[End of Page 65]

Binary Random Variables
â–¶Event ğ‘‹. Such as
â–¶the coin will lead head on the next toss
â–¶it will rain tomorrow
â–¶Sample space of ğ‘‹âˆˆ{false, true} or for simplicity {0, 1}
45

--------------------------------------------------------------------------------
[End of Page 66]

Binary Random Variables
â–¶Event ğ‘‹. Such as
â–¶the coin will lead head on the next toss
â–¶it will rain tomorrow
â–¶Sample space of ğ‘‹âˆˆ{false, true} or for simplicity {0, 1}
â–¶Probability ğ‘ƒ(ğ‘‹= ğ‘¥) or ğ‘ƒ(ğ‘¥)
â–¶Let ğ‘‹be the event that the coin will lead head on the next toss, then
the probability from the previous example is
ğ‘ƒ(ğ‘‹= 1) = 0.52
(29)
45

--------------------------------------------------------------------------------
[End of Page 67]

Bernoulli Distribution
Given the binary random variable ğ‘‹and
its sample space as {0, 1}
ğ‘ƒ(ğ‘‹= ğ‘¥) = ğœƒğ‘¥(1 âˆ’ğœƒ)1âˆ’ğ‘¥
with a single parameter ğœƒas
ğœƒ= ğ‘ƒ(ğ‘‹= 1)
Jacob Bernoulli
46

--------------------------------------------------------------------------------
[End of Page 68]

Tossing a Coin Twice?
â–¶Let ğ‘‹be the number of heads
â–¶Sample space of ğ‘‹âˆˆ{0, 1, 2}
47

--------------------------------------------------------------------------------
[End of Page 69]

Tossing a Coin Twice?
â–¶Let ğ‘‹be the number of heads
â–¶Sample space of ğ‘‹âˆˆ{0, 1, 2}
â–¶Assume we use the same coin, the probability distribution of ğ‘‹
â–¶ğ‘ƒ(ğ‘‹= 0) = (1 âˆ’ğœƒ)2
47

--------------------------------------------------------------------------------
[End of Page 70]

Tossing a Coin Twice?
â–¶Let ğ‘‹be the number of heads
â–¶Sample space of ğ‘‹âˆˆ{0, 1, 2}
â–¶Assume we use the same coin, the probability distribution of ğ‘‹
â–¶ğ‘ƒ(ğ‘‹= 0) = (1 âˆ’ğœƒ)2
â–¶ğ‘ƒ(ğ‘‹= 2) = ğœƒ2
47

--------------------------------------------------------------------------------
[End of Page 71]

Tossing a Coin Twice?
â–¶Let ğ‘‹be the number of heads
â–¶Sample space of ğ‘‹âˆˆ{0, 1, 2}
â–¶Assume we use the same coin, the probability distribution of ğ‘‹
â–¶ğ‘ƒ(ğ‘‹= 0) = (1 âˆ’ğœƒ)2
â–¶ğ‘ƒ(ğ‘‹= 2) = ğœƒ2
â–¶ğ‘ƒ(ğ‘‹= 1) = ğœƒ(1 âˆ’ğœƒ) + (1 âˆ’ğœƒ)ğœƒ= 2ğœƒ(1 âˆ’ğœƒ)
47

--------------------------------------------------------------------------------
[End of Page 72]

General Case: Binomial Distribution
Consider a general case, in which we toss the coin ğ‘›times, then the
random variable ğ‘Œcan be formulated as a binomial distribution
ğ‘ƒ(ğ‘Œ= ğ‘˜) =
ğ‘›
ğ‘˜

ğœƒğ‘˜(1 âˆ’ğœƒ)ğ‘›âˆ’ğ‘˜
(30)
where
ğ‘›
ğ‘˜

=
ğ‘›!
ğ‘˜!(ğ‘›âˆ’ğ‘˜)!
is the binomial coefficient and
ğ‘›! = ğ‘›Â· (ğ‘›âˆ’1) Â· (ğ‘›âˆ’2) Â· Â· Â· 1
48

--------------------------------------------------------------------------------
[End of Page 73]

Tossing a Dice
How to define the corresponding random variable?
â–¶ğ‘‹âˆˆ{1, 2, 3, 4, 5, 6}
â–¶ğ‘¿âˆˆ{100000, 010000, 001000, 000100, 000010, 000001}
49

--------------------------------------------------------------------------------
[End of Page 74]

Categorical Distribution
ğ‘ƒ(ğ‘¿= ğ’™) =
6
Ã–
ğ‘˜=1
(ğœƒğ‘˜)ğ‘¥ğ‘˜
(31)
where
â–¶ğ’™= (ğ‘¥1, ğ‘¥2, . . . , ğ‘¥6)
â–¶ğ‘¥ğ‘˜âˆˆ{0, 1}, and
â–¶{ğœƒğ‘˜}6
ğ‘˜=1 are the parameters of this distribution, which is also the
probability of side ğ‘˜showing up.
50

--------------------------------------------------------------------------------
[End of Page 75]

Multinomial Distribution
Repeat the previous event ğ‘›times, the corresponding probability
distribution is modeled as
ğ‘ƒ(ğ‘¿= ğ’™) =

ğ‘›
ğ‘¥1 Â· Â· Â· ğ‘¥ğ¾

ğ¾
Ã–
ğ‘˜=1
ğœƒğ‘¥ğ‘˜
ğ‘˜
(32)
where ğ’™= (ğ‘¥1, . . . , ğ‘¥ğ¾) and each ğ‘¥ğ‘˜âˆˆ{0, 1, 2, . . . , ğ‘›} indicates the
number of times that side ğ‘˜showing up.

ğ‘›
ğ‘¥1 Â· Â· Â· ğ‘¥ğ¾

=
ğ‘›!
ğ‘¥1! Â· Â· Â· ğ‘¥ğ¾!
The sum of {ğ‘¥ğ‘˜} follows the constraint:
ğ¾
Ã•
ğ‘˜=1
ğ‘¥ğ‘˜= ğ‘›
51

--------------------------------------------------------------------------------
[End of Page 76]

Gaussian Distribution
A random variable ğ‘‹âˆˆâ„is said to follow a normal (or Gaussian)
distribution N(ğœ‡, ğœ2) if its probability density function is given by
ğ‘“(ğ‘¥) =
1
âˆš
2ğœ‹ğœ2 exp

âˆ’(ğ‘¥âˆ’ğœ‡)2
2ğœ2

(33)
â–¶ğœ‡: mean
â–¶ğœ2: variance
â–¶Probability of ğ‘‹âˆˆ[ğ‘, ğ‘]: ğ‘ƒ(ğ‘â‰¤ğ‘‹â‰¤ğ‘) =
âˆ«ğ‘
ğ‘ğ‘“(ğ‘¥)ğ‘‘ğ‘¥
âˆ’4
âˆ’2
0
2
4
0
0.1
0.2
0.3
0.4
52

--------------------------------------------------------------------------------
[End of Page 77]

Gaussian Distribution (II)
ğ‘“(ğ‘¥) =
1
âˆš
2ğœ‹ğœ2 exp

âˆ’(ğ‘¥âˆ’ğœ‡)2
2ğœ2

(34)
There examples of Gaussian distributions
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
8
0
0.1
0.2
0.3
0.4
â–¶Blue: N(0, 1) (standard normal distribution)
â–¶Red: N(0, 2)
â–¶Green: N(1, 1)
53

--------------------------------------------------------------------------------
[End of Page 78]

Gaussian Distribution (II)
ğ‘“(ğ‘¥) =
1
âˆš
2ğœ‹ğœ2 exp

âˆ’(ğ‘¥âˆ’ğœ‡)2
2ğœ2

(34)
There examples of Gaussian distributions
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
8
0
0.1
0.2
0.3
0.4
â–¶Blue: N(0, 1) (standard normal distribution)
â–¶Red: N(0, 2)
â–¶Green: N(1, 1)
Question for Homework: describe a random event with the
probabilistic language
53

--------------------------------------------------------------------------------
[End of Page 79]

Probability of Two Random Variables
Modeling two random variables together with a joint distribution
ğ‘ƒ(ğ‘‹, ğ‘Œ)
(35)
Related concepts
â–¶Independence
â–¶Conditional probability and chain rule
â–¶Bayes rule
54

--------------------------------------------------------------------------------
[End of Page 80]

Independence
Definition Two random variable ğ‘‹and ğ‘Œare independent with each
other, if we can represent the joint probability as the product of their
marginal distributions for any values of ğ‘‹and ğ‘Œ, or mathematically,
ğ‘ƒ(ğ‘‹, ğ‘Œ) = ğ‘ƒ(ğ‘‹) Â· ğ‘ƒ(ğ‘Œ)
(36)
Marginal distributions
ğ‘ƒ(ğ‘‹)
=
Ã•
ğ‘Œ
ğ‘ƒ(ğ‘‹, ğ‘Œ)
(37)
ğ‘ƒ(ğ‘Œ)
=
Ã•
ğ‘‹
ğ‘ƒ(ğ‘‹, ğ‘Œ)
(38)
55

--------------------------------------------------------------------------------
[End of Page 81]

Independence
Definition Two random variable ğ‘‹and ğ‘Œare independent with each
other, if we can represent the joint probability as the product of their
marginal distributions for any values of ğ‘‹and ğ‘Œ, or mathematically,
ğ‘ƒ(ğ‘‹, ğ‘Œ) = ğ‘ƒ(ğ‘‹) Â· ğ‘ƒ(ğ‘Œ)
(36)
Marginal distributions
ğ‘ƒ(ğ‘‹)
=
Ã•
ğ‘Œ
ğ‘ƒ(ğ‘‹, ğ‘Œ)
(37)
ğ‘ƒ(ğ‘Œ)
=
Ã•
ğ‘‹
ğ‘ƒ(ğ‘‹, ğ‘Œ)
(38)
â–¶ğ‘‹: whether it is cloudy
â–¶ğ‘Œ: whether it will rain
ğ‘ƒ(ğ‘‹âˆ©ğ‘Œ)
ğ‘‹= 0
ğ‘‹= 1
ğ‘Œ= 0
0.35
0.15
ğ‘Œ= 1
0.05
0.45
55

--------------------------------------------------------------------------------
[End of Page 82]

Conditional Probability
Conditional probability of ğ‘Œgiven ğ‘‹
ğ‘ƒ(ğ‘Œ| ğ‘‹) = ğ‘ƒ(ğ‘‹, ğ‘Œ)
ğ‘ƒ(ğ‘‹)
(39)
Example: document classification
â–¶ğ‘‹: a document
â–¶ğ‘Œ: the label of this document
A special case: if ğ‘‹and ğ‘Œare independent
ğ‘ƒ(ğ‘Œ| ğ‘‹) = ğ‘ƒ(ğ‘Œ)
(40)
Intuitively, it means Knowing ğ‘‹does not provide any new information
about ğ‘Œ
56

--------------------------------------------------------------------------------
[End of Page 83]

Conditional Probability
â–¶ğ‘‹: whether it is cloudy
â–¶ğ‘Œ: whether it will rain
ğ‘ƒ(ğ‘‹, ğ‘Œ)
ğ‘‹= 0
ğ‘‹= 1
ğ‘Œ= 0
0.35
0.15
ğ‘Œ= 1
0.05
0.45
â–¶ğ‘ƒ(ğ‘Œ| ğ‘‹= 1):
â–¶ğ‘ƒ(ğ‘Œ= 0 | ğ‘‹= 1) = 0.25,
â–¶ğ‘ƒ(ğ‘Œ= 1 | ğ‘‹= 1) = 0.75
57

--------------------------------------------------------------------------------
[End of Page 84]

Conditional Probability
â–¶ğ‘‹: whether it is cloudy
â–¶ğ‘Œ: whether it will rain
ğ‘ƒ(ğ‘‹, ğ‘Œ)
ğ‘‹= 0
ğ‘‹= 1
ğ‘Œ= 0
0.35
0.15
ğ‘Œ= 1
0.05
0.45
â–¶ğ‘ƒ(ğ‘Œ| ğ‘‹= 1):
â–¶ğ‘ƒ(ğ‘Œ= 0 | ğ‘‹= 1) = 0.25,
â–¶ğ‘ƒ(ğ‘Œ= 1 | ğ‘‹= 1) = 0.75
â–¶ğ‘ƒ(ğ‘Œ): ğ‘ƒ(ğ‘Œ= 0) = ğ‘ƒ(ğ‘Œ= 1) = 0.5
57

--------------------------------------------------------------------------------
[End of Page 85]

Conditional Probability
â–¶ğ‘‹: whether it is cloudy
â–¶ğ‘Œ: whether it will rain
ğ‘ƒ(ğ‘‹, ğ‘Œ)
ğ‘‹= 0
ğ‘‹= 1
ğ‘Œ= 0
0.35
0.15
ğ‘Œ= 1
0.05
0.45
â–¶ğ‘ƒ(ğ‘Œ| ğ‘‹= 1):
â–¶ğ‘ƒ(ğ‘Œ= 0 | ğ‘‹= 1) = 0.25,
â–¶ğ‘ƒ(ğ‘Œ= 1 | ğ‘‹= 1) = 0.75
â–¶ğ‘ƒ(ğ‘Œ): ğ‘ƒ(ğ‘Œ= 0) = ğ‘ƒ(ğ‘Œ= 1) = 0.5
Question for Homework: compute conditional probability from a
given probabilistic table
57

--------------------------------------------------------------------------------
[End of Page 86]

Multivariate Gaussian
The probability density function of a multivariate Gaussian
distribution N(ğ, Î£) is defined as
ğ‘“(ğ’™) =
1
(2ğœ‹)ğ‘›/2
1
|Î£|1/2 exp

âˆ’1
2(ğ’™âˆ’ğ)TÎ£âˆ’1(ğ’™âˆ’ğ)

(41)
where
â–¶ğis the ğ‘›-dimensional mean vector and
â–¶Î£ is the ğ‘›Ã— ğ‘›covariance matrix.
58

--------------------------------------------------------------------------------
[End of Page 87]

Covariance Matrix Î£
Assume ğ= 0, the probability density function is
ğ‘“(ğ’™) âˆexp

âˆ’1
2 ğ’™TÎ£âˆ’1ğ’™

(42)
In general, Î£ is required to be a symmetric positive definite matrix
Î£ = I
ğ‘¥1
ğ‘¥2
Î£ = diag(2, 1)
ğ‘¥1
ğ‘¥2
59

--------------------------------------------------------------------------------
[End of Page 88]

Sampling from Gaussians
(a)
(b)
(a) : Î£ = I
(b) : Î£ = diag(2, 1)
Question for Homework: sample from a Gaussian distribution with a
pre-defined mean and variance
60

--------------------------------------------------------------------------------
[End of Page 89]

Thank You!
61

--------------------------------------------------------------------------------
[End of Page 90]