CS 4774 Machine Learning
CNNs and RNNs
Yangfeng Ji
Information and Language Processing Lab
Department of Computer Science
University of Virginia

--------------------------------------------------------------------------------
[End of Page 1]

Overview
1. Convolutional Neural Networks
2. Recurrent Neural Networks
3. RNN Language Modeling
4. Challenge of Training RNNs
1

--------------------------------------------------------------------------------
[End of Page 2]

Convolutional Neural Networks

--------------------------------------------------------------------------------
[End of Page 3]

LeNet-5
A classical neural network architecture designed for handwritten and
machine-printed character recognition.
This architecture repeat the two components twice before connecting
with a fully-connected layer
â–¶convolutional layer
â–¶subsampling (pooling) layer
[LeCun et al., 1998]
3

--------------------------------------------------------------------------------
[End of Page 4]

Convolutional Operations
1-D convolutional operation is defined as
ğ‘ğ‘—= ğ’Tğ’™ğ‘—:ğ‘—+ğ‘›âˆ’1
(1)
where ğ’âˆˆâ„ğ‘›is convolutional filter with window size ğ‘›, ğ’™âˆˆâ„ğ‘‡
input signal with size ğ‘‡, and ğ‘—â‰¤ğ‘‡âˆ’ğ‘›+ 1.
4

--------------------------------------------------------------------------------
[End of Page 5]

Convolutional Operations
1-D convolutional operation is defined as
ğ‘ğ‘—= ğ’Tğ’™ğ‘—:ğ‘—+ğ‘›âˆ’1
(1)
where ğ’âˆˆâ„ğ‘›is convolutional filter with window size ğ‘›, ğ’™âˆˆâ„ğ‘‡
input signal with size ğ‘‡, and ğ‘—â‰¤ğ‘‡âˆ’ğ‘›+ 1.
Example
With
â–¶ğ’= [ğ‘š1, ğ‘š2, ğ‘š3]T, and
â–¶ğ’™= [ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¥4, . . . , ğ‘¥ğ‘‡]T,
when ğ‘—= 2
ğ‘2 = ğ‘š1ğ‘¥2 + ğ‘š2ğ‘¥3 + ğ‘š3ğ‘¥4
(2)
4

--------------------------------------------------------------------------------
[End of Page 6]

Convolutional Operations (II)
An example of 1-D convolutional operations
5

--------------------------------------------------------------------------------
[End of Page 7]

Pooling
There are three popularly used pooling techniques
â–¶Max pooling
6

--------------------------------------------------------------------------------
[End of Page 8]

Pooling
There are three popularly used pooling techniques
â–¶Max pooling
â–¶Average pooling [LeCun et al., 1998]
â–¶Min pooling
6

--------------------------------------------------------------------------------
[End of Page 9]

TextCNN
A simple and effective convolutional neural network architecture for
text classification [Kim, 2014]
Word 
Embedding
Convolutional
Filter 1
Convolutional
Filter 2
Convolutional
Filter 3
Tanh
Tanh
Tanh
Max
Pooling
Max
Pooling
Max
Pooling
Softmax
Function
â–¶torch.nn.Conv1d: convolutional operation on each dimension of the word
embeddings (no cross-dimension convolution)
â–¶torch.tanh
â–¶torch.max: max pooling on each dimension of the word embeddings
â–¶torch.cat: concatenate three vectors from max pooling to form one single vector
â–¶In actual implementation, the input is a 3-D tensor instead of a 2-D matrix
7

--------------------------------------------------------------------------------
[End of Page 10]

Advantages of CNNs
Comparing to Feed-forward NNs: Parameter sharing, sparse
connections
Figure: (1) upper plot: convolutional layer; (2) lower plot: fully-connected
layer.
[Goodfellow et al., 2016]
8

--------------------------------------------------------------------------------
[End of Page 11]

Recurrent Neural Networks

--------------------------------------------------------------------------------
[End of Page 12]

Recurrent Neural Networks (RNNs)
A simple RNN is defined by the following recursive function
ğ’‰ğ‘¡= ğ’‡(ğ’™ğ‘¡, ğ’‰ğ‘¡âˆ’1)
(3)
and depicted as
Â· Â· Â·
ğ’‰ğ‘¡âˆ’1
ğ’‰ğ‘¡
Â· Â· Â·
ğ’™ğ‘¡âˆ’1
ğ’™ğ‘¡
where
â–¶ğ’‰ğ‘¡âˆ’1: hidden state at time step ğ‘¡âˆ’1
â–¶ğ’™ğ‘¡: input at time step ğ‘¡
â–¶ğ’‰ğ‘¡: hidden state at time step ğ‘¡
10

--------------------------------------------------------------------------------
[End of Page 13]

A Simple Transition Function
In the simplest case, the transition function ğ’‡is defined with an
element-wise Sigmoid function and a linear transformation of ğ’™ğ‘¡and
ğ’‰ğ‘¡âˆ’1
ğ’‰ğ‘¡= ğ’‡(ğ’™ğ‘¡, ğ’‰ğ‘¡âˆ’1) = ğˆ(Wâ„ğ’‰ğ‘¡âˆ’1 + Wğ‘–ğ’™ğ‘¡+ ğ’ƒ)
(4)
where
â–¶ğ’™ğ‘¡: input word embedding
â–¶ğ’‰ğ‘¡âˆ’1: hidden statement from previous time step
â–¶Wâ„: parameter matrix for hidden states
â–¶Wğ‘–: parameter matrix for inputs
â–¶ğ’ƒ: bias term (also a parameter)
11

--------------------------------------------------------------------------------
[End of Page 14]

Sigmoid Function
A Sigmoid function with one-dimensional input ğ‘¥âˆˆ(âˆ’âˆ, âˆ)
ğœ(ğ‘¥) =
1
1 âˆ’ğ‘’âˆ’ğ‘¥
The potential numeric issue caused by the Sigmoid function
â–¶ğœ(ğ‘¥) â†’1 with ğ‘¥â‰«6
â–¶ğœ(ğ‘¥) â†’0, ğ‘¥â‰ªâˆ’6
The output of the Sigmoid function will approximate a constant,
when the input value is beyond certain ranges
12

--------------------------------------------------------------------------------
[End of Page 15]

Unfolding RNNs
We can unfold this recursive definition of a RNN
ğ’‰ğ‘¡= ğ’‡(ğ’™ğ‘¡, ğ’‰ğ‘¡âˆ’1)
(5)
13

--------------------------------------------------------------------------------
[End of Page 16]

Unfolding RNNs
We can unfold this recursive definition of a RNN
ğ’‰ğ‘¡= ğ’‡(ğ’™ğ‘¡, ğ’‰ğ‘¡âˆ’1)
(5)
as
ğ’‰0
ğ’‰1
Â· Â· Â·
ğ’‰ğ‘¡âˆ’1
ğ’‰ğ‘¡
Â· Â· Â·
ğ’™1
ğ’™ğ‘¡âˆ’1
ğ’™ğ‘¡
ğ’‰ğ‘¡
=
ğ’‡(ğ’™ğ‘¡, ğ’‡(ğ’™ğ‘¡âˆ’1, ğ’‰ğ‘¡âˆ’2))
=
ğ’‡(ğ’™ğ‘¡, ğ’‡(ğ’™ğ‘¡âˆ’1, ğ’‡(ğ’™ğ‘¡âˆ’2, ğ’‰ğ‘¡âˆ’3)))
=
Â· Â· Â·
=
ğ’‡(ğ’™ğ‘¡, ğ’‡(ğ’™ğ‘¡âˆ’1, ğ’‡(ğ’™ğ‘¡âˆ’2, Â· Â· Â· ğ’‡(ğ’™1, ğ’‰0) Â· Â· Â· )))
(6)
13

--------------------------------------------------------------------------------
[End of Page 17]

Base Condition
Base condition defines the starting point of the recursive computation
ğ’‰0
ğ’‰1
Â· Â· Â·
ğ’™1
ğ’‰ğ‘¡= ğ’‡(ğ’™ğ‘¡, ğ’‡(ğ’™ğ‘¡âˆ’1, ğ’‡(ğ’™ğ‘¡âˆ’2, Â· Â· Â· ğ’‡(ğ’™1, ğ’‰0) Â· Â· Â· )))
(7)
â–¶ğ’‰0: zero vector or parameter
â–¶ğ’™1: input at time ğ‘¡= 1
14

--------------------------------------------------------------------------------
[End of Page 18]

RNN for Sequential Prediction
In general, RNNs can be used for any sequential modeling tasks
ğ‘¦1
ğ‘¦2
ğ‘¦3
ğ‘¦4
ğ’‰0
ğ’‰1
ğ’‰2
ğ’‰3
ğ’‰4
ğ’™1
ğ’™2
ğ’™3
ğ’™4
15

--------------------------------------------------------------------------------
[End of Page 19]

Sequential Modeling as Classification
ğ‘¦1
ğ‘¦2
ğ‘¦3
ğ‘¦4
ğ’‰0
ğ’‰1
ğ’‰2
ğ’‰3
ğ’‰4
ğ’™1
ğ’™2
ğ’™3
ğ’™4
â–¶Prediction at each time step ğ‘¡
Ë†ğ‘¦ğ‘¡= argmax
ğ‘¦
ğ‘ƒ(ğ‘¦; ğ’‰ğ‘¡)
(8)
16

--------------------------------------------------------------------------------
[End of Page 20]

Sequential Modeling as Classification
ğ‘¦1
ğ‘¦2
ğ‘¦3
ğ‘¦4
ğ’‰0
ğ’‰1
ğ’‰2
ğ’‰3
ğ’‰4
ğ’™1
ğ’™2
ğ’™3
ğ’™4
â–¶Prediction at each time step ğ‘¡
Ë†ğ‘¦ğ‘¡= argmax
ğ‘¦
ğ‘ƒ(ğ‘¦; ğ’‰ğ‘¡)
(8)
â–¶Loss at single time step ğ‘¡
ğ¿ğ‘¡(ğ‘¦ğ‘¡, Ë†ğ‘¦ğ‘¡) = âˆ’log ğ‘ƒ(ğ‘¦ğ‘¡; ğ’‰ğ‘¡)
(9)
16

--------------------------------------------------------------------------------
[End of Page 21]

Sequential Modeling as Classification
ğ‘¦1
ğ‘¦2
ğ‘¦3
ğ‘¦4
ğ’‰0
ğ’‰1
ğ’‰2
ğ’‰3
ğ’‰4
ğ’™1
ğ’™2
ğ’™3
ğ’™4
â–¶Prediction at each time step ğ‘¡
Ë†ğ‘¦ğ‘¡= argmax
ğ‘¦
ğ‘ƒ(ğ‘¦; ğ’‰ğ‘¡)
(8)
â–¶Loss at single time step ğ‘¡
ğ¿ğ‘¡(ğ‘¦ğ‘¡, Ë†ğ‘¦ğ‘¡) = âˆ’log ğ‘ƒ(ğ‘¦ğ‘¡; ğ’‰ğ‘¡)
(9)
â–¶The total loss
â„“=
ğ‘‡
Ã•
ğ‘¡=1
ğ¿ğ‘¡(ğ‘¦ğ‘¡, Ë†ğ‘¦ğ‘¡)
(10)
16

--------------------------------------------------------------------------------
[End of Page 22]

RNN Language Modeling

--------------------------------------------------------------------------------
[End of Page 23]

Language Models
A language model defines the probability of ğ‘¥ğ‘¡given
ğ’™= (ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘¡âˆ’1) as
ğ‘ƒ(ğ‘¥ğ‘¡| ğ‘¥1, . . . , ğ‘¥ğ‘¡âˆ’1)
(11)
and the joint probability as
ğ‘ƒ(ğ’™1:ğ‘‡)
=
ğ‘ƒ(ğ‘¥1) Â· ğ‘ƒ(ğ‘¥2 | ğ‘¥1)
Â· Â· Â· Â· Â·
Â·ğ‘ƒ(ğ‘¥ğ‘‡| ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘‡âˆ’1)
18

--------------------------------------------------------------------------------
[End of Page 24]

Language Modeling with RNNs
Using RNNs for language modeling
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
â– 
ğ’‰â–¡
ğ’‰1
ğ’‰2
ğ’‰3
ğ’‰4
â–¡
ğ’™1
ğ’™2
ğ’™3
ğ’™4
with two special tokens
{â–¡, ğ‘¥1, . . . , ğ‘¥ğ‘‡, â– }
19

--------------------------------------------------------------------------------
[End of Page 25]

RNN Language Models
For a given sentence {ğ‘¥1, . . . , ğ‘¥ğ‘¡}, the input at time ğ‘¡is word
embedding ğ’™ğ‘¡
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
â– 
ğ’‰â–¡
ğ’‰1
ğ’‰2
ğ’‰3
ğ’‰4
â–¡
ğ’™1
ğ’™2
ğ’™3
ğ’™4
The probability distribution of next word ğ‘‹ğ‘¡
ğ‘ƒ(ğ‘‹ğ‘¡= ğ‘¥| ğ’™1:ğ‘¡âˆ’1) =
exp(ğ’˜T
ğ‘œ,ğ‘¥ğ’‰ğ‘¡âˆ’1)
Ã
ğ‘¥â€²âˆˆVexp(ğ’˜T
ğ‘œ,ğ‘¥â€²ğ’‰ğ‘¡âˆ’1)
(12)
where
â–¶ğ’˜ğ‘œ,ğ‘¥is the output weight vector (parameter) associated with
word ğ‘¥
â–¶Vis the word vocabulary
20

--------------------------------------------------------------------------------
[End of Page 26]

Special Cases
Similar to statistical language modeling, there are also two special
cases that we need to consider
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
â– 
ğ’‰â–¡
ğ’‰1
ğ’‰2
ğ’‰3
ğ’‰4
â–¡
ğ’™1
ğ’™2
ğ’™3
ğ’™4
{â–¡, ğ‘¥1, . . . , ğ‘¥ğ‘‡, â– }
The corresponding prediction functions are defined as
â–¶At time ğ‘¡= 1
ğ‘ƒ(ğ‘‹1 = ğ‘¥) âˆexp(ğ’˜T
ğ‘œ,ğ‘¥ğ’‰â–¡)
(13)
â–¶At time ğ‘¡= ğ‘‡
ğ‘ƒ(ğ‘‹ğ‘‡= â– | ğ’™1:ğ‘‡âˆ’1) âˆexp(ğ’˜T
ğ‘œ,ğ‘¥ğ’‰ğ‘‡âˆ’1)
(14)
21

--------------------------------------------------------------------------------
[End of Page 27]

Challenge of Training RNNs

--------------------------------------------------------------------------------
[End of Page 28]

Objective
The training objective for each timestep is to predict the next token in
the text
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
â– 
ğ’‰â–¡
ğ’‰1
ğ’‰2
ğ’‰3
ğ’‰4
â–¡
ğ’™1
ğ’™2
ğ’™3
ğ’™4
â–¶Prediction at step ğ‘¡, ğ‘ƒ(ğ‘‹ğ‘¡= ğ‘¥| ğ’™1:ğ‘¡âˆ’1) =
exp(ğ’˜T
ğ‘œ,ğ‘¥ğ’‰ğ‘¡âˆ’1)
Ã
ğ‘¥â€²âˆˆVexp(ğ’˜T
ğ‘œ,ğ‘¥â€² ğ’‰ğ‘¡âˆ’1)
â–¶Loss at step ğ‘¡, ğ¿ğ‘¡= âˆ’log ğ‘ƒ(ğ‘‹ğ‘¡= ğ‘¥| ğ’™1:ğ‘¡âˆ’1)
23

--------------------------------------------------------------------------------
[End of Page 29]

Gradients
Let ğœ½denote all model parameters
ğœ•â„“
ğœ•ğœ½=
ğ‘‡
Ã•
ğ‘¡=1
ğœ•ğ¿ğ‘¡
ğœ•ğœ½
(15)
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
â– 
ğ’‰â–¡
ğ’‰1
ğ’‰2
ğ’‰3
ğ’‰4
â–¡
ğ’™1
ğ’™2
ğ’™3
ğ’™4
Backpropagation Through Time [Rumelhart et al., 1985, BPTT]
24

--------------------------------------------------------------------------------
[End of Page 30]

Model Parameters
Before computing the gradient of each ğ¿ğ‘¡with respect to model
parameters, let us count how many parameters that we need consider
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
â– 
ğ’‰â–¡
ğ’‰1
ğ’‰2
ğ’‰3
ğ’‰4
â–¡
ğ’™1
ğ’™2
ğ’™3
ğ’™4
â–¶Output parameter matrix ğ‘¾ğ‘œ= (ğ’˜ğ‘œ,1, . . . , ğ’˜ğ‘œ,ğ‘‰)
25

--------------------------------------------------------------------------------
[End of Page 31]

Model Parameters
Before computing the gradient of each ğ¿ğ‘¡with respect to model
parameters, let us count how many parameters that we need consider
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
â– 
ğ’‰â–¡
ğ’‰1
ğ’‰2
ğ’‰3
ğ’‰4
â–¡
ğ’™1
ğ’™2
ğ’™3
ğ’™4
â–¶Output parameter matrix ğ‘¾ğ‘œ= (ğ’˜ğ‘œ,1, . . . , ğ’˜ğ‘œ,ğ‘‰)
â–¶Input word embedding matrix ğ‘¿= (ğ’™1, . . . , ğ’™ğ‘‰)
25

--------------------------------------------------------------------------------
[End of Page 32]

Model Parameters
Before computing the gradient of each ğ¿ğ‘¡with respect to model
parameters, let us count how many parameters that we need consider
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
â– 
ğ’‰â–¡
ğ’‰1
ğ’‰2
ğ’‰3
ğ’‰4
â–¡
ğ’™1
ğ’™2
ğ’™3
ğ’™4
â–¶Output parameter matrix ğ‘¾ğ‘œ= (ğ’˜ğ‘œ,1, . . . , ğ’˜ğ‘œ,ğ‘‰)
â–¶Input word embedding matrix ğ‘¿= (ğ’™1, . . . , ğ’™ğ‘‰)
â–¶Neural network parameters ğ‘¾â„, ğ‘¾ğ‘–, ğ’ƒ
25

--------------------------------------------------------------------------------
[End of Page 33]

Backpropagation Through Time
Take time step ğ‘¡as an example, we can take a look the gradient
computation of some specific parameters
â–¶Output model parameter
ğœ•ğ¿ğ‘¡
ğœ•ğ’˜ğ‘œ,Â·
26

--------------------------------------------------------------------------------
[End of Page 34]

Backpropagation Through Time
Take time step ğ‘¡as an example, we can take a look the gradient
computation of some specific parameters
â–¶Output model parameter
ğœ•ğ¿ğ‘¡
ğœ•ğ’˜ğ‘œ,Â·
â–¶Neural network parameters, for example ğ‘¾â„
ğœ•ğ¿ğ‘¡
ğœ•ğ‘¾â„
=
ğ‘¡Ã•
ğ‘–=1
 ğœ•ğ¿ğ‘¡
ğœ•ğ’‰ğ‘¡
Â·   ğ‘¡âˆ’1
Ã–
ğ‘—=ğ‘–
ğœ•ğ’‰ğ‘—+1
ğœ•ğ’‰ğ‘—
 Â· ğœ•ğ’‰ğ‘–
ğœ•ğ‘¾â„
	
(16)
Similar patterns for the other two neural network parameters ğ‘¾ğ‘–
and ğ’ƒ
26

--------------------------------------------------------------------------------
[End of Page 35]

Backpropagation Through Time
Take time step ğ‘¡as an example, we can take a look the gradient
computation of some specific parameters
â–¶Output model parameter
ğœ•ğ¿ğ‘¡
ğœ•ğ’˜ğ‘œ,Â·
â–¶Neural network parameters, for example ğ‘¾â„
ğœ•ğ¿ğ‘¡
ğœ•ğ‘¾â„
=
ğ‘¡Ã•
ğ‘–=1
 ğœ•ğ¿ğ‘¡
ğœ•ğ’‰ğ‘¡
Â·   ğ‘¡âˆ’1
Ã–
ğ‘—=ğ‘–
ğœ•ğ’‰ğ‘—+1
ğœ•ğ’‰ğ‘—
 Â· ğœ•ğ’‰ğ‘–
ğœ•ğ‘¾â„
	
(16)
Similar patterns for the other two neural network parameters ğ‘¾ğ‘–
and ğ’ƒ
â–¶Word embedding ğœ•ğ¿ğ‘¡
ğœ•ğ’™ğ‘¡â€²
â–¶E.g., word embedding ğ’™ğ‘¡â€² is the input of ğ’‰ğ‘¡if ğ‘¡â€² â‰¤ğ‘¡, so ...
26

--------------------------------------------------------------------------------
[End of Page 36]

Challenges
For each timestep, we need to compute the gradient using the chain
rule:
ğœ•ğ¿ğ‘¡
ğœ•ğ‘¾â„
=
ğ‘¡Ã•
ğ‘–=1
 ğœ•ğ¿ğ‘¡
ğœ•ğ’‰ğ‘¡
Â·   ğ‘¡âˆ’1
Ã–
ğ‘—=ğ‘–
ğœ•ğ’‰ğ‘—+1
ğœ•ğ’‰ğ‘—
 Â· ğœ•ğ’‰ğ‘–
ğœ•ğ‘¾â„
	
(17)
The chain rule of gradient will cause two potential problems in
training RNNs
â–¶vanishing gradients: ğœ•ğ¿ğ‘¡
ğœ•ğœ½â†’0
â–¶exploding gradients: ğœ•ğ¿ğ‘¡
ğœ•ğœ½â‰¥ğ‘€
[Pascanu et al., 2013]
27

--------------------------------------------------------------------------------
[End of Page 37]

Exploding Gradients
Solution: norm clipping [Pascanu et al., 2013].
Consider the gradient ğ’ˆ= ğœ•â„“
ğœ•ğœ½,
Ë†ğ’ˆâ†ğœÂ·
ğ’ˆ
âˆ¥ğ’ˆâˆ¥
(18)
when âˆ¥ğ’ˆâˆ¥> ğœ.
â–¶Usually, ğœ= 3 or 5 in practice.
â–¶Smaller gradient will cause slower learning progress
28

--------------------------------------------------------------------------------
[End of Page 38]

Vanishing Gradients
Solution:
â–¶initialize parameters carefully
â–¶replace hidden state transition function ğˆ(Â·) with other options
ğ’‡(ğ’™ğ‘¡, ğ’‰ğ‘¡âˆ’1) = ğˆ(Wâ„ğ’‰ğ‘¡âˆ’1 + Wğ‘–ğ’™ğ‘¡+ ğ’ƒ)
(19)
â–¶LSTM [Hochreiter and Schmidhuber, 1997]
â–¶GRU [Cho et al., 2014]
29

--------------------------------------------------------------------------------
[End of Page 39]

Long Short-Term Memory
Rather than directly taking input and hidden state as simple
transition function, LSTM relies on three cates to control how much
information it should take from input and hidden state before
combining them together
ğ’Šğ‘¡
=
ğœ(Wğ‘¥ğ‘–ğ’™ğ‘¡+ Wâ„ğ‘–ğ’‰ğ‘¡âˆ’1 + Wğ‘ğ‘–ğ’„ğ‘¡âˆ’1 + ğ’ƒğ‘–)
ğ’‡ğ‘¡
=
ğœ(Wğ‘¥ğ‘“ğ’™ğ‘¡+ Wâ„ğ‘“ğ’‰ğ‘¡âˆ’1 + Wğ‘ğ‘“ğ’„ğ‘¡âˆ’1 + ğ’ƒğ‘“)
ğ’„ğ‘¡
=
ğ’‡ğ‘¡â—¦ğ’„ğ‘¡âˆ’1 + ğ’Šğ‘¡â—¦tanh(Wğ‘¥ğ‘ğ’™ğ‘¡+ Wâ„ğ‘ğ’‰ğ‘¡âˆ’1 + ğ’ƒğ‘)
ğ’ğ‘¡
=
ğœ(Wğ‘¥ğ‘œğ’™ğ‘¡+ Wâ„ğ‘œğ’‰ğ‘¡âˆ’1 + Wğ‘ğ‘œğ’„ğ‘¡+ ğ’ƒğ‘œ)
ğ’‰ğ‘¡
=
ğ’ğ‘¡â—¦tanh(ğ’„ğ‘¡)
where â—¦is the element-wise multiplication, {ğ‘¾Â·} and {ğ’ƒÂ·} are
parameters.
[Graves, 2013]
30

--------------------------------------------------------------------------------
[End of Page 40]

Reference
Cho, K., Van MerriÃ«nboer, B., Bahdanau, D., and Bengio, Y. (2014).
On the properties of neural machine translation: Encoder-decoder approaches.
arXiv preprint arXiv:1409.1259.
Goodfellow, I., Bengio, Y., Courville, A., and Bengio, Y. (2016).
Deep Learning, volume 1.
MIT press Cambridge.
Graves, A. (2013).
Generating sequences with recurrent neural networks.
arXiv preprint arXiv:1308.0850.
Hochreiter, S. and Schmidhuber, J. (1997).
Long short-term memory.
Neural computation, 9(8):1735â€“1780.
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998).
Gradient-based learning applied to document recognition.
Proceedings of the IEEE, 86(11):2278â€“2324.
Pascanu, R., Mikolov, T., and Bengio, Y. (2013).
On the difficulty of training recurrent neural networks.
In Dasgupta, S. and McAllester, D., editors, Proceedings of the 30th International Conference on Machine Learning, volume 28 of
Proceedings of Machine Learning Research, pages 1310â€“1318, Atlanta, Georgia, USA. PMLR.
Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1985).
Learning internal representations by error propagation.
Technical report, California Univ San Diego La Jolla Inst for Cognitive Science.
31

--------------------------------------------------------------------------------
[End of Page 41]