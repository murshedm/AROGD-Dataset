CS 4774 Machine Learning
Dimensionality Reduction
Yangfeng Ji
Information and Language Processing Lab
Department of Computer Science
University of Virginia

--------------------------------------------------------------------------------
[End of Page 1]

Overview
1. Reducing Dimensions
2. Principal Component Analysis
3. A Different Viewpoint of PCA
1

--------------------------------------------------------------------------------
[End of Page 2]

Reducing Dimensions

--------------------------------------------------------------------------------
[End of Page 3]

Curse of Dimensionality
What is the volume difference between two ğ‘‘-dimensional balls with
radii ğ‘Ÿ1 = 1 and ğ‘Ÿ2 = 0.99
3

--------------------------------------------------------------------------------
[End of Page 4]

Curse of Dimensionality
What is the volume difference between two ğ‘‘-dimensional balls with
radii ğ‘Ÿ1 = 1 and ğ‘Ÿ2 = 0.99
â–¶ğ‘‘= 2: 1
2ğœ‹(ğ‘Ÿ2
1 âˆ’ğ‘Ÿ2
2) â‰ˆ0.03
â–¶ğ‘‘= 3: 4
3ğœ‹(ğ‘Ÿ3
1 âˆ’ğ‘Ÿ3
2) â‰ˆ0.12
3

--------------------------------------------------------------------------------
[End of Page 5]

Curse of Dimensionality
What is the volume difference between two ğ‘‘-dimensional balls with
radii ğ‘Ÿ1 = 1 and ğ‘Ÿ2 = 0.99
â–¶ğ‘‘= 2: 1
2ğœ‹(ğ‘Ÿ2
1 âˆ’ğ‘Ÿ2
2) â‰ˆ0.03
â–¶ğ‘‘= 3: 4
3ğœ‹(ğ‘Ÿ3
1 âˆ’ğ‘Ÿ3
2) â‰ˆ0.12
â–¶General form:
ğœ‹ğ‘‘/2
Î“( ğ‘‘
2 +1)(ğ‘Ÿğ‘‘
1 âˆ’ğ‘Ÿğ‘‘
2) with
ğ‘Ÿğ‘‘
2 â†’0 when ğ‘‘â†’âˆ
â–¶E.g., ğ‘Ÿ500
2
= 0.00657
3

--------------------------------------------------------------------------------
[End of Page 6]

Curse of Dimensionality
What is the volume difference between two ğ‘‘-dimensional balls with
radii ğ‘Ÿ1 = 1 and ğ‘Ÿ2 = 0.99
â–¶ğ‘‘= 2: 1
2ğœ‹(ğ‘Ÿ2
1 âˆ’ğ‘Ÿ2
2) â‰ˆ0.03
â–¶ğ‘‘= 3: 4
3ğœ‹(ğ‘Ÿ3
1 âˆ’ğ‘Ÿ3
2) â‰ˆ0.12
â–¶General form:
ğœ‹ğ‘‘/2
Î“( ğ‘‘
2 +1)(ğ‘Ÿğ‘‘
1 âˆ’ğ‘Ÿğ‘‘
2) with
ğ‘Ÿğ‘‘
2 â†’0 when ğ‘‘â†’âˆ
â–¶E.g., ğ‘Ÿ500
2
= 0.00657
Question: what will happen if we uniformly sample from a
ğ‘‘-dimensional ball?
3

--------------------------------------------------------------------------------
[End of Page 7]

Curse of Dimensionality (II)
If we randomly sample 1K unit vectors from a ğ‘‘-dimensional space
and calculate the the Euclidean distance between any two vectors,
then the distance distribution looks like
4

--------------------------------------------------------------------------------
[End of Page 8]

Curse of Dimensionality (II)
If we randomly sample 1K unit vectors from a ğ‘‘-dimensional space
and calculate the the Euclidean distance between any two vectors,
then the distance distribution looks like
Figure: ğ‘‘= 100
4

--------------------------------------------------------------------------------
[End of Page 9]

Curse of Dimensionality (II)
If we randomly sample 1K unit vectors from a ğ‘‘-dimensional space
and calculate the the Euclidean distance between any two vectors,
then the distance distribution looks like
Figure: ğ‘‘= 500
4

--------------------------------------------------------------------------------
[End of Page 10]

Curse of Dimensionality (II)
If we randomly sample 1K unit vectors from a ğ‘‘-dimensional space
and calculate the the Euclidean distance between any two vectors,
then the distance distribution looks like
Figure: ğ‘‘= 1000
4

--------------------------------------------------------------------------------
[End of Page 11]

Dimensionality Reduction
Dimensionality Reduction is the process of taking data in a high
dimensional space and mapping it into a new space whose
dimensionality is much smaller.
5

--------------------------------------------------------------------------------
[End of Page 12]

Dimensionality Reduction
Dimensionality Reduction is the process of taking data in a high
dimensional space and mapping it into a new space whose
dimensionality is much smaller.
Mathematically, it means
ğ‘“: ğ’™â†’Ëœğ’™
(1)
where ğ’™âˆˆâ„ğ‘‘, Ëœğ’™âˆˆâ„ğ‘›with ğ‘›< ğ‘‘
5

--------------------------------------------------------------------------------
[End of Page 13]

Reducing Dimensions: A toy example
For the purpose of reducing dimensions, we can project ğ’™= (ğ‘¥1, ğ‘¥2)
into the direction along ğ‘¥1 or ğ‘¥2
ğ‘¥1
ğ‘¥2
Question: Given these two data examples, which direction we should
pick? ğ‘¥1 or ğ‘¥2?
6

--------------------------------------------------------------------------------
[End of Page 14]

Reducing Dimensions: A toy example
For the purpose of reducing dimensions, we can project ğ’™= (ğ‘¥1, ğ‘¥2)
into the direction along ğ‘¥1 or ğ‘¥2
ğ‘¥1
ğ‘¥2
Question: Given these two data examples, which direction we should
pick? ğ‘¥1 or ğ‘¥2?
6

--------------------------------------------------------------------------------
[End of Page 15]

Reducing Dimensions: A toy example (II)
There is a better solution if we are allowed to rotate the coordinate
ğ‘¥1
ğ‘¥2
7

--------------------------------------------------------------------------------
[End of Page 16]

Reducing Dimensions: A toy example (II)
There is a better solution if we are allowed to rotate the coordinate
ğ‘¥1
ğ‘¥2
ğ‘¢1
ğ‘¢2
Pick ğ‘¢1, then we preserve all the variance of the examples
7

--------------------------------------------------------------------------------
[End of Page 17]

Reducing Dimensions: A toy example (III)
Consider a general case, where the examples do not lie on a perfect
line
[Bishop, 2006, Section 12.1]
8

--------------------------------------------------------------------------------
[End of Page 18]

Reducing Dimensions: A toy example (III)
Consider a general case, where the examples do not lie on a perfect
line
We can follow the same idea by finding a direction that can preserve
most of the variance of the examples
[Bishop, 2006, Section 12.1]
8

--------------------------------------------------------------------------------
[End of Page 19]

Principal Component Analysis

--------------------------------------------------------------------------------
[End of Page 20]

Formulation
Given a set of example ğ‘†= {ğ’™1, . . . , ğ’™ğ‘š}
â–¶Centering the data by removing the mean Â¯ğ’™= 1
ğ‘š
Ãğ‘š
ğ‘–=1 ğ’™ğ‘–
ğ’™ğ‘–â†ğ’™ğ‘–âˆ’Â¯ğ’™
âˆ€ğ‘–âˆˆ[ğ‘š]
(2)
10

--------------------------------------------------------------------------------
[End of Page 21]

Formulation
Given a set of example ğ‘†= {ğ’™1, . . . , ğ’™ğ‘š}
â–¶Centering the data by removing the mean Â¯ğ’™= 1
ğ‘š
Ãğ‘š
ğ‘–=1 ğ’™ğ‘–
ğ’™ğ‘–â†ğ’™ğ‘–âˆ’Â¯ğ’™
âˆ€ğ‘–âˆˆ[ğ‘š]
(2)
â–¶Assume the direction that we would like to project the data is ğ’–,
then the objective function is the data variance
ğ½(ğ’–) = 1
ğ‘š
ğ‘š
Ã•
ğ‘–=1
(ğ’–Tğ’™ğ‘–)2
(3)
10

--------------------------------------------------------------------------------
[End of Page 22]

Formulation
Given a set of example ğ‘†= {ğ’™1, . . . , ğ’™ğ‘š}
â–¶Centering the data by removing the mean Â¯ğ’™= 1
ğ‘š
Ãğ‘š
ğ‘–=1 ğ’™ğ‘–
ğ’™ğ‘–â†ğ’™ğ‘–âˆ’Â¯ğ’™
âˆ€ğ‘–âˆˆ[ğ‘š]
(2)
â–¶Assume the direction that we would like to project the data is ğ’–,
then the objective function is the data variance
ğ½(ğ’–) = 1
ğ‘š
ğ‘š
Ã•
ğ‘–=1
(ğ’–Tğ’™ğ‘–)2
(3)
â–¶Maximize ğ½(ğ’–) is trivial, if there is no constriant on ğ’–. Therefore,
we set âˆ¥ğ’–âˆ¥2
2 = ğ’–Tğ’–= 1
10

--------------------------------------------------------------------------------
[End of Page 23]

Covariance Matrix
The definition of ğ½(ğ’–) can be written as
ğ½(ğ’–)
=
1
ğ‘š
ğ‘š
Ã•
ğ‘–=1
(ğ’–Tğ’™ğ‘–)2
(4)
=
1
ğ‘š
ğ‘š
Ã•
ğ‘–=1
ğ’–Tğ’™ğ‘–ğ’–Tğ’™ğ‘–
(5)
=
1
ğ‘š
ğ‘š
Ã•
ğ‘–=1
ğ’–Tğ’™ğ‘–ğ’™T
ğ‘–ğ’–
(6)
=
ğ’–T 1
ğ‘š
ğ‘š
Ã•
ğ‘–=1
ğ’™ğ‘–ğ’™T
ğ‘–

ğ’–
(7)
=
ğ’–Tğšºğ’–
(8)
where ğšºis the data covariance matrix
11

--------------------------------------------------------------------------------
[End of Page 24]

Optimization
â–¶The optimization of finding a single direction projection is
max
ğ’–
ğ½(ğ’–)
=
ğ’–Tğšºğ’–
(9)
s.t.
ğ’–Tğ’–= 1
(10)
12

--------------------------------------------------------------------------------
[End of Page 25]

Optimization
â–¶The optimization of finding a single direction projection is
max
ğ’–
ğ½(ğ’–)
=
ğ’–Tğšºğ’–
(9)
s.t.
ğ’–Tğ’–= 1
(10)
â–¶It can be converted to an unconstrained optimization problem
with a Lagrange multiplier
max
ğ’–

ğ’–Tğšºğ’–+ ğœ†(1 âˆ’ğ’–Tğ’–)	
(11)
12

--------------------------------------------------------------------------------
[End of Page 26]

Optimization
â–¶The optimization of finding a single direction projection is
max
ğ’–
ğ½(ğ’–)
=
ğ’–Tğšºğ’–
(9)
s.t.
ğ’–Tğ’–= 1
(10)
â–¶It can be converted to an unconstrained optimization problem
with a Lagrange multiplier
max
ğ’–

ğ’–Tğšºğ’–+ ğœ†(1 âˆ’ğ’–Tğ’–)	
(11)
â–¶The optimal solution is given by
ğšºğ’–âˆ’ğœ†ğ’–= 0
(12)
ğšºğ’–= ğœ†ğ’–
(13)
12

--------------------------------------------------------------------------------
[End of Page 27]

Two Observations
There are two observations from
ğšºğ’–= ğœ†ğ’–
(14)
â–¶First, ğœ†is an eigenvalue of ğšºand ğ’–is the corresponding
eigenvector
13

--------------------------------------------------------------------------------
[End of Page 28]

Two Observations
There are two observations from
ğšºğ’–= ğœ†ğ’–
(14)
â–¶First, ğœ†is an eigenvalue of ğšºand ğ’–is the corresponding
eigenvector
â–¶Second, multiplying ğ’–T on both sides, we have
ğ’–Tğšºğ’–= ğœ†
(15)
In order to maximize ğ½(ğ’–), ğœ†has to the largest eigenvalue ğ’–is the
corresponding eigen vector.
13

--------------------------------------------------------------------------------
[End of Page 29]

Principal Component Analysis
â–¶As ğ’–indicates the first major direction that can preserve the data
variance, it is called the first principal component
14

--------------------------------------------------------------------------------
[End of Page 30]

Principal Component Analysis
â–¶As ğ’–indicates the first major direction that can preserve the data
variance, it is called the first principal component
â–¶In general, with eigen decomposition, we have
ğ‘¼Tğšºğ‘¼= ğš²
(16)
â–¶Eigenvalues ğš²= diag(ğœ†1, . . . , ğœ†ğ‘‘)
â–¶Eigenvectors ğ‘¼= [ğ’–1, . . . , ğ’–ğ‘‘]
14

--------------------------------------------------------------------------------
[End of Page 31]

Principal Component Analysis (II)
Assume in ğš²= diag(ğœ†1, . . . , ğœ†ğ‘‘),
ğœ†1 â‰¥ğœ†2 â‰¥Â· Â· Â· â‰¥ğœ†ğ‘‘
(17)
15

--------------------------------------------------------------------------------
[End of Page 32]

Principal Component Analysis (II)
Assume in ğš²= diag(ğœ†1, . . . , ğœ†ğ‘‘),
ğœ†1 â‰¥ğœ†2 â‰¥Â· Â· Â· â‰¥ğœ†ğ‘‘
(17)
To reduce the dimensionality of ğ’™from ğ‘‘to ğ‘›, with ğ‘›< ğ‘‘
â–¶Take the first ğ‘›eigenvectors in ğ‘¼and form
Ëœğ‘¼= [ğ’–1, . . . , ğ’–ğ‘›] âˆˆâ„ğ‘‘Ã—ğ‘›
(18)
15

--------------------------------------------------------------------------------
[End of Page 33]

Principal Component Analysis (II)
Assume in ğš²= diag(ğœ†1, . . . , ğœ†ğ‘‘),
ğœ†1 â‰¥ğœ†2 â‰¥Â· Â· Â· â‰¥ğœ†ğ‘‘
(17)
To reduce the dimensionality of ğ’™from ğ‘‘to ğ‘›, with ğ‘›< ğ‘‘
â–¶Take the first ğ‘›eigenvectors in ğ‘¼and form
Ëœğ‘¼= [ğ’–1, . . . , ğ’–ğ‘›] âˆˆâ„ğ‘‘Ã—ğ‘›
(18)
â–¶Reduce the dimensionality of ğ’™as
Ëœğ’™= Ëœğ‘¼Tğ’™âˆˆâ„ğ‘›
(19)
15

--------------------------------------------------------------------------------
[End of Page 34]

Principal Component Analysis (II)
Assume in ğš²= diag(ğœ†1, . . . , ğœ†ğ‘‘),
ğœ†1 â‰¥ğœ†2 â‰¥Â· Â· Â· â‰¥ğœ†ğ‘‘
(17)
To reduce the dimensionality of ğ’™from ğ‘‘to ğ‘›, with ğ‘›< ğ‘‘
â–¶Take the first ğ‘›eigenvectors in ğ‘¼and form
Ëœğ‘¼= [ğ’–1, . . . , ğ’–ğ‘›] âˆˆâ„ğ‘‘Ã—ğ‘›
(18)
â–¶Reduce the dimensionality of ğ’™as
Ëœğ’™= Ëœğ‘¼Tğ’™âˆˆâ„ğ‘›
(19)
â–¶The value of ğ‘›can be determined by the following
Ãğ‘›
ğ‘–=1 ğœ†ğ‘–
Ãğ‘‘
ğ‘–=1 ğœ†ğ‘–
â‰ˆ0.95
(20)
15

--------------------------------------------------------------------------------
[End of Page 35]

Reconstructing ğ’™?
What if we want to reconstruct ğ’™(â„ğ‘‘) from Ëœğ’™(â„ğ‘›)?
â–¶The answer is
ğ’™pca = Ëœğ‘¼Ëœğ’™âˆˆâ„ğ‘‘
16

--------------------------------------------------------------------------------
[End of Page 36]

Reconstructing ğ’™?
What if we want to reconstruct ğ’™(â„ğ‘‘) from Ëœğ’™(â„ğ‘›)?
â–¶The answer is
ğ’™pca = Ëœğ‘¼Ëœğ’™âˆˆâ„ğ‘‘
â–¶Therefore, we have
ğ’™pca = Ëœğ‘¼Ëœğ‘¼Tğ’™
as a reasonable approximation of ğ’™
16

--------------------------------------------------------------------------------
[End of Page 37]

Applications: Image Processing
Reduce the dimensionality of an image dataset from 28 Ã— 28 = 784 to
ğ‘€
(a) Original data
[Bishop, 2006, Section 12.1]
17

--------------------------------------------------------------------------------
[End of Page 38]

Applications: Image Processing
Reduce the dimensionality of an image dataset from 28 Ã— 28 = 784 to
ğ‘€
(a) Original data
(b) With the first ğ‘€principal components
[Bishop, 2006, Section 12.1]
17

--------------------------------------------------------------------------------
[End of Page 39]

A Different Viewpoint of PCA

--------------------------------------------------------------------------------
[End of Page 40]

Data Reconstruction
Another way to formulate the objective function of PCA
min
ğ‘¾,ğ‘¼
ğ‘š
Ã•
ğ‘–=1
âˆ¥ğ’™ğ‘–âˆ’ğ‘¼ğ‘¾ğ’™ğ‘–âˆ¥2
2
(21)
where
â–¶ğ‘¾âˆˆâ„ğ‘›Ã—ğ‘‘: mapping ğ’™ğ‘–from the original space to a
lower-dimensional space â„ğ‘›
â–¶ğ‘¼âˆˆâ„ğ‘‘Ã—ğ‘›: mapping back the original space â„ğ‘‘
[Shalev-Shwartz and Ben-David, 2014, Chap 23]
19

--------------------------------------------------------------------------------
[End of Page 41]

Data Reconstruction
Another way to formulate the objective function of PCA
min
ğ‘¾,ğ‘¼
ğ‘š
Ã•
ğ‘–=1
âˆ¥ğ’™ğ‘–âˆ’ğ‘¼ğ‘¾ğ’™ğ‘–âˆ¥2
2
(21)
where
â–¶ğ‘¾âˆˆâ„ğ‘›Ã—ğ‘‘: mapping ğ’™ğ‘–from the original space to a
lower-dimensional space â„ğ‘›
â–¶ğ‘¼âˆˆâ„ğ‘‘Ã—ğ‘›: mapping back the original space â„ğ‘‘
â–¶Dimensionality reduction is performed as Ëœğ’™= ğ‘¼ğ’™, while ğ‘¾
make sure the reduction does not loss much information
[Shalev-Shwartz and Ben-David, 2014, Chap 23]
19

--------------------------------------------------------------------------------
[End of Page 42]

Optimization
Consider the optimization problem
min
ğ‘¾,ğ‘½
ğ‘š
Ã•
ğ‘–=1
âˆ¥ğ’™ğ‘–âˆ’ğ‘¼ğ‘¾ğ’™ğ‘–âˆ¥2
2
(22)
â–¶Let ğ‘¾, ğ‘¼be a solution of equation 24
[Shalev-Shwartz and Ben-David, 2014, Lemma 23.1]
â–¶the columns of ğ‘¼are orthonormal
â–¶ğ‘¾= ğ‘¼T
20

--------------------------------------------------------------------------------
[End of Page 43]

Optimization
Consider the optimization problem
min
ğ‘¾,ğ‘½
ğ‘š
Ã•
ğ‘–=1
âˆ¥ğ’™ğ‘–âˆ’ğ‘¼ğ‘¾ğ’™ğ‘–âˆ¥2
2
(22)
â–¶Let ğ‘¾, ğ‘¼be a solution of equation 24
[Shalev-Shwartz and Ben-David, 2014, Lemma 23.1]
â–¶the columns of ğ‘¼are orthonormal
â–¶ğ‘¾= ğ‘¼T
â–¶The optimization problem can be simplified as
min
ğ‘¼Tğ‘¼=ğ‘°
ğ‘š
Ã•
ğ‘–=1
âˆ¥ğ’™ğ‘–âˆ’ğ‘¼ğ‘¼Tğ’™ğ‘–âˆ¥2
2
(23)
The solution will be the same.
20

--------------------------------------------------------------------------------
[End of Page 44]

Nonlinear Extension
If we extend the both mappings to be nonlinear, then the model
becomes a simple encoder-decoder neural network model
min
ğ‘¾,ğ‘½
ğ‘š
Ã•
ğ‘–=1
âˆ¥ğ’™ğ‘–âˆ’tanh(ğ‘¼Â· tanh(ğ‘¾ğ’™ğ‘–))âˆ¥2
2
(24)
where
â–¶Ëœğ’™= tanh(ğ‘¾ğ’™ğ‘–) is a simple encoder
â–¶ğ’™= tanh(ğ‘¼Ëœğ’™) is a simple decoder
â–¶No closed-form solutions of ğ‘¾, ğ‘¼, although the
backpropagation algorithm still applies here
21

--------------------------------------------------------------------------------
[End of Page 45]

Reference
Bishop, C. M. (2006).
Pattern recognition and machine learning.
Springer.
Shalev-Shwartz, S. and Ben-David, S. (2014).
Understanding machine learning: From theory to algorithms.
Cambridge university press.
22

--------------------------------------------------------------------------------
[End of Page 46]