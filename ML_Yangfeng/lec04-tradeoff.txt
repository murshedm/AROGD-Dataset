CS 4774 Machine Learning
The Bias-Complexity Tradeoff
Yangfeng Ji
Information and Language Processing Lab
Department of Computer Science
University of Virginia

--------------------------------------------------------------------------------
[End of Page 1]

Overview
1. The Bias-Complexity Tradeoff
2. The Bias-Variance Tradeoff
3. The VC Dimension
Readings: [Shalev-Shwartz and Ben-David, 2014, Chapter 5 & 6]
1

--------------------------------------------------------------------------------
[End of Page 2]

Question
For a real-world machine learning problem, which of the following
items are usually available to us?
2

--------------------------------------------------------------------------------
[End of Page 3]

Question
For a real-world machine learning problem, which of the following
items are usually available to us?
â–¶Training set ğ‘†= {(ğ’™1, ğ‘¦1), . . . , (ğ’™ğ‘š, ğ‘¦ğ‘š)}
â–¶Domain set X
â–¶Label set Y
2

--------------------------------------------------------------------------------
[End of Page 4]

Question
For a real-world machine learning problem, which of the following
items are usually available to us?
â–¶Training set ğ‘†= {(ğ’™1, ğ‘¦1), . . . , (ğ’™ğ‘š, ğ‘¦ğ‘š)}
â–¶Domain set X
â–¶Label set Y
â–¶Labeling function (the oracle) ğ‘“
â–¶Distribution D over XÃ— Y
â–¶The Bayes predictor ğ‘“D(ğ’™)
2

--------------------------------------------------------------------------------
[End of Page 5]

Question
For a real-world machine learning problem, which of the following
items are usually available to us?
â–¶Training set ğ‘†= {(ğ’™1, ğ‘¦1), . . . , (ğ’™ğ‘š, ğ‘¦ğ‘š)}
â–¶Domain set X
â–¶Label set Y
â–¶Labeling function (the oracle) ğ‘“
â–¶Distribution D over XÃ— Y
â–¶The Bayes predictor ğ‘“D(ğ’™)
â–¶The size of the hypothesis space H
â–¶The empirical risk of a hypothesis â„(ğ’™) âˆˆH, ğ¿ğ‘†(â„(ğ’™))
â–¶The true risk of a hypothesis â„(ğ’™) âˆˆH, ğ¿D(â„(ğ’™))
2

--------------------------------------------------------------------------------
[End of Page 6]

A 2-dimensional Classification Problem
Consider the following four situations
Given Data Distribution
Given Training Examples
Nonlinear classifier
Linear classifier
3

--------------------------------------------------------------------------------
[End of Page 7]

Agnostic PAC Learnability
A hypothesis class His agnostic PAC learnable if there exist a
function ğ‘šH : (0, 1)2 â†’â„•and a learning algorithm with the
following property:
â–¶for every distribution D over XÃ— {âˆ’1, +1} and
â–¶for every ğœ–, ğ›¿âˆˆ(0, 1),
when running the learning algorithm on ğ‘šâ‰¥ğ‘šH(ğœ–, ğ›¿) i.i.d. examples
generated by D, the algorithm returns a hypothesis â„ğ‘†1 such that,
with probability of at least 1 âˆ’ğ›¿,
ğ¿D(â„ğ‘†) â‰¤min
â„â€²âˆˆHğ¿D(â„â€²) + ğœ–
(1)
1Sometimes, as â„ğ‘†(ğ’™) or â„(ğ’™, ğ‘†)
4

--------------------------------------------------------------------------------
[End of Page 8]

Agnostic PAC Learnability
A hypothesis class His agnostic PAC learnable if there exist a
function ğ‘šH : (0, 1)2 â†’â„•and a learning algorithm with the
following property:
â–¶for every distribution D over XÃ— {âˆ’1, +1} and
â–¶for every ğœ–, ğ›¿âˆˆ(0, 1),
when running the learning algorithm on ğ‘šâ‰¥ğ‘šH(ğœ–, ğ›¿) i.i.d. examples
generated by D, the algorithm returns a hypothesis â„ğ‘†1 such that,
with probability of at least 1 âˆ’ğ›¿,
ğ¿D(â„ğ‘†) â‰¤min
â„â€²âˆˆHğ¿D(â„â€²) + ğœ–
(1)
This explains the relation between the hypothesis learned with limited
data (â„ğ‘†) and the best hypothesis in the space (argminâ„â€²âˆˆH ğ¿D(â„â€²)).
1Sometimes, as â„ğ‘†(ğ’™) or â„(ğ’™, ğ‘†)
4

--------------------------------------------------------------------------------
[End of Page 9]

The Bayes Optimal Predictor
â–¶The Bayes optimal predictor: given a probability distribution D
over XÃ— {âˆ’1, +1}, the predictor is defined as
ğ‘“D(ğ‘¥) =

+1
if â„™[ğ‘¦= 1|ğ‘¥] â‰¥1
2
âˆ’1
otherwise
(2)
â–¶No other predictor can do better: for any predictor â„
ğ¿D(ğ‘“D) â‰¤ğ¿D(â„)
(3)
5

--------------------------------------------------------------------------------
[End of Page 10]

The Bayes Optimal Predictor
â–¶The Bayes optimal predictor: given a probability distribution D
over XÃ— {âˆ’1, +1}, the predictor is defined as
ğ‘“D(ğ‘¥) =

+1
if â„™[ğ‘¦= 1|ğ‘¥] â‰¥1
2
âˆ’1
otherwise
(2)
â–¶No other predictor can do better: for any predictor â„
ğ¿D(ğ‘“D) â‰¤ğ¿D(â„)
(3)
â–¶Question: for a given hypothesis space H, does the following
relation hold?
ğ‘“D âˆˆargmin
â„â€²âˆˆH
ğ¿D(â„â€²)
5

--------------------------------------------------------------------------------
[End of Page 11]

The Bayes Optimal Predictor
â–¶The Bayes optimal predictor: given a probability distribution D
over XÃ— {âˆ’1, +1}, the predictor is defined as
ğ‘“D(ğ‘¥) =

+1
if â„™[ğ‘¦= 1|ğ‘¥] â‰¥1
2
âˆ’1
otherwise
(2)
â–¶No other predictor can do better: for any predictor â„
ğ¿D(ğ‘“D) â‰¤ğ¿D(â„)
(3)
â–¶Question: for a given hypothesis space H, does the following
relation hold?
ğ‘“D âˆˆargmin
â„â€²âˆˆH
ğ¿D(â„â€²)
â–¶Answer: it depends the selection of the hypothesis space H,
usually not.
â–¶Example: if ğ‘“D is a nonlinear classifier, while we choose to use
logistic regression.
5

--------------------------------------------------------------------------------
[End of Page 12]

The Gap between â„ğ‘†and ğ‘“D
For illustration purpose, let us assume the gap between â„ğ‘†and ğ‘“D can
be visualized in the following plot
ğ‘¤1
ğ‘¤2
â„ğ‘†
ğ‘“D
ğœ–
â–¶â„ğ‘†= argminâ„â€²âˆˆH ğ¿ğ‘†(â„â€²): learned by minimizing the empirical risk
â–¶Constrained by the selection of H
â–¶ğ‘“D: the optimal predictor if we know the data distribution D
â–¶Not constrained by the selection of H
6

--------------------------------------------------------------------------------
[End of Page 13]

Outline
The previous example implies the error gap between â„ğ‘†and ğ‘“D can
be decomposed into two components
ğ‘¤1
ğ‘¤2
â„ğ‘†
ğ‘“D
ğœ–
7

--------------------------------------------------------------------------------
[End of Page 14]

Outline
The previous example implies the error gap between â„ğ‘†and ğ‘“D can
be decomposed into two components
ğ‘¤1
ğ‘¤2
â„ğ‘†
ğ‘“D
ğœ–
Two different perspectives of the decomposition
â–¶The bias-complexity tradeoff: from the perspective of learning
theory
â–¶The bias-variance tradeoff: from the perspective of statistical
estimation
7

--------------------------------------------------------------------------------
[End of Page 15]

The Bias-Complexity Tradeoff

--------------------------------------------------------------------------------
[End of Page 16]

Basic Learning Procedure
The basic component of formulating a learning process
â–¶Input/output space XÃ— Y
â–¶A collection of training examples ğ‘†= {(ğ’™ğ‘–, ğ‘¦ğ‘–)}ğ‘š
ğ‘–=1
â–¶Hypothesis space H
â–¶Learning via empirical risk minimization
â„ğ‘†âˆˆargmin
â„â€²âˆˆH
ğ¿ğ‘†(â„â€²) = 1
ğ‘š|{â„â€²(ğ’™ğ‘–) â‰ ğ‘¦ğ‘–}|
(4)
â–¶Analyzing the true error of â„ğ‘†
ğ¿D(â„ğ‘†) = ğ”¼[â„ğ‘†(ğ‘¥) â‰ ğ‘“(ğ‘¥)]
(5)
9

--------------------------------------------------------------------------------
[End of Page 17]

Example
Consider the binary classification problem with the data sampled
from the following distribution
D = 1
2 B(ğ‘¥; 5, 1) + 1
2 B(ğ‘¥; 1, 2)
(6)
10

--------------------------------------------------------------------------------
[End of Page 18]

Example (Cont.)
Given the distribution, we can compute the true risk/error of the
Bayes predictor ğ‘“D as
ğ¿D(ğ‘“D)
=
1
2 B(ğ‘¥< ğ‘Bayes; 5, 1) + 1
2(1 âˆ’B(ğ‘¥< ğ‘Bayes; 1, 2))
=
0.11799
(7)
11

--------------------------------------------------------------------------------
[End of Page 19]

Example (Cont.)
The hypothesis space His defined as
â„ğ‘–(ğ‘¥) =

+1
ğ‘¥>
ğ‘–
ğ‘
âˆ’1
ğ‘¥<
ğ‘–
ğ‘
(8)
where ğ‘âˆˆâ„•is a predefined integer
12

--------------------------------------------------------------------------------
[End of Page 20]

Example (Cont.)
The hypothesis space His defined as
â„ğ‘–(ğ‘¥) =

+1
ğ‘¥>
ğ‘–
ğ‘
âˆ’1
ğ‘¥<
ğ‘–
ğ‘
(8)
where ğ‘âˆˆâ„•is a predefined integer
â–¶The value of ğ‘is the size of the hypothesis space
12

--------------------------------------------------------------------------------
[End of Page 21]

Example (Cont.)
The hypothesis space His defined as
â„ğ‘–(ğ‘¥) =

+1
ğ‘¥>
ğ‘–
ğ‘
âˆ’1
ğ‘¥<
ğ‘–
ğ‘
(8)
where ğ‘âˆˆâ„•is a predefined integer
â–¶The value of ğ‘is the size of the hypothesis space
â–¶The best hypothesis in H
â„âˆ—âˆˆargmin
â„â€²âˆˆH
ğ¿D(â„â€²)
(9)
â–¶Very likely the best predictor in His not the Bayes predictor,
unless ğ‘Bayes âˆˆ{ ğ‘–
ğ‘: ğ‘–âˆˆ[ğ‘]}
12

--------------------------------------------------------------------------------
[End of Page 22]

Error Decomposition
The error gap between â„ğ‘†and ğ‘“D can be decomposed as two parts
ğ¿D(â„ğ‘†) âˆ’ğ¿D(ğ‘“D) = ğœ–app + ğœ–est
(10)
ğ‘¤1
ğ‘¤2
â„ğ‘†
ğ‘“D
â„âˆ—
ğœ–app
ğœ–est
13

--------------------------------------------------------------------------------
[End of Page 23]

Error Decomposition
The error gap between â„ğ‘†and ğ‘“D can be decomposed as two parts
ğ¿D(â„ğ‘†) âˆ’ğ¿D(ğ‘“D) = ğœ–app + ğœ–est
(10)
ğ‘¤1
ğ‘¤2
â„ğ‘†
ğ‘“D
â„âˆ—
ğœ–app
ğœ–est
â–¶Approximation error ğœ–app caused by selecting a specific
hypothesis space H(model bias)
â–¶Estimation error ğœ–est caused by selecting â„ğ‘†with a specific
training set (model complexity)
13

--------------------------------------------------------------------------------
[End of Page 24]

Approximation Error ğœ–app
To reduce the approximation error ğœ–app, we could increase the size of
the hypothesis space
ğ‘¤1
ğ‘¤2
â„ğ‘†
ğ‘“D
â„âˆ—
ğœ–app
ğœ–est
The cost is that we also increase the size of training set, in order to
maintain the overall error in the same level (recall the sample
complexity of finite hypothesis spaces).
14

--------------------------------------------------------------------------------
[End of Page 25]

Approximation Error ğœ–app
To reduce the approximation error ğœ–app, we could increase the size of
the hypothesis space
ğ‘¤1
ğ‘¤2
ğ‘“D
â„âˆ—
â„âˆ—
The cost is that we also increase the size of training set, in order to
maintain the overall error in the same level (recall the sample
complexity of finite hypothesis spaces).
14

--------------------------------------------------------------------------------
[End of Page 26]

Estimation Error ğœ–est
On the other hand, if we use the same training set ğ‘†, then we may
have a larger estimation error
ğ‘¤1
ğ‘¤2
â„ğ‘†
ğ‘“D
â„âˆ—
â„âˆ—
â„ğ‘†
15

--------------------------------------------------------------------------------
[End of Page 27]

Estimation Error ğœ–est
On the other hand, if we use the same training set ğ‘†, then we may
have a larger estimation error
ğ‘¤1
ğ‘¤2
â„ğ‘†
ğ‘“D
â„âˆ—
â„âˆ—
â„ğ‘†
The bias-complexity tradeoff: find the right balance to reduce both
approximation error and estimation error.
15

--------------------------------------------------------------------------------
[End of Page 28]

Example: 200 training examples
We randomly sampled 100 examples from each class
D = 1
2 B(ğ‘¥; 5, 1) + 1
2 B(ğ‘¥; 1, 2)
(11)
16

--------------------------------------------------------------------------------
[End of Page 29]

Example: 200 training examples
Given 200 training examples, the errors with respect to different
hypothesis space is the following (ğ‘¥axis is the size of H)
There is a tradeoff with respect to the size of H
17

--------------------------------------------------------------------------------
[End of Page 30]

Example: 2000 training examples
We randomly sampled 1000 examples from each class
D = 1
2 B(ğ‘¥; 5, 1) + 1
2 B(ğ‘¥; 1, 2)
(12)
18

--------------------------------------------------------------------------------
[End of Page 31]

Example: 2000 training examples
With these 2000 training examples, the errors with respect to different
hypothesis space is the following
Both errors are smaller, but the tradeoff still exists
19

--------------------------------------------------------------------------------
[End of Page 32]

Summary
Three components in this decomposition
â–¶â„ğ‘†âˆˆargminâ„â€²âˆˆH ğ¿ğ‘†(â„â€²): the ERM predictor given the training set
ğ‘†
â–¶â„âˆ—âˆˆargminâ„â€²âˆˆH ğ¿D(â„â€²): the optimal predictor from H
â–¶ğ‘“D: the Bayes predictor given D
20

--------------------------------------------------------------------------------
[End of Page 33]

Summary
Three components in this decomposition
â–¶â„ğ‘†âˆˆargminâ„â€²âˆˆH ğ¿ğ‘†(â„â€²): the ERM predictor given the training set
ğ‘†
â–¶â„âˆ—âˆˆargminâ„â€²âˆˆH ğ¿D(â„â€²): the optimal predictor from H
â–¶ğ‘“D: the Bayes predictor given D
Balancing strategy:
â–¶we can incrase the complexity of hypothesis space to reduce the
bias, e.g.,
â–¶enlarge the hypothesis space (as in the running example)
â–¶replacing linear predictors with nonlinear predictors
20

--------------------------------------------------------------------------------
[End of Page 34]

Summary
Three components in this decomposition
â–¶â„ğ‘†âˆˆargminâ„â€²âˆˆH ğ¿ğ‘†(â„â€²): the ERM predictor given the training set
ğ‘†
â–¶â„âˆ—âˆˆargminâ„â€²âˆˆH ğ¿D(â„â€²): the optimal predictor from H
â–¶ğ‘“D: the Bayes predictor given D
Balancing strategy:
â–¶we can incrase the complexity of hypothesis space to reduce the
bias, e.g.,
â–¶enlarge the hypothesis space (as in the running example)
â–¶replacing linear predictors with nonlinear predictors
â–¶in the meantime, we have to increase the training size to reduce
the approximation error.
20

--------------------------------------------------------------------------------
[End of Page 35]

The Bias-Variance Tradeoff

--------------------------------------------------------------------------------
[End of Page 36]

A New Perspective
Let us analyze the error ğœ–without the assumption of
â–¶knowing the best predictor from H, â„âˆ—âˆˆargminâ„â€²âˆˆH ğ¿D(â„â€²)
â–¶changing the size of ğ‘†
ğ‘¤1
ğ‘¤2
â„ğ‘†
ğ‘“D
ğœ–
22

--------------------------------------------------------------------------------
[End of Page 37]

A New Perspective
Let us analyze the error ğœ–without the assumption of
â–¶knowing the best predictor from H, â„âˆ—âˆˆargminâ„â€²âˆˆH ğ¿D(â„â€²)
â–¶changing the size of ğ‘†
ğ‘¤1
ğ‘¤2
â„ğ‘†
ğ‘“D
ğœ–
We still need (1) the ERM predictor â„ğ‘†and (2) the Bayes predictor ğ‘“D
22

--------------------------------------------------------------------------------
[End of Page 38]

A New Way of Decomposition
â–¶Consider the randomness in ğ‘†with ğ‘štraining examples
23

--------------------------------------------------------------------------------
[End of Page 39]

A New Way of Decomposition
â–¶Consider the randomness in ğ‘†with ğ‘štraining examples
23

--------------------------------------------------------------------------------
[End of Page 40]

A New Way of Decomposition
â–¶Consider the randomness in ğ‘†with ğ‘štraining examples
23

--------------------------------------------------------------------------------
[End of Page 41]

A New Way of Decomposition
â–¶Consider the randomness in ğ‘†with ğ‘štraining examples
â–¶In this case, ğ‘†is a random variable, â„(ğ’™, ğ‘†) is a function of ğ‘†and ğ’™
23

--------------------------------------------------------------------------------
[End of Page 42]

A New Way of Decomposition
â–¶Consider the randomness in ğ‘†with ğ‘štraining examples
â–¶In this case, ğ‘†is a random variable, â„(ğ’™, ğ‘†) is a function of ğ‘†and ğ’™
â–¶The average prediction function given by ğ¸[â„(ğ’™, ğ‘†)] where
ğ‘†âˆ¼Dğ‘š
â–¶Overall, ğ¸[â„(ğ’™, ğ‘†)] will give good performance on any possible
dataset with size ğ‘š
23

--------------------------------------------------------------------------------
[End of Page 43]

Data Generation Model
Consider the following data generation model
â–¶ğ‘‹âˆ¼ğ‘ˆ[0, 1] uniform distribution
â–¶ğ‘Œ= N(ğ‘‹+ sin(2ğ‘‹), ğœ2) with ğœ2 = 0.1
An example of ğ‘†is
24

--------------------------------------------------------------------------------
[End of Page 44]

Hypothesis Spaces
Given ğ‘†and the following hypothesis space H1
H1 = {ğ‘¤0 + ğ‘¤1ğ‘¥: ğ‘¤0, ğ‘¤1 âˆˆâ„}
(13)
the regression result
25

--------------------------------------------------------------------------------
[End of Page 45]

Hypothesis Spaces (Cont.)
Given ğ‘†and the following hypothesis space H3
H3 = {ğ‘¤0 + ğ‘¤1ğ‘¥+ ğ‘¤2ğ‘¥2 + ğ‘¤3ğ‘¥3 : ğ‘¤0, ğ‘¤1, ğ‘¤2, ğ‘¤3 âˆˆâ„}
(14)
the regression result
26

--------------------------------------------------------------------------------
[End of Page 46]

Hypothesis Spaces (Cont.)
Given ğ‘†and the following hypothesis space H15
H15 = {ğ‘¤0 + ğ‘¤1ğ‘¥+ Â· Â· Â· + ğ‘¤15ğ‘¥15 : ğ‘¤0, ğ‘¤1, Â· Â· Â· , ğ‘¤15 âˆˆâ„}
(15)
27

--------------------------------------------------------------------------------
[End of Page 47]

Hypothesis Spaces (Cont.)
Given ğ‘†and the following hypothesis space H15
H15 = {ğ‘¤0 + ğ‘¤1ğ‘¥+ Â· Â· Â· + ğ‘¤15ğ‘¥15 : ğ‘¤0, ğ‘¤1, Â· Â· Â· , ğ‘¤15 âˆˆâ„}
(15)
â–¶Intuitively, the degree of the polynomials indicates the
potential/complexity of the hypothesis space
â–¶Refer to the VC dimension section for more discussion
27

--------------------------------------------------------------------------------
[End of Page 48]

Error Decomposition
The difference between the best hypothesis â„(ğ’™, ğ‘†) and the Bayes
predictor ğ‘“D(ğ’™) is measured as
ğœ–2 = {â„(ğ’™, ğ‘†) âˆ’ğ‘“D(ğ’™)}2
(16)
Introduce ğ¸[â„(ğ’™, ğ‘†)] into the calculation, we have
ğœ–2
=
{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)] + ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}2
28

--------------------------------------------------------------------------------
[End of Page 49]

Error Decomposition
The difference between the best hypothesis â„(ğ’™, ğ‘†) and the Bayes
predictor ğ‘“D(ğ’™) is measured as
ğœ–2 = {â„(ğ’™, ğ‘†) âˆ’ğ‘“D(ğ’™)}2
(16)
Introduce ğ¸[â„(ğ’™, ğ‘†)] into the calculation, we have
ğœ–2
=
{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)] + ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}2
=
{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)]}2 + {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}2
+2{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)]} Â· {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}
28

--------------------------------------------------------------------------------
[End of Page 50]

Review: Mean
Given a random variable ğ‘‹and its probability density function ğ‘(ğ‘¥)
â–¶Mean: ğ¸[ğ‘‹] =
âˆ«
ğ‘¥ğ‘(ğ‘¥)ğ‘‘ğ‘¥
â–¶Example: the mean of a Gaussian distribution N(ğ‘¥; ğœ‡, ğœ2)
ğ¸[ğ‘‹] = ğœ‡
(17)
29

--------------------------------------------------------------------------------
[End of Page 51]

Review: Mean
Given a random variable ğ‘‹and its probability density function ğ‘(ğ‘¥)
â–¶Mean: ğ¸[ğ‘‹] =
âˆ«
ğ‘¥ğ‘(ğ‘¥)ğ‘‘ğ‘¥
â–¶Example: the mean of a Gaussian distribution N(ğ‘¥; ğœ‡, ğœ2)
ğ¸[ğ‘‹] = ğœ‡
(17)
â–¶Approximation to the mean with samples {ğ‘¥1, . . . , ğ‘¥ğ‘š}
ğ¸[ğ‘‹] â‰ˆ1
ğ‘š
ğ‘š
Ã•
ğ‘–=1
ğ‘¥ğ‘–
(18)
29

--------------------------------------------------------------------------------
[End of Page 52]

Review: Mean
Given a random variable ğ‘‹and its probability density function ğ‘(ğ‘¥)
â–¶Mean: ğ¸[ğ‘‹] =
âˆ«
ğ‘¥ğ‘(ğ‘¥)ğ‘‘ğ‘¥
â–¶Example: the mean of a Gaussian distribution N(ğ‘¥; ğœ‡, ğœ2)
ğ¸[ğ‘‹] = ğœ‡
(17)
â–¶Approximation to the mean with samples {ğ‘¥1, . . . , ğ‘¥ğ‘š}
ğ¸[ğ‘‹] â‰ˆ1
ğ‘š
ğ‘š
Ã•
ğ‘–=1
ğ‘¥ğ‘–
(18)
â–¶Property: ğ¸[ğ›¼ğ‘‹] = ğ›¼ğ¸[ğ‘‹] for ğ›¼is determinstic
29

--------------------------------------------------------------------------------
[End of Page 53]

Review: Variance
Given a random variable ğ‘‹, its probability density function ğ‘(ğ‘¥), and
its mean ğ¸[ğ‘‹]
â–¶Variance: Var(ğ‘‹) = ğ¸
(ğ‘‹âˆ’ğ¸[ğ‘‹])2
â–¶Example: the variance of a Gaussian distribution N(ğ‘¥; ğœ‡, ğœ2)
Var(ğ‘‹) = ğœ2
(19)
30

--------------------------------------------------------------------------------
[End of Page 54]

Review: Variance
Given a random variable ğ‘‹, its probability density function ğ‘(ğ‘¥), and
its mean ğ¸[ğ‘‹]
â–¶Variance: Var(ğ‘‹) = ğ¸
(ğ‘‹âˆ’ğ¸[ğ‘‹])2
â–¶Example: the variance of a Gaussian distribution N(ğ‘¥; ğœ‡, ğœ2)
Var(ğ‘‹) = ğœ2
(19)
â–¶Relation between Var(ğ‘‹) and ğ¸[ğ‘‹]
Var(ğ‘‹)
=
ğ¸
(ğ‘‹âˆ’ğ¸[ğ‘‹])2
=
ğ¸
ğ‘‹2 âˆ’2ğ‘‹ğ¸[ğ‘‹] + ğ¸[ğ‘‹]2
=
ğ¸
ğ‘‹2
âˆ’2ğ¸[ğ‘‹] ğ¸[ğ‘‹] + ğ¸[ğ‘‹]2
=
ğ¸
ğ‘‹2
âˆ’ğ¸[ğ‘‹]2
30

--------------------------------------------------------------------------------
[End of Page 55]

Error Decomposition (Cont.)
Recall
ğœ–2
=
{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)] + ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}2
=
{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)]}2 + {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}2
+2{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)]} Â· {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}
31

--------------------------------------------------------------------------------
[End of Page 56]

Error Decomposition (Cont.)
Recall
ğœ–2
=
{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)] + ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}2
=
{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)]}2 + {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}2
+2{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)]} Â· {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}
Taking the expectation of ğœ–2
ğ¸
h
ğœ–2i
=
ğ¸
h
{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)]}2i
+ {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}2
+2ğ¸[{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)]}] Â· {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}
31

--------------------------------------------------------------------------------
[End of Page 57]

Error Decomposition (Cont.)
Recall
ğœ–2
=
{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)] + ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}2
=
{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)]}2 + {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}2
+2{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)]} Â· {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}
Taking the expectation of ğœ–2
ğ¸
h
ğœ–2i
=
ğ¸
h
{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)]}2i
+ {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}2
+2ğ¸[{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)]}] Â· {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}
=
ğ¸
h
{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)]}2i
+ {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}2
+2{ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ¸[â„(ğ’™, ğ‘†)]} Â· {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}
31

--------------------------------------------------------------------------------
[End of Page 58]

Error Decomposition (Cont.)
Recall
ğœ–2
=
{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)] + ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}2
=
{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)]}2 + {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}2
+2{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)]} Â· {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}
Taking the expectation of ğœ–2
ğ¸
h
ğœ–2i
=
ğ¸
h
{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)]}2i
+ {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}2
+2ğ¸[{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)]}] Â· {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}
=
ğ¸
h
{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)]}2i
+ {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}2
+2{ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ¸[â„(ğ’™, ğ‘†)]} Â· {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}
=
ğ¸
h
{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)]}2i
+ {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}2
31

--------------------------------------------------------------------------------
[End of Page 59]

The Bias-Variance Decomposition
The expected error is decomposed as
ğ¸
ğœ–2
= ğ¸
{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)]}2
|                             {z                             }
variance
+ {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}2
|                      {z                      }
bias2
32

--------------------------------------------------------------------------------
[End of Page 60]

The Bias-Variance Decomposition
The expected error is decomposed as
ğ¸
ğœ–2
= ğ¸
{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)]}2
|                             {z                             }
variance
+ {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}2
|                      {z                      }
bias2
â–¶bias: how far the expected prediction ğ¸[â„(ğ’™, ğ‘†)] diverges from
the optimal predictor ğ‘“D(ğ’™)
32

--------------------------------------------------------------------------------
[End of Page 61]

The Bias-Variance Decomposition
The expected error is decomposed as
ğ¸
ğœ–2
= ğ¸
{â„(ğ’™, ğ‘†) âˆ’ğ¸[â„(ğ’™, ğ‘†)]}2
|                             {z                             }
variance
+ {ğ¸[â„(ğ’™, ğ‘†)] âˆ’ğ‘“D(ğ’™)}2
|                      {z                      }
bias2
â–¶bias: how far the expected prediction ğ¸[â„(ğ’™, ğ‘†)] diverges from
the optimal predictor ğ‘“D(ğ’™)
â–¶variance: how a hypothesis learned from a specific ğ‘†diverges
from the average prediction ğ¸[â„(ğ’™, ğ‘†)]
32

--------------------------------------------------------------------------------
[End of Page 62]

Computing ğ¸[â„(ğ’™, ğ‘†)]
The key of computing ğ¸[â„(ğ’™, ğ‘†)] is to eliminate the randomness
introduced by ğ‘†
1: for ğ‘˜= 1, Â· Â· Â· , ğ¾do
2:
Sample a traing set ğ‘†ğ‘˜with size ğ‘šfrom the data generation
model
3:
Find the best hypothesis via â„(ğ’™, ğ‘†ğ‘˜) âˆˆargminâ„â€² ğ¿(â„â€², ğ‘†ğ‘˜)
4: end for
5: Output:
ğ¸[â„(ğ’™, ğ‘†)] â‰ˆ1
ğ¾
ğ¾
Ã•
ğ‘˜=1
â„(ğ’™, ğ‘†ğ‘˜)
The larger ğ¾, the better approximation
33

--------------------------------------------------------------------------------
[End of Page 63]

Example: Bias and Variance
With ğ¾= 50, ğ‘š= 100, and H1, we can visualize the bias and variance
of a linear regression example as following
High bias and low variance (Underfitting)
34

--------------------------------------------------------------------------------
[End of Page 64]

Example: Bias and Variance (Cont.)
Same training set with H3
Both bias and variance are fine
35

--------------------------------------------------------------------------------
[End of Page 65]

Example: Bias and Variance (Cont.)
Same training set with H15
Low bias and high variance (Overfitting)
36

--------------------------------------------------------------------------------
[End of Page 66]

Example: Bias and Variance (Cont.)
Same training set with H15
Low bias and high variance (Overfitting)
Exercise: The bias-variance tradeoff on linear regression with â„“2
regularization
36

--------------------------------------------------------------------------------
[End of Page 67]

The Bias-Variance Tradeoff
â–¶bias: how far the expected prediction ğ¸[â„(ğ’™, ğ‘†)] diverges from
the optimal predictor ğ‘“D(ğ’™)
â–¶Error of this part is caused by the selection of a hypothesis space
37

--------------------------------------------------------------------------------
[End of Page 68]

The Bias-Variance Tradeoff
â–¶bias: how far the expected prediction ğ¸[â„(ğ’™, ğ‘†)] diverges from
the optimal predictor ğ‘“D(ğ’™)
â–¶Error of this part is caused by the selection of a hypothesis space
â–¶variance: how a hypothesis learned from a specific ğ‘†diverges
from the average prediction ğ¸[â„(ğ’™, ğ‘†)]
â–¶Error of this part is caused by using a particular data set ğ‘†
37

--------------------------------------------------------------------------------
[End of Page 69]

The VC Dimension

--------------------------------------------------------------------------------
[End of Page 70]

Learnability with Infinite Hypotheses
Infinite-size hypothesis space is learnable
Examples
â–¶Half-space predictor
â–¶Logistic regression predictor
â–¶Many others
39

--------------------------------------------------------------------------------
[End of Page 71]

Shattering
For a given set ğ‘†and a hypothesis space H,
â–¶A dichotomy of the set ğ‘†is one of the possible ways of labeling
the points in ğ‘†using a hypothesis â„âˆˆH
[Mohri et al., 2018, Page 36]
40

--------------------------------------------------------------------------------
[End of Page 72]

Shattering
For a given set ğ‘†and a hypothesis space H,
â–¶A dichotomy of the set ğ‘†is one of the possible ways of labeling
the points in ğ‘†using a hypothesis â„âˆˆH
â–¶A set ğ‘†of ğ‘šâ‰¥1 points is said to be shattered by a hypothesis
space H, if all possible dichotomies of ğ‘†can be realized by H
[Mohri et al., 2018, Page 36]
40

--------------------------------------------------------------------------------
[End of Page 73]

Shattering: Example
Consider the following set ğ‘†and the half-space hypothesis space
Hhalf = {ğ‘¤0 + ğ‘¤1ğ‘¥1 + ğ‘¤2ğ‘¥2 = 0 : ğ‘¤0, ğ‘¤1, ğ‘¤2 âˆˆâ„}
(20)
and the following specific set ğ‘†
ğ‘¥1
ğ‘¥2
There are 23 = 8 different ways to label the points and Hhalf can
realized all of them.
41

--------------------------------------------------------------------------------
[End of Page 74]

VC Dimension
The VC-dimension of a hypothesis space H, denoted VCdim(H), is
the maximal size of a set ğ‘†âŠ‚Xthat can be shattered by H.
[Shalev-Shwartz and Ben-David, 2014, Page 70]
42

--------------------------------------------------------------------------------
[End of Page 75]

VC Dimension
The VC-dimension of a hypothesis space H, denoted VCdim(H), is
the maximal size of a set ğ‘†âŠ‚Xthat can be shattered by H.
A: How to find the VC-dimension of a given hypothesis space?
Q: The proof consists of two parts:
â–¶There exists a set ğ‘†of size ğ‘‘that is shattered by H
â–¶Every set ğ‘†of size ğ‘‘+ 1 is not shattered by H
[Shalev-Shwartz and Ben-David, 2014, Page 70]
42

--------------------------------------------------------------------------------
[End of Page 76]

Half Spaces
Consider a special case as following, where VC-dim(Hhalf) = 3
Hhalf = {ğ‘¤0 + ğ‘¤1ğ‘¥1 + ğ‘¤2ğ‘¥2 = 0 : ğ‘¤0, ğ‘¤1, ğ‘¤2 âˆˆâ„}
(21)
(1) Exist a case
ğ‘¥1
ğ‘¥2
43

--------------------------------------------------------------------------------
[End of Page 77]

Half Spaces
Consider a special case as following, where VC-dim(Hhalf) = 3
Hhalf = {ğ‘¤0 + ğ‘¤1ğ‘¥1 + ğ‘¤2ğ‘¥2 = 0 : ğ‘¤0, ğ‘¤1, ğ‘¤2 âˆˆâ„}
(21)
(1) Exist a case
ğ‘¥1
ğ‘¥2
(2) For any case
ğ‘¥1
ğ‘¥2
43

--------------------------------------------------------------------------------
[End of Page 78]

Axis-aligned Rectangles
Let Hbe the class of axis-aligned rectangle, formally
H = {â„(ğ‘1,ğ‘2,ğ‘1,ğ‘2) : ğ‘1 â‰¤ğ‘2 and ğ‘1 â‰¤ğ‘2}
(22)
where
â„(ğ‘1,ğ‘2,ğ‘1,ğ‘2)(ğ‘¥1, ğ‘¥2) =

+1
ğ‘¥1 âˆˆ[ğ‘1, ğ‘2]and ğ‘¥2 âˆˆ[ğ‘1, ğ‘2]
âˆ’1
otherwise
44

--------------------------------------------------------------------------------
[End of Page 79]

Axis-aligned Rectangles
Let Hbe the class of axis-aligned rectangle, formally
H = {â„(ğ‘1,ğ‘2,ğ‘1,ğ‘2) : ğ‘1 â‰¤ğ‘2 and ğ‘1 â‰¤ğ‘2}
(22)
where
â„(ğ‘1,ğ‘2,ğ‘1,ğ‘2)(ğ‘¥1, ğ‘¥2) =

+1
ğ‘¥1 âˆˆ[ğ‘1, ğ‘2]and ğ‘¥2 âˆˆ[ğ‘1, ğ‘2]
âˆ’1
otherwise
Exist a case
44

--------------------------------------------------------------------------------
[End of Page 80]

Axis-aligned Rectangles
Let Hbe the class of axis-aligned rectangle, formally
H = {â„(ğ‘1,ğ‘2,ğ‘1,ğ‘2) : ğ‘1 â‰¤ğ‘2 and ğ‘1 â‰¤ğ‘2}
(22)
where
â„(ğ‘1,ğ‘2,ğ‘1,ğ‘2)(ğ‘¥1, ğ‘¥2) =

+1
ğ‘¥1 âˆˆ[ğ‘1, ğ‘2]and ğ‘¥2 âˆˆ[ğ‘1, ğ‘2]
âˆ’1
otherwise
For any case
44

--------------------------------------------------------------------------------
[End of Page 81]

Axis-aligned Rectangles
Let Hbe the class of axis-aligned rectangle, formally
H = {â„(ğ‘1,ğ‘2,ğ‘1,ğ‘2) : ğ‘1 â‰¤ğ‘2 and ğ‘1 â‰¤ğ‘2}
(22)
where
â„(ğ‘1,ğ‘2,ğ‘1,ğ‘2)(ğ‘¥1, ğ‘¥2) =

+1
ğ‘¥1 âˆˆ[ğ‘1, ğ‘2]and ğ‘¥2 âˆˆ[ğ‘1, ğ‘2]
âˆ’1
otherwise
For any case
VC-dim(Hrect) = 4
44

--------------------------------------------------------------------------------
[End of Page 82]

VC Dimension and the Number of Parameters
â–¶For linear predictors, the VC dimensions are equal to the
numbers of parameters
Hhalf = {ğ‘¤0 + ğ‘¤1ğ‘¥1 + ğ‘¤2ğ‘¥2 = 0 : ğ‘¤0, ğ‘¤1, ğ‘¤2 âˆˆâ„}
(23)
ğ‘¥1
ğ‘¥2
â–¶However, the number of parameters is not always a good indictor
for the VC dimension. Considering the following hypothesis
space
45

--------------------------------------------------------------------------------
[End of Page 83]

Sine Functions
The hypothesis space of sine functions is defined as
Hsin = {sin(ğ›¼Â· ğ‘¥) : ğ›¼âˆˆâ„}
(24)
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
âˆ’0.5
0
0.5
1
â–¶ğ›¼= ğœ‹
4
â–¶ğ›¼= ğœ‹
2
â–¶ğ›¼= ğœ‹
46

--------------------------------------------------------------------------------
[End of Page 84]

Sine Functions
The hypothesis space of sine functions is defined as
Hsin = {sin(ğ›¼Â· ğ‘¥) : ğ›¼âˆˆâ„}
(24)
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
âˆ’0.5
0
0.5
1
â–¶ğ›¼= ğœ‹
4
â–¶ğ›¼= ğœ‹
2
â–¶ğ›¼= ğœ‹
46

--------------------------------------------------------------------------------
[End of Page 85]

Sine Functions
The hypothesis space of sine functions is defined as
Hsin = {sin(ğ›¼Â· ğ‘¥) : ğ›¼âˆˆâ„}
(24)
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
âˆ’0.5
0
0.5
1
â–¶ğ›¼= ğœ‹
4
â–¶ğ›¼= ğœ‹
2
â–¶ğ›¼= ğœ‹
46

--------------------------------------------------------------------------------
[End of Page 86]

Sine Functions
The hypothesis space of sine functions is defined as
Hsin = {sin(ğ›¼Â· ğ‘¥) : ğ›¼âˆˆâ„}
(24)
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
âˆ’0.5
0
0.5
1
â–¶ğ›¼= ğœ‹
4
â–¶ğ›¼= ğœ‹
2
â–¶ğ›¼= ğœ‹
VC-dim(Hsin) = âˆ
46

--------------------------------------------------------------------------------
[End of Page 87]

Reference
Mohri, M., Rostamizadeh, A., and Talwalkar, A. (2018).
Foundations of machine learning.
MIT press.
Shalev-Shwartz, S. and Ben-David, S. (2014).
Understanding machine learning: From theory to algorithms.
Cambridge university press.
47

--------------------------------------------------------------------------------
[End of Page 88]