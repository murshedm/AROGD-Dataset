CS 4774 Machine Learning
Gradient-based Optimization
Yangfeng Ji
Information and Language Processing Lab
Department of Computer Science
University of Virginia

--------------------------------------------------------------------------------
[End of Page 1]

Overview
1. Gradient Descent
2. Stochastic Gradient Descent
3. SGD with Momentum
4. Adaptive Learning Rates
1

--------------------------------------------------------------------------------
[End of Page 2]

Gradient Descent

--------------------------------------------------------------------------------
[End of Page 3]

Learning as Optimization
As discussed before, learning can be viewed as optimization problem.
â–¶Training set ğ‘†= {(ğ’™1, ğ‘¦1), . . . , (ğ’™ğ‘š, ğ‘¦ğ‘š)}
â–¶Empirical risk
ğ¿(â„ğœ½, ğ‘†) = 1
ğ‘š
ğ‘š
Ã•
ğ‘–=1
ğ‘…(â„ğœ½(ğ’™ğ‘–), ğ‘¦ğ‘–)
(1)
where ğ‘…is the risk function
â–¶Learning: minimize the empirical risk
ğœ½â†argmin
ğœ½â€²
ğ¿ğ‘†(â„ğœ½â€², ğ‘†)
(2)
3

--------------------------------------------------------------------------------
[End of Page 4]

Learning as Optimization (II)
Some examples of risk functions
â–¶Logistic regression
ğ‘…(â„ğœ½(ğ’™ğ‘–), ğ‘¦ğ‘–) = âˆ’log ğ‘(ğ‘¦ğ‘–| ğ’™ğ‘–; ğœ½)
(3)
4

--------------------------------------------------------------------------------
[End of Page 5]

Learning as Optimization (II)
Some examples of risk functions
â–¶Logistic regression
ğ‘…(â„ğœ½(ğ’™ğ‘–), ğ‘¦ğ‘–) = âˆ’log ğ‘(ğ‘¦ğ‘–| ğ’™ğ‘–; ğœ½)
(3)
â–¶Linear regression
ğ‘…(â„ğœ½(ğ’™ğ‘–), ğ‘¦ğ‘–) = âˆ¥â„ğœ½(ğ’™ğ‘–) âˆ’ğ‘¦ğ‘–âˆ¥2
2
(4)
4

--------------------------------------------------------------------------------
[End of Page 6]

Learning as Optimization (II)
Some examples of risk functions
â–¶Logistic regression
ğ‘…(â„ğœ½(ğ’™ğ‘–), ğ‘¦ğ‘–) = âˆ’log ğ‘(ğ‘¦ğ‘–| ğ’™ğ‘–; ğœ½)
(3)
â–¶Linear regression
ğ‘…(â„ğœ½(ğ’™ğ‘–), ğ‘¦ğ‘–) = âˆ¥â„ğœ½(ğ’™ğ‘–) âˆ’ğ‘¦ğ‘–âˆ¥2
2
(4)
â–¶Neural network
ğ‘…(â„ğœ½(ğ’™ğ‘–), ğ‘¦ğ‘–) = Cross-entropy(â„ğœ½(ğ’™ğ‘–), ğ‘¦ğ‘–)
(5)
4

--------------------------------------------------------------------------------
[End of Page 7]

Learning as Optimization (II)
Some examples of risk functions
â–¶Logistic regression
ğ‘…(â„ğœ½(ğ’™ğ‘–), ğ‘¦ğ‘–) = âˆ’log ğ‘(ğ‘¦ğ‘–| ğ’™ğ‘–; ğœ½)
(3)
â–¶Linear regression
ğ‘…(â„ğœ½(ğ’™ğ‘–), ğ‘¦ğ‘–) = âˆ¥â„ğœ½(ğ’™ğ‘–) âˆ’ğ‘¦ğ‘–âˆ¥2
2
(4)
â–¶Neural network
ğ‘…(â„ğœ½(ğ’™ğ‘–), ğ‘¦ğ‘–) = Cross-entropy(â„ğœ½(ğ’™ğ‘–), ğ‘¦ğ‘–)
(5)
â–¶Percetpron and AdaBoost can also be viewed as minimizing
certain loss functions
4

--------------------------------------------------------------------------------
[End of Page 8]

Constrained Optimization
The dual optimization problem for SVMs of the separable cases is
max
ğœ¶
ğ‘š
Ã•
ğ‘–=1
ğ›¼ğ‘–âˆ’1
2
ğ‘š
Ã•
ğ‘–,ğ‘—=1
ğ›¼ğ‘–ğ›¼ğ‘—ğ‘¦ğ‘–ğ‘¦ğ‘—âŸ¨ğ’™ğ‘–, ğ’™ğ‘—âŸ©
(6)
s.t.
ğ›¼ğ‘–â‰¥0
(7)
ğ‘š
Ã•
ğ‘–=1
ğ›¼ğ‘–ğ‘¦ğ‘–= 0 âˆ€ğ‘–âˆˆ[ğ‘š]
(8)
5

--------------------------------------------------------------------------------
[End of Page 9]

Constrained Optimization
The dual optimization problem for SVMs of the separable cases is
max
ğœ¶
ğ‘š
Ã•
ğ‘–=1
ğ›¼ğ‘–âˆ’1
2
ğ‘š
Ã•
ğ‘–,ğ‘—=1
ğ›¼ğ‘–ğ›¼ğ‘—ğ‘¦ğ‘–ğ‘¦ğ‘—âŸ¨ğ’™ğ‘–, ğ’™ğ‘—âŸ©
(6)
s.t.
ğ›¼ğ‘–â‰¥0
(7)
ğ‘š
Ã•
ğ‘–=1
ğ›¼ğ‘–ğ‘¦ğ‘–= 0 âˆ€ğ‘–âˆˆ[ğ‘š]
(8)
â–¶Lagrange multiplier ğœ¶is also called dual variable
â–¶This is an optimization problem only about ğœ¶
â–¶The dual problem is defined on the inner product âŸ¨ğ’™ğ‘–, ğ’™ğ‘—âŸ©
5

--------------------------------------------------------------------------------
[End of Page 10]

Optimization via Gradient Descent
The basic form of an optimization problem
min ğ‘“(ğœ½)
s.t.ğœ½âˆˆğµ
(9)
where ğ‘“(ğœ½) : â„ğ‘‘â†’â„is the objective function and ğµâŠ†â„ğ‘‘is the
constraint on ğœ½, which usually can be formulated as a set of
inequalities (e.g., SVM)
6

--------------------------------------------------------------------------------
[End of Page 11]

Optimization via Gradient Descent
The basic form of an optimization problem
min ğ‘“(ğœ½)
s.t.ğœ½âˆˆğµ
(9)
where ğ‘“(ğœ½) : â„ğ‘‘â†’â„is the objective function and ğµâŠ†â„ğ‘‘is the
constraint on ğœ½, which usually can be formulated as a set of
inequalities (e.g., SVM)
In this lecture
â–¶we only focus on unconstrained optimization problem, in other
words, ğœ½âˆˆâ„ğ‘‘
â–¶assume ğ‘“is convex and differentiable
6

--------------------------------------------------------------------------------
[End of Page 12]

Review: Gradient of a 1-D Function
Consider the gradient of this 1-dimensional function
ğ‘¦= ğ‘“(ğ‘¥) = ğ‘¥2 âˆ’ğ‘¥âˆ’2
(10)
7

--------------------------------------------------------------------------------
[End of Page 13]

Review: Gradient of a 2-D Function
Now, consider a 2-dimensional function with ğ’™= (ğ‘¥1, ğ‘¥2)
ğ‘¦= ğ‘“(ğ’™) = ğ‘¥2
1 + 2ğ‘¥2
2
(11)
Here is the contour plot of this function
We are going to use this as our running example
8

--------------------------------------------------------------------------------
[End of Page 14]

Gradient Descent
To learn the parameter ğœ½, the learning algorithm needs to update it
iteratively using the following three steps
1. Choose an initial point ğœ½(0) âˆˆâ„ğ‘‘
2. Repeat
ğœ½(ğ‘¡+1) â†ğœ½(ğ‘¡) âˆ’ğœ‚ğ‘¡Â· âˆ‡ğ‘“(ğœ½)|ğœ½=ğœ½(ğ‘¡)
(12)
where ğœ‚ğ‘¡is the learning rate at time ğ‘¡
3. Go back step 1 until it converges
9

--------------------------------------------------------------------------------
[End of Page 15]

Gradient Descent
To learn the parameter ğœ½, the learning algorithm needs to update it
iteratively using the following three steps
1. Choose an initial point ğœ½(0) âˆˆâ„ğ‘‘
2. Repeat
ğœ½(ğ‘¡+1) â†ğœ½(ğ‘¡) âˆ’ğœ‚ğ‘¡Â· âˆ‡ğ‘“(ğœ½)|ğœ½=ğœ½(ğ‘¡)
(12)
where ğœ‚ğ‘¡is the learning rate at time ğ‘¡
3. Go back step 1 until it converges
âˆ‡ğ‘“(ğœ½) is defined as
âˆ‡ğ‘“(ğœ½) =
 ğœ•ğ‘“(ğœ½)
ğœ•ğœƒ1
, Â· Â· Â· , ğœ•ğ‘“(ğœ½)
ğœ•ğœƒğ‘‘

(13)
9

--------------------------------------------------------------------------------
[End of Page 16]

Gradient Descent Interpretation
An intuitive justification of the gradient descent algorithm is to
consider the following plot
The direction of the gradient is the direction that the function has the
â€œfastest increaseâ€.
10

--------------------------------------------------------------------------------
[End of Page 17]

Gradient Descent Interpretation (II)
Theoretical justification
â–¶First-order Taylor approximation
ğ‘“(ğœ½+ Î”ğœ½) â‰ˆğ‘“(ğœ½) + âŸ¨Î”ğœ½, âˆ‡ğ‘“âŸ©

ğœ½
(14)
11

--------------------------------------------------------------------------------
[End of Page 18]

Gradient Descent Interpretation (II)
Theoretical justification
â–¶First-order Taylor approximation
ğ‘“(ğœ½+ Î”ğœ½) â‰ˆğ‘“(ğœ½) + âŸ¨Î”ğœ½, âˆ‡ğ‘“âŸ©

ğœ½
(14)
â–¶In gradient descent, Î”ğœ½= âˆ’ğœ‚âˆ‡ğ‘“

ğœ½
11

--------------------------------------------------------------------------------
[End of Page 19]

Gradient Descent Interpretation (II)
Theoretical justification
â–¶First-order Taylor approximation
ğ‘“(ğœ½+ Î”ğœ½) â‰ˆğ‘“(ğœ½) + âŸ¨Î”ğœ½, âˆ‡ğ‘“âŸ©

ğœ½
(14)
â–¶In gradient descent, Î”ğœ½= âˆ’ğœ‚âˆ‡ğ‘“

ğœ½
â–¶Therefore, we have
ğ‘“(ğœ½+ Î”ğœ½)
â‰ˆ
ğ‘“(ğœ½) + âŸ¨Î”ğœ½, âˆ‡ğ‘“âŸ©

ğœ½
=
ğ‘“(ğœ½) âˆ’âŸ¨ğœ‚âˆ‡ğ‘“, âˆ‡ğ‘“âŸ©

ğœ½
=
ğ‘“(ğœ½) âˆ’ğœ‚âˆ¥âˆ‡ğ‘“âˆ¥2
2

ğœ½â‰¤ğ‘“(ğœ½)
(15)
11

--------------------------------------------------------------------------------
[End of Page 20]

Gradient Descent Interpretation (III)
Consider the second-order Taylor approximation of ğ‘“
ğ‘“(ğœ½â€²) â‰ˆğ‘“(ğœ½) + âˆ‡ğ‘“(ğœ½)(ğœ½â€² âˆ’ğœ½) + 1
2(ğœ½â€² âˆ’ğœ½)Tâˆ‡2 ğ‘“(ğœ½)(ğœ½â€² âˆ’ğœ½)
12

--------------------------------------------------------------------------------
[End of Page 21]

Gradient Descent Interpretation (III)
Consider the second-order Taylor approximation of ğ‘“
ğ‘“(ğœ½â€²) â‰ˆğ‘“(ğœ½) + âˆ‡ğ‘“(ğœ½)(ğœ½â€² âˆ’ğœ½) + 1
2(ğœ½â€² âˆ’ğœ½)Tâˆ‡2 ğ‘“(ğœ½)(ğœ½â€² âˆ’ğœ½)
â–¶The quadratic approximation of ğ‘“with the following
ğ‘“(ğœ½â€²) â‰ˆğ‘“(ğœ½) + âˆ‡ğ‘“(ğœ½)(ğœ½â€² âˆ’ğœ½) + 1
2ğœ‚(ğœ½â€² âˆ’ğœ½)T(ğœ½â€² âˆ’ğœ½)
12

--------------------------------------------------------------------------------
[End of Page 22]

Gradient Descent Interpretation (III)
Consider the second-order Taylor approximation of ğ‘“
ğ‘“(ğœ½â€²) â‰ˆğ‘“(ğœ½) + âˆ‡ğ‘“(ğœ½)(ğœ½â€² âˆ’ğœ½) + 1
2(ğœ½â€² âˆ’ğœ½)Tâˆ‡2 ğ‘“(ğœ½)(ğœ½â€² âˆ’ğœ½)
â–¶The quadratic approximation of ğ‘“with the following
ğ‘“(ğœ½â€²) â‰ˆğ‘“(ğœ½) + âˆ‡ğ‘“(ğœ½)(ğœ½â€² âˆ’ğœ½) + 1
2ğœ‚(ğœ½â€² âˆ’ğœ½)T(ğœ½â€² âˆ’ğœ½)
â–¶Minimize ğ‘“(ğœ½â€²) wrt ğœ½â€²
ğœ•ğ‘“(ğœ½â€²)
ğœ•ğœ½â€²
â‰ˆ
âˆ‡ğ‘“(ğœ½) + 1
2ğœ‚(ğœ½â€² âˆ’ğœ½) = 0
â‡’
ğœ½â€² = ğœ½âˆ’ğœ‚Â· âˆ‡ğ‘“(ğœ½)
(16)
12

--------------------------------------------------------------------------------
[End of Page 23]

Gradient Descent Interpretation (III)
Consider the second-order Taylor approximation of ğ‘“
ğ‘“(ğœ½â€²) â‰ˆğ‘“(ğœ½) + âˆ‡ğ‘“(ğœ½)(ğœ½â€² âˆ’ğœ½) + 1
2(ğœ½â€² âˆ’ğœ½)Tâˆ‡2 ğ‘“(ğœ½)(ğœ½â€² âˆ’ğœ½)
â–¶The quadratic approximation of ğ‘“with the following
ğ‘“(ğœ½â€²) â‰ˆğ‘“(ğœ½) + âˆ‡ğ‘“(ğœ½)(ğœ½â€² âˆ’ğœ½) + 1
2ğœ‚(ğœ½â€² âˆ’ğœ½)T(ğœ½â€² âˆ’ğœ½)
â–¶Minimize ğ‘“(ğœ½â€²) wrt ğœ½â€²
ğœ•ğ‘“(ğœ½â€²)
ğœ•ğœ½â€²
â‰ˆ
âˆ‡ğ‘“(ğœ½) + 1
2ğœ‚(ğœ½â€² âˆ’ğœ½) = 0
â‡’
ğœ½â€² = ğœ½âˆ’ğœ‚Â· âˆ‡ğ‘“(ğœ½)
(16)
â–¶Gradient descent chooses the next point ğœ½â€² to minimize the
function
12

--------------------------------------------------------------------------------
[End of Page 24]

Step size
ğœ½(ğ‘¡+1) â†ğœ½(ğ‘¡) âˆ’ğœ‚ğ‘¡Â· ğœ•ğ‘“(ğœ½)
ğœ•ğœ½

ğœ½=ğœ½(ğ‘¡)
(17)
If choose fixed step size ğœ‚ğ‘¡= ğœ‚0, consider the following function
ğ‘“(ğœ½) = (10ğœƒ2
1 + ğœƒ2
2)/2
(a) Too small
13

--------------------------------------------------------------------------------
[End of Page 25]

Step size
ğœ½(ğ‘¡+1) â†ğœ½(ğ‘¡) âˆ’ğœ‚ğ‘¡Â· ğœ•ğ‘“(ğœ½)
ğœ•ğœ½

ğœ½=ğœ½(ğ‘¡)
(17)
If choose fixed step size ğœ‚ğ‘¡= ğœ‚0, consider the following function
ğ‘“(ğœ½) = (10ğœƒ2
1 + ğœƒ2
2)/2
(d) Too small
(e) Too large
13

--------------------------------------------------------------------------------
[End of Page 26]

Step size
ğœ½(ğ‘¡+1) â†ğœ½(ğ‘¡) âˆ’ğœ‚ğ‘¡Â· ğœ•ğ‘“(ğœ½)
ğœ•ğœ½

ğœ½=ğœ½(ğ‘¡)
(17)
If choose fixed step size ğœ‚ğ‘¡= ğœ‚0, consider the following function
ğ‘“(ğœ½) = (10ğœƒ2
1 + ğœƒ2
2)/2
(g) Too small
(h) Too large
(i) Just right
13

--------------------------------------------------------------------------------
[End of Page 27]

Optimal Step Sizes
â–¶Exact Line Search Solve this one-dimensional subproblem
ğ‘¡â†argmin
ğ‘ â‰¥0
ğ‘“(ğœ½âˆ’ğ‘ âˆ‡ğ‘“(ğœ½))
(18)
14

--------------------------------------------------------------------------------
[End of Page 28]

Optimal Step Sizes
â–¶Exact Line Search Solve this one-dimensional subproblem
ğ‘¡â†argmin
ğ‘ â‰¥0
ğ‘“(ğœ½âˆ’ğ‘ âˆ‡ğ‘“(ğœ½))
(18)
â–¶Backtracking Line Search: with parameters 0 < ğ›½< 1,
0 < ğ›¼â‰¤1/2, and large initial value ğœ‚ğ‘¡, if
ğ‘“(ğœ½âˆ’ğœ‚âˆ‡ğ‘“(ğœ½)) > ğ‘“(ğœ½) âˆ’ğ›¼ğœ‚ğ‘¡âˆ¥âˆ‡ğ‘“(ğœ½)âˆ¥2
2
(19)
shrink ğœ‚ğ‘¡â†ğ›½ğœ‚ğ‘¡
14

--------------------------------------------------------------------------------
[End of Page 29]

Optimal Step Sizes
â–¶Exact Line Search Solve this one-dimensional subproblem
ğ‘¡â†argmin
ğ‘ â‰¥0
ğ‘“(ğœ½âˆ’ğ‘ âˆ‡ğ‘“(ğœ½))
(18)
â–¶Backtracking Line Search: with parameters 0 < ğ›½< 1,
0 < ğ›¼â‰¤1/2, and large initial value ğœ‚ğ‘¡, if
ğ‘“(ğœ½âˆ’ğœ‚âˆ‡ğ‘“(ğœ½)) > ğ‘“(ğœ½) âˆ’ğ›¼ğœ‚ğ‘¡âˆ¥âˆ‡ğ‘“(ğœ½)âˆ¥2
2
(19)
shrink ğœ‚ğ‘¡â†ğ›½ğœ‚ğ‘¡
â–¶Usually, this is not worth the effort, since the computational
complexity may be too high (e.g., ğ‘“is a neural network)
14

--------------------------------------------------------------------------------
[End of Page 30]

Convergence Analysis
â–¶ğ‘“is convex and differentiable, additionally
âˆ¥âˆ‡ğ‘“(ğœ½) âˆ’âˆ‡ğ‘“(ğœ½â€²)âˆ¥2 â‰¤ğ¿Â· âˆ¥ğœ½âˆ’ğœ½â€²âˆ¥2
(20)
for any ğœ½, ğœ½â€² âˆˆâ„ğ‘‘and ğ¿is a fixed positive value
15

--------------------------------------------------------------------------------
[End of Page 31]

Convergence Analysis
â–¶ğ‘“is convex and differentiable, additionally
âˆ¥âˆ‡ğ‘“(ğœ½) âˆ’âˆ‡ğ‘“(ğœ½â€²)âˆ¥2 â‰¤ğ¿Â· âˆ¥ğœ½âˆ’ğœ½â€²âˆ¥2
(20)
for any ğœ½, ğœ½â€² âˆˆâ„ğ‘‘and ğ¿is a fixed positive value
â–¶Theorem: Gradient descent with fixed step size ğœ‚0 â‰¤1/ğ¿satisfies
ğ‘“(ğœ½(ğ‘¡)) âˆ’ğ‘“âˆ—â‰¤
âˆ¥ğœ½(0) âˆ’ğœ½âˆ—âˆ¥2
2
2ğœ‚0ğ‘¡
(21)
where ğ‘“âˆ—is the optimal value and ğœ½âˆ—is the optimal parameter
15

--------------------------------------------------------------------------------
[End of Page 32]

Convergence Analysis
â–¶ğ‘“is convex and differentiable, additionally
âˆ¥âˆ‡ğ‘“(ğœ½) âˆ’âˆ‡ğ‘“(ğœ½â€²)âˆ¥2 â‰¤ğ¿Â· âˆ¥ğœ½âˆ’ğœ½â€²âˆ¥2
(20)
for any ğœ½, ğœ½â€² âˆˆâ„ğ‘‘and ğ¿is a fixed positive value
â–¶Theorem: Gradient descent with fixed step size ğœ‚0 â‰¤1/ğ¿satisfies
ğ‘“(ğœ½(ğ‘¡)) âˆ’ğ‘“âˆ—â‰¤
âˆ¥ğœ½(0) âˆ’ğœ½âˆ—âˆ¥2
2
2ğœ‚0ğ‘¡
(21)
where ğ‘“âˆ—is the optimal value and ğœ½âˆ—is the optimal parameter
â–¶Same result holds for backtracking with ğœ‚0 replaced by ğ›½/ğ¿
15

--------------------------------------------------------------------------------
[End of Page 33]

Stochastic Gradient Descent

--------------------------------------------------------------------------------
[End of Page 34]

Gradient Descent
Given a training set {(ğ’™ğ‘–, ğ‘¦ğ‘–)}ğ‘š
ğ‘–=1, the loss function is defined as
ğ¿(â„ğœ½, ğ‘†) = 1
ğ‘š
ğ‘š
Ã•
ğ‘–=1
ğ‘…(â„ğœ½(ğ’™ğ‘–), ğ‘¦ğ‘–)
(22)
where ğ‘…is the risk function
Examples:
â–¶Logistic regression
ğ‘…(â„ğœ½(ğ’™ğ‘–), ğ‘¦ğ‘–) = âˆ’log ğ‘(ğ‘¦ğ‘–| ğ’™ğ‘–; ğœ½)
(23)
â–¶Linear regression
ğ‘…(â„ğœ½(ğ’™ğ‘–), ğ‘¦ğ‘–) = âˆ¥â„ğœ½(ğ’™ğ‘–) âˆ’ğ‘¦ğ‘–âˆ¥2
2
(24)
17

--------------------------------------------------------------------------------
[End of Page 35]

Gradient Descent (II)
â–¶Consider the gradient of loss function âˆ‡ğ¿(â„ğœ½, ğ‘†)
âˆ‡ğ¿(â„ğœ½, ğ‘†) = 1
ğ‘š
ğ‘š
Ã•
ğ‘–=1
âˆ‡ğ‘…(â„ğœ½(ğ’™ğ‘–), ğ‘¦ğ‘–)
(25)
18

--------------------------------------------------------------------------------
[End of Page 36]

Gradient Descent (II)
â–¶Consider the gradient of loss function âˆ‡ğ¿(â„ğœ½, ğ‘†)
âˆ‡ğ¿(â„ğœ½, ğ‘†) = 1
ğ‘š
ğ‘š
Ã•
ğ‘–=1
âˆ‡ğ‘…(â„ğœ½(ğ’™ğ‘–), ğ‘¦ğ‘–)
(25)
â–¶To simplify the notation, let ğ‘“ğ‘–(ğœ½) = ğ‘…(â„ğœ½(ğ’™ğ‘–), ğ‘¦ğ‘–) and
ğ‘“(ğœ½) = ğ¿(â„ğœ½, ğ‘†), then
âˆ‡ğ‘“(ğœ½) = 1
ğ‘š
ğ‘š
Ã•
ğ‘–=1
âˆ‡ğ‘“ğ‘–(ğœ½)
(26)
18

--------------------------------------------------------------------------------
[End of Page 37]

Stochastic Gradient Descent
To learn the parameter ğœ½, we can compute the gradient with one
training example (ğ’™ğ‘–, ğ‘¦ğ‘–) each time step and update the parameter as
ğœ½(ğ‘¡+1) â†ğœ½(ğ‘¡) âˆ’ğœ‚ğ‘¡Â· âˆ‡ğ‘“ğ‘–(ğœ½)|ğœ½(ğ‘¡)
(27)
where
â–¶ğ‘¡: time step
â–¶âˆ‡ğ‘“ğ‘–(ğœ½(ğ‘¡)) is the gradient of the single-example loss ğ¿
â–¶ğœ‚ğ‘¡is the learning rate (step size)
19

--------------------------------------------------------------------------------
[End of Page 38]

Stochastic?
Compare gradient descent and stochastic gradient descent
As each step SGD only uses the gradient from one training example,
it can be viewed as a gradient descent method with some randomness
20

--------------------------------------------------------------------------------
[End of Page 39]

Motivation
There are at least two motivations of using SGD
â–¶SGD can be a big savings in terms of memory usage
â–¶learning with large-scale data
â–¶models with lots of parameters
â–¶The iteration cost of SGD is independent of sample size ğ‘š
21

--------------------------------------------------------------------------------
[End of Page 40]

Motivation (II)
An empirical comparison between SGD and a batch optimization
method (L-BFGS) on a binary classification problem with logistic
regression [Bottou et al., 2018]
22

--------------------------------------------------------------------------------
[End of Page 41]

How to Choose an Example
â–¶Cyclic Rule: choose ğ‘–âˆˆ(1, 2, . . . , ğ‘š) in order
23

--------------------------------------------------------------------------------
[End of Page 42]

How to Choose an Example
â–¶Cyclic Rule: choose ğ‘–âˆˆ(1, 2, . . . , ğ‘š) in order
â–¶Randomized Rule: Every iteration, choose ğ‘–âˆˆ[ğ‘š] uniformly at
random
â–¶In practice, randomized rule is more common, since we have
ğ¸[âˆ‡ğ‘“ğ‘–(ğœ½)] â‰ˆ1
ğ‘š
ğ‘š
Ã•
ğ‘–=1
âˆ‡ğ‘“ğ‘–(ğœ½) = âˆ‡ğ‘“(ğœ½)
(28)
as an unbiased estimate of âˆ‡ğ‘“(ğœ½)
â–¶Alternatively, shuffle the training example at the end of each
training epoch
23

--------------------------------------------------------------------------------
[End of Page 43]

Convergence of SGD
The convergence of SGD usually requires diminishing step sizes
â–¶The usual conditions on the learning rates are
âˆ
Ã•
ğ‘¡=1
ğœ‚ğ‘¡= âˆ
âˆ
Ã•
ğ‘¡=1
ğœ‚2
ğ‘¡â‰¤âˆ
(29)
[Bottou et al., 1998]
24

--------------------------------------------------------------------------------
[End of Page 44]

Convergence of SGD
The convergence of SGD usually requires diminishing step sizes
â–¶The usual conditions on the learning rates are
âˆ
Ã•
ğ‘¡=1
ğœ‚ğ‘¡= âˆ
âˆ
Ã•
ğ‘¡=1
ğœ‚2
ğ‘¡â‰¤âˆ
(29)
â–¶A simplest function that satisfies these conditions is
ğœ‚ğ‘¡= 1
ğ‘¡
(30)
[Bottou et al., 1998]
24

--------------------------------------------------------------------------------
[End of Page 45]

SGD with Momentum

--------------------------------------------------------------------------------
[End of Page 46]

Review: Vector Addition
The parallelogram law of vector addition
ğ’„= ğ’‚+ ğ’ƒ
(31)
26

--------------------------------------------------------------------------------
[End of Page 47]

SGD with Momentum
Given the loss function ğ‘“(ğœ½) to be minimized, SGD with momentum
is given by
ğ’—(ğ‘¡)
=
ğœ‡ğ’—(ğ‘¡âˆ’1) + âˆ‡ğ‘“(ğœ½)|ğœ½(ğ‘¡âˆ’1)
(32)
ğœ½(ğ‘¡)
=
ğœ½(ğ‘¡âˆ’1) âˆ’ğœ‚ğ‘¡ğ’—(ğ‘¡)
(33)
where
â–¶ğœ‚ğ‘¡is still the learning rate
â–¶ğœ‡âˆˆ[0, 1] is the momentum coefficient. Usually, ğœ‡= 0.99 or 0.999.
27

--------------------------------------------------------------------------------
[End of Page 48]

Intuitive Explanation
(Note: the arrow show the opposite direction of the gradient)
(a) SGD without momentum
Figure: The effect of momentum in SGD: reduce the fluctuation (Credit:
Genevieve B. Orr)
28

--------------------------------------------------------------------------------
[End of Page 49]

Intuitive Explanation
(Note: the arrow show the opposite direction of the gradient)
(a) SGD without momentum
(b) SGD with momentum
Figure: The effect of momentum in SGD: reduce the fluctuation (Credit:
Genevieve B. Orr)
28

--------------------------------------------------------------------------------
[End of Page 50]

Another Example with Contour Plot
Consider the following problem
ğ‘¦= ğ‘¥2
1 + 10ğ‘¥2
2
(34)
ğœ•ğ‘¦
ğœ•ğ‘¥1
= 2ğ‘¥1
ğœ•ğ‘¦
ğœ•ğ‘¥2
= 20ğ‘¥2
(35)
Note: the arrow show the opposite direction of the gradient
29

--------------------------------------------------------------------------------
[End of Page 51]

Another Example with Contour Plot (Cont.)
Add the previous gradient reduce the fluctuation of stochastic
gradients
ğ’—(ğ‘¡) = ğœ‡ğ’—(ğ‘¡âˆ’1) + ğ’ˆ(ğ‘¡âˆ’1)
(36)
!"($%&)
(($%&)
"($)
Note: the arrow show the opposite direction of the gradient
30

--------------------------------------------------------------------------------
[End of Page 52]

Adaptive Learning Rates

--------------------------------------------------------------------------------
[End of Page 53]

Basic Idea
The basic idea of using adaptive learning rates is to make sure that
all ğœ½ğ‘˜â€™s converge roughly at the same speed
For neural networks, the motivation of picking a different learning
rate for each ğœ½ğ‘˜(the ğ‘˜-th component of parameter ğœ½) is not
new [LeCun et al., 2012] (the article was originally published in 1998).
32

--------------------------------------------------------------------------------
[End of Page 54]

AdaGrad
The basic idea of AdaGrad [Duchi et al., 2011] is to modify the
learning rate ğœ‚for ğœ½ğ‘˜by using the history of the gradients
ğœƒ(ğ‘¡)
ğ‘˜
= ğœƒ(ğ‘¡âˆ’1)
ğ‘˜
âˆ’
ğœ‚0
q
ğº(ğ‘¡âˆ’1)
ğ‘˜,ğ‘˜
+ ğœ–
ğ‘”(ğ‘¡âˆ’1)
ğ‘˜
(37)
33

--------------------------------------------------------------------------------
[End of Page 55]

AdaGrad
The basic idea of AdaGrad [Duchi et al., 2011] is to modify the
learning rate ğœ‚for ğœ½ğ‘˜by using the history of the gradients
ğœƒ(ğ‘¡)
ğ‘˜
= ğœƒ(ğ‘¡âˆ’1)
ğ‘˜
âˆ’
ğœ‚0
q
ğº(ğ‘¡âˆ’1)
ğ‘˜,ğ‘˜
+ ğœ–
ğ‘”(ğ‘¡âˆ’1)
ğ‘˜
(37)
â–¶ğ‘”(ğ‘¡âˆ’1)
ğ‘˜
= [âˆ‡ğ‘“(ğœ½)|ğœ½(ğ‘¡âˆ’1)]ğ‘˜is the ğ‘˜-th component of âˆ‡ğ‘“(ğœ½)|ğœ½(ğ‘¡âˆ’1)
â–¶ğº(ğ‘¡âˆ’1)
ğ‘˜,ğ‘˜
= Ãğ‘¡âˆ’1
ğ‘–=1(ğ‘”(ğ‘–)
ğ‘˜)2
33

--------------------------------------------------------------------------------
[End of Page 56]

AdaGrad
The basic idea of AdaGrad [Duchi et al., 2011] is to modify the
learning rate ğœ‚for ğœ½ğ‘˜by using the history of the gradients
ğœƒ(ğ‘¡)
ğ‘˜
= ğœƒ(ğ‘¡âˆ’1)
ğ‘˜
âˆ’
ğœ‚0
q
ğº(ğ‘¡âˆ’1)
ğ‘˜,ğ‘˜
+ ğœ–
ğ‘”(ğ‘¡âˆ’1)
ğ‘˜
(37)
â–¶ğ‘”(ğ‘¡âˆ’1)
ğ‘˜
= [âˆ‡ğ‘“(ğœ½)|ğœ½(ğ‘¡âˆ’1)]ğ‘˜is the ğ‘˜-th component of âˆ‡ğ‘“(ğœ½)|ğœ½(ğ‘¡âˆ’1)
â–¶ğº(ğ‘¡âˆ’1)
ğ‘˜,ğ‘˜
= Ãğ‘¡âˆ’1
ğ‘–=1(ğ‘”(ğ‘–)
ğ‘˜)2
â–¶ğœ‚0 is the initial learning rate
â–¶ğœ–is a smoothing parameter usually with order 10âˆ’6
33

--------------------------------------------------------------------------------
[End of Page 57]

AdaGrad: Intuitive Explanation
Consider the gradient of a 2-dimensional optimization problem with
ğœ½= (ğœƒ1, ğœƒ2)
ğœƒ(ğ‘¡)
ğ‘˜
= ğœƒ(ğ‘¡âˆ’1)
ğ‘˜
âˆ’
ğœ‚0
q
ğº(ğ‘¡âˆ’1)
ğ‘˜,ğ‘˜
+ ğœ–
ğ‘”(ğ‘¡âˆ’1)
ğ‘˜
(38)
The magnitude of gradient along ğœƒ2 is often larger then ğœƒ1
34

--------------------------------------------------------------------------------
[End of Page 58]

AdaGrad: Intuitive Explanation
Consider the gradient of a 2-dimensional optimization problem with
ğœ½= (ğœƒ1, ğœƒ2)
ğœƒ(ğ‘¡)
ğ‘˜
= ğœƒ(ğ‘¡âˆ’1)
ğ‘˜
âˆ’
ğœ‚0
q
ğº(ğ‘¡âˆ’1)
ğ‘˜,ğ‘˜
+ ğœ–
ğ‘”(ğ‘¡âˆ’1)
ğ‘˜
(38)
The magnitude of gradient along ğœƒ2 is often larger then ğœƒ1
AdaGrad helps shrink step sizes along ğœƒ2 that allows the procedure
converges roughly at the same speed
34

--------------------------------------------------------------------------------
[End of Page 59]

RMSProp
RMSProp (Root Mean Square Propagation) uses a moving average
over the past gradients
ğœƒ(ğ‘¡)
ğ‘˜
= ğœƒ(ğ‘¡âˆ’1)
ğ‘˜
âˆ’
ğœ‚0
q
ğ’“(ğ‘¡)
ğ‘˜+ ğœ–
ğ‘”(ğ‘¡âˆ’1)
ğ‘˜
(39)
where
ğ‘Ÿ(ğ‘¡)
ğ‘˜
= ğœŒğ‘Ÿ(ğ‘¡âˆ’1)
ğ‘˜
+ (1 âˆ’ğœŒ)[ğ‘”(ğ‘¡âˆ’1)
ğ‘˜
]2
(40)
and ğœŒâˆˆ(0, 1), ğ‘˜is the dimension index, and ğ‘¡is the time stemp
[Hinton, 2012]
35

--------------------------------------------------------------------------------
[End of Page 60]

Adam
The Adam algorithm [Kingma and Ba, 2014] is proposed to combine
the idea of SGD with moment and RMSProp
36

--------------------------------------------------------------------------------
[End of Page 61]

Adam
The Adam algorithm [Kingma and Ba, 2014] is proposed to combine
the idea of SGD with moment and RMSProp
ğ‘£(ğ‘¡)
ğ‘˜
=
ğœ‡ğ‘£(ğ‘¡âˆ’1)
ğ‘˜
+ (1 âˆ’ğœ‡)ğ‘”(ğ‘¡âˆ’1)
ğ‘˜
(41)
ğ‘Ÿ(ğ‘¡)
ğ‘˜
=
ğœŒğ‘Ÿ(ğ‘¡âˆ’1)
ğ‘˜
+ (1 âˆ’ğœŒ)[ğ‘”(ğ‘¡âˆ’1)
ğ‘˜
]2
(42)
Ë†ğ‘£(ğ‘¡)
ğ‘˜
=
ğ‘£(ğ‘¡)
ğ‘˜
1 âˆ’ğœ‡ğ‘¡
(43)
Ë†ğ‘Ÿ(ğ‘¡)
ğ‘˜
=
ğ‘Ÿ(ğ‘¡)
ğ‘˜
1 âˆ’ğœŒğ‘¡
(44)
ğœƒ(ğ‘¡)
ğ‘˜
=
ğœƒ(ğ‘¡âˆ’1)
ğ‘˜
âˆ’ğœ‚0
Ë†ğ‘£(ğ‘¡)
ğ‘˜
q
Ë†ğ‘Ÿ(ğ‘¡)
ğ‘˜
+ ğœ–
(45)
The default values of ğœ‡and ğœŒare 0.9 and 0.999 respectively.
36

--------------------------------------------------------------------------------
[End of Page 62]

Adam
The Adam algorithm [Kingma and Ba, 2014] is proposed to combine
the idea of SGD with moment and RMSProp
ğ‘£(ğ‘¡)
ğ‘˜
=
ğœ‡ğ‘£(ğ‘¡âˆ’1)
ğ‘˜
+ (1 âˆ’ğœ‡)ğ‘”(ğ‘¡âˆ’1)
ğ‘˜
(41)
ğ‘Ÿ(ğ‘¡)
ğ‘˜
=
ğœŒğ‘Ÿ(ğ‘¡âˆ’1)
ğ‘˜
+ (1 âˆ’ğœŒ)[ğ‘”(ğ‘¡âˆ’1)
ğ‘˜
]2
(42)
(43)
(44)
ğœƒ(ğ‘¡)
ğ‘˜
=
ğœƒ(ğ‘¡âˆ’1)
ğ‘˜
âˆ’ğœ‚0
Ë†ğ‘£(ğ‘¡)
ğ‘˜
q
Ë†ğ‘Ÿ(ğ‘¡)
ğ‘˜
+ ğœ–
(45)
The default values of ğœ‡and ğœŒare 0.9 and 0.999 respectively.
36

--------------------------------------------------------------------------------
[End of Page 63]

How to Choose a Optimization Algorithm?
[Hinton, 2012, Lecture Notes in 2012]
37

--------------------------------------------------------------------------------
[End of Page 64]

Reference
Bottou, L., Curtis, F. E., and Nocedal, J. (2018).
Optimization methods for large-scale machine learning.
Siam Review, 60(2):223â€“311.
Bottou, L. et al. (1998).
Online learning and stochastic approximations.
On-line learning in neural networks, 17(9):142.
Duchi, J., Hazan, E., and Singer, Y. (2011).
Adaptive subgradient methods for online learning and stochastic optimization.
Journal of machine learning research, 12(Jul):2121â€“2159.
Hinton, G. (2012).
Neural networks for machine learning.
Lecture notes.
Kingma, D. P. and Ba, J. (2014).
Adam: A method for stochastic optimization.
arXiv preprint arXiv:1412.6980.
LeCun, Y. A., Bottou, L., Orr, G. B., and MÃ¼ller, K.-R. (2012).
Efficient backprop.
In Neural networks: Tricks of the trade, pages 9â€“48. Springer.
38

--------------------------------------------------------------------------------
[End of Page 65]